{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Objective\n## 1. Pre-process the data\n## 1. Fit a LDA model to the cleaned and pre-processed data\n## 2. Group topics together based on their Hellinger distance\n## 3. Label comments with topics\n",
   "metadata": {
    "tags": [],
    "cell_id": "00000-9b2fcf2a-d04a-4dc6-bf4d-7e4256a29722",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Import the data and the modules",
   "metadata": {
    "tags": [],
    "cell_id": "00002-a1fedc04-c3b4-4134-98ee-72dad0b82e4b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cdd645cc",
    "execution_millis": 8830,
    "execution_start": 1614849498254,
    "cell_id": "00003-a3d2bd3f-3a4e-4540-9308-3686758b03fa",
    "deepnote_cell_type": "code"
   },
   "source": "!pip install gensim==3.8.3\n!pip install pyLDAvis==3.2.0\n!pip install wordcloud==1.8.1",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\nRequirement already satisfied: gensim==3.8.3 in /root/venv/lib/python3.7/site-packages (3.8.3)\nRequirement already satisfied: six>=1.5.0 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from gensim==3.8.3) (1.15.0)\nRequirement already satisfied: numpy>=1.11.3 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from gensim==3.8.3) (1.19.5)\nRequirement already satisfied: smart-open>=1.8.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from gensim==3.8.3) (3.0.0)\nRequirement already satisfied: scipy>=0.18.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from gensim==3.8.3) (1.6.1)\nRequirement already satisfied: requests in /shared-libs/python3.7/py/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim==3.8.3) (2.25.1)\nRequirement already satisfied: idna<3,>=2.5 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim==3.8.3) (1.26.3)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: pyLDAvis==3.2.0 in /root/venv/lib/python3.7/site-packages (3.2.0)\nRequirement already satisfied: scipy>=0.18.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pyLDAvis==3.2.0) (1.6.1)\nRequirement already satisfied: funcy in /root/venv/lib/python3.7/site-packages (from pyLDAvis==3.2.0) (1.15)\nRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/site-packages (from pyLDAvis==3.2.0) (0.36.2)\nRequirement already satisfied: joblib>=0.8.4 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pyLDAvis==3.2.0) (1.0.1)\nRequirement already satisfied: jinja2>=2.7.2 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from pyLDAvis==3.2.0) (2.11.3)\nRequirement already satisfied: pandas>=0.17.0; python_version > \"3.5\" in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pyLDAvis==3.2.0) (1.2.2)\nRequirement already satisfied: future in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pyLDAvis==3.2.0) (0.18.2)\nRequirement already satisfied: numpy>=1.9.2 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pyLDAvis==3.2.0) (1.19.5)\nRequirement already satisfied: numexpr in /root/venv/lib/python3.7/site-packages (from pyLDAvis==3.2.0) (2.7.3)\nRequirement already satisfied: MarkupSafe>=0.23 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis==3.2.0) (1.1.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from pandas>=0.17.0; python_version > \"3.5\"->pyLDAvis==3.2.0) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from pandas>=0.17.0; python_version > \"3.5\"->pyLDAvis==3.2.0) (2020.5)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.17.0; python_version > \"3.5\"->pyLDAvis==3.2.0) (1.15.0)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: wordcloud==1.8.1 in /root/venv/lib/python3.7/site-packages (1.8.1)\nRequirement already satisfied: numpy>=1.6.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from wordcloud==1.8.1) (1.19.5)\nRequirement already satisfied: matplotlib in /shared-libs/python3.7/py/lib/python3.7/site-packages (from wordcloud==1.8.1) (3.3.4)\nRequirement already satisfied: pillow in /shared-libs/python3.7/py/lib/python3.7/site-packages (from wordcloud==1.8.1) (8.1.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from matplotlib->wordcloud==1.8.1) (2.4.7)\nRequirement already satisfied: kiwisolver>=1.0.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from matplotlib->wordcloud==1.8.1) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.1 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from matplotlib->wordcloud==1.8.1) (2.8.1)\nRequirement already satisfied: cycler>=0.10 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from matplotlib->wordcloud==1.8.1) (0.10.0)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib->wordcloud==1.8.1) (1.15.0)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 31,
    "scrolled": true,
    "source_hash": "422f0d15",
    "tags": [],
    "execution_start": 1614849507097,
    "cell_id": "00004-f39db08a-baee-4bf8-8fb8-ade424411e68",
    "deepnote_cell_type": "code"
   },
   "source": "import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tqdm\nimport gensim\nimport pyLDAvis.gensim\nimport pickle \nimport pyLDAvis\nimport pyLDAvis.gensim\nimport nltk\nimport numpy as np\nnp.random.seed(2018)\n\nfrom gensim.models import CoherenceModel\nfrom gensim.utils import simple_preprocess\nfrom gensim.matutils import hellinger\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim import corpora, models\nfrom pprint import pprint\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer \nfrom nltk.stem.porter import *\nfrom wordcloud import WordCloud\nfrom textblob import TextBlob\nfrom sklearn.model_selection import train_test_split\n\nnltk.download('wordnet')\n\nall_stopwords_gensim = STOPWORDS.union(set(['awww', \n                                            'afternoon', \n                                            'mmmm', \n                                            'haha', \n                                            'amazing',\n                                            'awesome',\n                                            'gorgeous',\n                                            'sexy',\n                                            'fantastic',\n                                            'beautiful',\n                                            'perfect',\n                                            'favorite',\n                                            'great',\n                                            'good',\n                                            'sweet',\n                                            'best',\n                                            'super',\n                                            'nice',\n                                            'xoxo',\n                                            'crazy',\n                                            'soooo',\n                                            'incredible',\n                                            'absolute',\n                                            'delicioous',\n                                            'mmmmm',\n                                            'gonna',\n                                            'bruh',\n                                            'yeah',\n                                            'perfect',\n                                            'follow',\n                                            'sooo',\n                                            'yummi',\n                                            'hello',\n                                            'woooow',\n                                            'thank',\n                                            \"thanks\",\n                                            \"get\",\n                                            \"superb\",\n                                            \"aboslute\",\n                                            \"absolutely\",\n                                            \"cool\",\n                                            \"beautiful\",\n                                            \"maybe\",\n                                            \"damn\",\n                                            \"that\",\n                                            \"perfect\",\n                                            \"shes\",\n                                            \"yummy\",\n                                            \"yummi\",\n                                            \"lmao\",\n                                            \"yooou\",\n                                            \"magnifique\",\n                                            \"love\",\n                                            \"lovely\",\n                                            \"mmmmmmm\",\n                                            \"yoou\",\n                                            \"wooow\",\n                                            \"xoxoxo\",\n                                            \"sooooo\",\n                                            \"wowww\",\n                                            \"yaay\",\n                                            \"ohhh\",\n                                            \"hotttt\",\n                                            \"whoa\",\n                                            \"hehe\",\n                                            \"dang\",\n                                            \"xxxx\",\n                                            \"goddamn\",\n                                            \"sublime\",\n                                            \"fabulous\",\n                                            \"fuckkk\",\n                                            \"gooood\",\n                                            \"xoxo\",\n                                            \"soooooo\",\n                                            \"mmmmmm\",\n                                            \"hahaha\",\n                                            \"xxxxx\",\n                                            \"nicee\",\n                                            \"veeery\",\n                                            \"sexxxxyy\",\n                                            \"delici\",\n                                            \"sensual\",\n                                            \"greatest\",                                            \n                                            \"didnt\"\n                                            \"will\",\n                                            \"cute\",\n                                            \"kind\",\n                                            \"couldn\",\n                                            \"magnific\",\n                                            \"gostosa\",\n                                            \"deliciosa\",\n                                            \"delicia\",\n                                            \"wonderful\",\n                                            \"genuin\",\n                                            \"love\",\n                                            \"instead\",\n                                            \"huge\",\n                                            \"impress\",\n                                            \"delicioso\",\n                                            \"wish\",\n                                            \"hotter\",\n                                            \"begin\",\n                                            \"unbeliev\",\n                                            \"congratul\",\n                                            \"esta\",\n                                            \"como\",\n                                            \"boca\",\n                                            \"buen\",\n                                            \"est\",\n                                            \"quiero\",\n                                            \"youu\",\n                                            \"doesnt\",\n                                            \"beauti\",\n                                            \"bravo\",\n                                            \"brilliant\",\n                                            \"prettiest\",\n                                            \"believ\",\n                                            \"wanna\",\n                                            \"eat\",\n                                            \"astonish\",\n                                            \"yesss\",\n                                            \"worst\",\n                                            \"princess\",\n                                            \"bella\",\n                                            \"aussi\",\n                                            \"superb\",\n                                            \"woow\",\n                                            \"hilari\",\n                                            \"divin\",\n                                            \"yessss\",\n                                            \"have\",\n                                            \"goddess\",\n                                            \"hmmm\",\n                                            \"hermosa\",\n                                            \"para\",\n                                            \"pero\",\n                                            \"hermoso\",\n                                            \"insan\",\n                                            \"later\",\n                                            \"sexiest\",\n                                            \"wasn\",\n                                            \"woah\",\n                                            \"oooh\",\n                                            \"sehr\",\n                                            \"geil\",\n                                            \"damm\",\n                                            \"flawless\",\n                                            \"yall\"]))\n\n\nstemmer = SnowballStemmer(\"english\")",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5701,
    "source_hash": "9aa87942",
    "tags": [],
    "execution_start": 1614849507125,
    "cell_id": "00005-6ae27cc3-30e1-4f0f-bf65-a2cf927127b1",
    "deepnote_cell_type": "code"
   },
   "source": "data = pd.read_csv(\"ReadyForAnalysis.csv\")\ncols = data.columns\n\n# Add all of the other columns\ndata_full = pd.DataFrame(data.loc[(data.clean==True) & (data.is_eng==True), [\"title\", \n                                                                             \"views\", \n                                                                             \"up_votes\", \n                                                                             \"down_votes\", \n                                                                             \"percent\", \n                                                                             \"author\",\n                                                                             \"author_subscriber\",\n                                                                             \"categories\",\n                                                                             \"tags\",\n                                                                             \"production\",\n                                                                             \"description\",\n                                                                             \"duration\",\n                                                                             \"upload_date\",\n                                                                             \"pornstars\",\n                                                                             \"download_urls\",\n                                                                             \"thumbnail_url\",\n                                                                             \"number_of_comment\",\n                                                                             \"url\",\n                                                                             \"error\",\n                                                                             \"repeat_n_times\",\n                                                                             \"comms\",\n                                                                             \"clean\",\n                                                                             \"is_eng\"\n                                                                            ]])\n\ndata_text = data_full = pd.DataFrame(data.loc[(data.clean==True) & (data.is_eng==True), [\"comms\"]])\n\ndata_text = data_text.dropna(subset=['comms'])\ndata_text = data_text.reset_index(drop=True)\ndata_text['index'] = data_text.index\n\ndocuments = data_text",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# PT1: Data pre-processing\n    ## -Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n    ## -Words are stemmed — words are reduced to their root form. (Happiness --> Happy)\n    ## -Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n    ## -Words that have fewer than 3 characters are removed.\n    ## -All stopwords are removed.",
   "metadata": {
    "tags": [],
    "cell_id": "00007-fc2bee67-09e2-43d5-a69f-d693ace1987e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Lemmatize and stemming function",
   "metadata": {
    "tags": [],
    "cell_id": "00008-cad30623-4af1-4898-a2c5-8d585f5e0785",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 39,
    "source_hash": "99f365e9",
    "tags": [],
    "execution_start": 1614849512848,
    "cell_id": "00009-d3e0890d-e58d-4d2f-afe6-971393adbe78",
    "deepnote_cell_type": "code"
   },
   "source": "def lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in all_stopwords_gensim and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Run the function over the whole dataset",
   "metadata": {
    "tags": [],
    "cell_id": "00012-710e859d-4282-41eb-82dd-0c1fc84ce52c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 13200,
    "scrolled": true,
    "source_hash": "17083f6",
    "tags": [],
    "execution_start": 1614849514495,
    "cell_id": "00013-e560b2d3-c7bd-4b24-b777-947e254ed411",
    "deepnote_cell_type": "code"
   },
   "source": "processed_docs = documents[\"comms\"].map(preprocess)",
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 12,
     "data": {
      "text/plain": "0    [horni, anymor, want, happi]\n1                 [silenc, wench]\n2                          [feel]\n3                              []\n4                    [want, love]\nName: comms, dtype: object"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Also store the results in the full dataframe",
   "metadata": {
    "cell_id": "00014-02eea463-9a37-4691-80cf-c9358408c251",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "266ccd73",
    "execution_millis": 8,
    "execution_start": 1614849527674,
    "cell_id": "00015-b30b6655-7309-4dff-a806-07dae30a2413",
    "deepnote_cell_type": "code"
   },
   "source": "processed_coms = processed_docs\ndata_full[\"processed_coms\"] = processed_coms",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Remove empty comments",
   "metadata": {
    "cell_id": "00016-1535bac6-8a2f-43e7-aec4-33773a2e8720",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b93fa8a",
    "execution_millis": 102,
    "execution_start": 1614849527687,
    "cell_id": "00017-c65ef84c-ce4f-47f8-8f53-69bf17a57026",
    "deepnote_cell_type": "code"
   },
   "source": "print(len(data_full))\n\n#Remove empty rows\ndata_full = data_full.dropna(subset=['processed_coms'])\ndata_full = data_full.reset_index(drop=True)\n\nprint(len(data_full))\n\n#Remove empty comments\ndata_full = data_full[data_full['processed_coms'].map(lambda d: len(d)) > 0]\ndata_full = data_full.reset_index(drop=True)\n\nprint(len(data_full))\n\n#Assign the new values to processed_docs\nprocessed_docs = data_full[\"processed_coms\"]",
   "outputs": [
    {
     "name": "stdout",
     "text": "201449\n120876\n99087\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Bigrams",
   "metadata": {
    "cell_id": "00018-3673216d-4aa0-40a9-81bd-1b8228ebf087",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Forming bigrams and trigram from documents (comments) is **important in topic modelling** because certain words tend to **occur together**. For example, \"french\" refers to the *language*, \"revolution\" to the *astronomical phenomenon*, but \"french\" + \"revolution\" refers to the *historical event*.",
   "metadata": {
    "cell_id": "00019-3fa50f40-18ff-4c07-a329-f3295908eaa8",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a8184e",
    "execution_millis": 1917,
    "execution_start": 1614849527853,
    "cell_id": "00020-314cdd3d-7f63-4be3-8d06-e599e1c52964",
    "deepnote_cell_type": "code"
   },
   "source": "# Build the bigram models\nbigram = gensim.models.phrases.Phrases(processed_docs, min_count=3, threshold=10)\nbigram_mod = gensim.models.phrases.Phraser(bigram)",
   "outputs": [
    {
     "name": "stdout",
     "text": "['horni_anymor', 'want', 'happi']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2457a619",
    "execution_millis": 2201,
    "execution_start": 1614849529781,
    "cell_id": "00021-fe739884-0167-4dc5-ade8-d4a8a3df9d50",
    "deepnote_cell_type": "code"
   },
   "source": "processed_docs_bigram = {\"bigram\": []}\nfor i in range(len(processed_docs)) :\n    big = bigram[processed_docs[i]]\n    \n    processed_docs_bigram[\"bigram\"].append(big)\n\nprocessed_docs_bigram = pd.DataFrame(processed_docs_bigram)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Visualisation of the whole processed document",
   "metadata": {
    "cell_id": "00023-dec66934-730e-4cc3-9282-4b6736ac7eb6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "77bc7d2a",
    "execution_millis": 4,
    "execution_start": 1614849532220,
    "cell_id": "00026-22d31019-07b5-4769-9501-126b55b64213",
    "deepnote_cell_type": "code"
   },
   "source": "processed_docs = processed_docs_bigram[\"bigram\"]\ndata_full[\"processed_coms\"] = processed_docs",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Save the processed data",
   "metadata": {
    "cell_id": "00027-db760e36-2cf7-4b89-992b-70a379a1828e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fb3db1b2",
    "execution_millis": 351,
    "execution_start": 1614849532224,
    "cell_id": "00028-66018115-6dcb-478c-b4b2-581097bbc291",
    "deepnote_cell_type": "code"
   },
   "source": "data_full.to_csv(r'Processed_Docs_full.csv')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# PT2: Training the model",
   "metadata": {
    "tags": [],
    "cell_id": "00029-3d668894-f314-4eda-ac3d-52db3b3ac7ff",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## We create a custom dictionary based on our dataset",
   "metadata": {
    "tags": [],
    "cell_id": "00030-56ea8f14-4b3b-4416-9ed2-978eebfe4bcf",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 345,
    "source_hash": "269d300e",
    "tags": [],
    "cell_id": "00032-2b1cfc21-b9ed-4cfa-92a9-4ce23c059ba5",
    "deepnote_cell_type": "code"
   },
   "source": "dictionary = gensim.corpora.Dictionary(processed_docs)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 happi\n1 horni_anymor\n2 wanna\n3 want\n4 silenc\n5 wench\n6 feel\n7 love\n8 cook\n9 know\n10 proper\n"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### We filter out words that:\n    #### -Are present in less than 15 comments (too rare)\n    #### -In more than 50% of the comments (too numerous --> stopwords)\n    #### - After the first two steps, we only keep the first 100k tokens/words (further filtering of stopwords)",
   "metadata": {
    "tags": [],
    "cell_id": "00033-ffafd29c-63d6-4d90-bdf5-31134b36580d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 9,
    "source_hash": "b0ee12c7",
    "tags": [],
    "cell_id": "00034-e676f11f-6ae6-4a92-8f31-69caac5f9bb6",
    "deepnote_cell_type": "code"
   },
   "source": "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 166,
    "source_hash": "87bbbc7f",
    "tags": [],
    "cell_id": "00035-27b7c361-44ae-40f9-ab4b-43dda7099673",
    "deepnote_cell_type": "code"
   },
   "source": "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]",
   "outputs": [
    {
     "data": {
      "text/plain": "[(5, 1)]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Model evaluation and tuning (sensitivity analysis): coherence = C_v",
   "metadata": {
    "cell_id": "00051-688ddab6-fa7a-4bac-ba59-e9ed4fbf84bb",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2ce7c064",
    "execution_millis": 1287,
    "execution_start": 1613555894398,
    "cell_id": "00052-939ea9cd-bfbb-49d3-b977-93680a0fda0f",
    "deepnote_cell_type": "code"
   },
   "source": "#Compute coherence score for range of topics\nSensitivity_analysis = {'Topics': [],\n                 'Alpha': [],\n                 'Beta': [],\n                 'Coherence': [],\n                        'Perplexity': []\n                }\n\n#Topics(K):\ntopics_range = range(3,66,1)\n\n#Alpha:\nalpha = list(np.arange(0.01, 1, 0.3))\n\n#Beta:\nbeta = list(np.arange(0.01, 1, 0.3))\n\n#Number of passes:\nnum_passes = 2\n\n#Create training and holdout data\ntrain, test = train_test_split(processed_docs, test_size=0.2)\n\n# Training dictionary and BoW\ndictionary = gensim.corpora.Dictionary(train)\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n\nbow_corpus = [dictionary.doc2bow(doc) for doc in train]\n\n# tfidf = models.TfidfModel(bow_corpus)\n# corpus_tfidf = tfidf[bow_corpus]\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ee00c1cc",
    "execution_millis": 46201013,
    "execution_start": 1613555897580,
    "cell_id": "00053-a5f8721c-3045-4e39-9db5-5bf5534a1e0c",
    "deepnote_cell_type": "code"
   },
   "source": "#Loop over the range of values\nfor k in topics_range:\n    print(k)\n    print(\"topics out of\")\n    print(max(topics_range))\n    print(\"-------------------------------------\")\n    for a in alpha:\n        for b in beta:\n            \n            lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n                                           id2word=dictionary,\n                                           num_topics=k, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=num_passes,\n                                           alpha=a,\n                                           eta=b)\n            coherence_model_lda = CoherenceModel(model=lda_model, \n                                                       texts=test, \n                                                       dictionary=dictionary, \n                                                       coherence='c_v')\n            coherence_lda = coherence_model_lda.get_coherence()\n            \n            perplexity_lda = lda_model.log_perplexity(bow_corpus)\n            \n            Sensitivity_analysis[\"Topics\"].append(k)\n            Sensitivity_analysis[\"Alpha\"].append(a)\n            Sensitivity_analysis[\"Beta\"].append(b)\n            Sensitivity_analysis[\"Coherence\"].append(coherence_lda)\n            Sensitivity_analysis[\"Perplexity\"].append(perplexity_lda)\n            ",
   "outputs": [
    {
     "name": "stdout",
     "text": "3\ntopics out of\n65\n-------------------------------------\n4\ntopics out of\n65\n-------------------------------------\n5\ntopics out of\n65\n-------------------------------------\n6\ntopics out of\n65\n-------------------------------------\n7\ntopics out of\n65\n-------------------------------------\n8\ntopics out of\n65\n-------------------------------------\n9\ntopics out of\n65\n-------------------------------------\n10\ntopics out of\n65\n-------------------------------------\n11\ntopics out of\n65\n-------------------------------------\n12\ntopics out of\n65\n-------------------------------------\n13\ntopics out of\n65\n-------------------------------------\n14\ntopics out of\n65\n-------------------------------------\n15\ntopics out of\n65\n-------------------------------------\n16\ntopics out of\n65\n-------------------------------------\n17\ntopics out of\n65\n-------------------------------------\n18\ntopics out of\n65\n-------------------------------------\n19\ntopics out of\n65\n-------------------------------------\n20\ntopics out of\n65\n-------------------------------------\n21\ntopics out of\n65\n-------------------------------------\n22\ntopics out of\n65\n-------------------------------------\n23\ntopics out of\n65\n-------------------------------------\n24\ntopics out of\n65\n-------------------------------------\n25\ntopics out of\n65\n-------------------------------------\n26\ntopics out of\n65\n-------------------------------------\n27\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "28\ntopics out of\n65\n-------------------------------------\n29\ntopics out of\n65\n-------------------------------------\n30\ntopics out of\n65\n-------------------------------------\n31\ntopics out of\n65\n-------------------------------------\n32\ntopics out of\n65\n-------------------------------------\n33\ntopics out of\n65\n-------------------------------------\n34\ntopics out of\n65\n-------------------------------------\n35\ntopics out of\n65\n-------------------------------------\n36\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "37\ntopics out of\n65\n-------------------------------------\n38\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "39\ntopics out of\n65\n-------------------------------------\n40\ntopics out of\n65\n-------------------------------------\n41\ntopics out of\n65\n-------------------------------------\n42\ntopics out of\n65\n-------------------------------------\n43\ntopics out of\n65\n-------------------------------------\n44\ntopics out of\n65\n-------------------------------------\n45\ntopics out of\n65\n-------------------------------------\n46\ntopics out of\n65\n-------------------------------------\n47\ntopics out of\n65\n-------------------------------------\n48\ntopics out of\n65\n-------------------------------------\n49\ntopics out of\n65\n-------------------------------------\n50\ntopics out of\n65\n-------------------------------------\n51\ntopics out of\n65\n-------------------------------------\n52\ntopics out of\n65\n-------------------------------------\n53\ntopics out of\n65\n-------------------------------------\n54\ntopics out of\n65\n-------------------------------------\n55\ntopics out of\n65\n-------------------------------------\n56\ntopics out of\n65\n-------------------------------------\n57\ntopics out of\n65\n-------------------------------------\n58\ntopics out of\n65\n-------------------------------------\n59\ntopics out of\n65\n-------------------------------------\n60\ntopics out of\n65\n-------------------------------------\n61\ntopics out of\n65\n-------------------------------------\n62\ntopics out of\n65\n-------------------------------------\n63\ntopics out of\n65\n-------------------------------------\n64\ntopics out of\n65\n-------------------------------------\n65\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c7599582",
    "execution_millis": 1,
    "execution_start": 1613602098629,
    "cell_id": "00054-475ce847-0aa5-48b0-b8aa-32f003141def",
    "deepnote_cell_type": "code"
   },
   "source": "#Save the results internally\nSensitivity_analysis_pd = pd.DataFrame.from_dict(Sensitivity_analysis)\n\n#Save the results externally\nSensitivity_analysis_pd.to_csv('Sensitivity_analysis.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Additional loop for alpha symmetric",
   "metadata": {
    "tags": [],
    "cell_id": "00055-3d4e11b5-064a-4484-beb2-6e36c16e87b7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9b2e72f7",
    "execution_millis": 10969573,
    "execution_start": 1613635479905,
    "cell_id": "00056-8eb0f95f-88b3-4c0e-b3ff-3738942d2a0b",
    "deepnote_cell_type": "code"
   },
   "source": "#Compute coherence score for range of topics\nSensitivity_analysis = {'Topics': [],\n                 'Alpha': [],\n                 'Beta': [],\n                 'Coherence': [],\n                        'Perplexity': []\n                }\n\n#Topics(K):\ntopics_range = range(3,66,1)\n\n#Alpha:\nalpha = \"symmetric\"\n\n#Beta:\nbeta = list(np.arange(0.01, 1, 0.3))\n\n#Number of passes:\nnum_passes = 2\n\n#Create training and holdout data\ntrain, test = train_test_split(processed_docs, test_size=0.2)\n\n# Training dictionary and BoW\ndictionary = gensim.corpora.Dictionary(train)\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n\nbow_corpus = [dictionary.doc2bow(doc) for doc in train]\n\n# tfidf = models.TfidfModel(bow_corpus)\n# corpus_tfidf = tfidf[bow_corpus]\n\n\n#Loop over the range of values\nfor k in topics_range:\n    print(k)\n    print(\"topics out of\")\n    print(max(topics_range))\n    print(\"-------------------------------------\")\n    \n    for b in beta:\n            \n        lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n                                           id2word=dictionary,\n                                           num_topics=k, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=num_passes,\n                                           alpha=alpha,\n                                           eta=b)\n        coherence_model_lda = CoherenceModel(model=lda_model, \n                                                       texts=test, \n                                                       dictionary=dictionary, \n                                                       coherence='c_v')\n        coherence_lda = coherence_model_lda.get_coherence()\n            \n        perplexity_lda = lda_model.log_perplexity(bow_corpus)\n            \n        Sensitivity_analysis[\"Topics\"].append(k)\n        Sensitivity_analysis[\"Alpha\"].append(alpha)\n        Sensitivity_analysis[\"Beta\"].append(b)\n        Sensitivity_analysis[\"Coherence\"].append(coherence_lda)\n        Sensitivity_analysis[\"Perplexity\"].append(perplexity_lda)\n            \n            ",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n3\ntopics out of\n65\n-------------------------------------\n4\ntopics out of\n65\n-------------------------------------\n5\ntopics out of\n65\n-------------------------------------\n6\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "7\ntopics out of\n65\n-------------------------------------\n8\ntopics out of\n65\n-------------------------------------\n9\ntopics out of\n65\n-------------------------------------\n10\ntopics out of\n65\n-------------------------------------\n11\ntopics out of\n65\n-------------------------------------\n12\ntopics out of\n65\n-------------------------------------\n13\ntopics out of\n65\n-------------------------------------\n14\ntopics out of\n65\n-------------------------------------\n15\ntopics out of\n65\n-------------------------------------\n16\ntopics out of\n65\n-------------------------------------\n17\ntopics out of\n65\n-------------------------------------\n18\ntopics out of\n65\n-------------------------------------\n19\ntopics out of\n65\n-------------------------------------\n20\ntopics out of\n65\n-------------------------------------\n21\ntopics out of\n65\n-------------------------------------\n22\ntopics out of\n65\n-------------------------------------\n23\ntopics out of\n65\n-------------------------------------\n24\ntopics out of\n65\n-------------------------------------\n25\ntopics out of\n65\n-------------------------------------\n26\ntopics out of\n65\n-------------------------------------\n27\ntopics out of\n65\n-------------------------------------\n28\ntopics out of\n65\n-------------------------------------\n29\ntopics out of\n65\n-------------------------------------\n30\ntopics out of\n65\n-------------------------------------\n31\ntopics out of\n65\n-------------------------------------\n32\ntopics out of\n65\n-------------------------------------\n33\ntopics out of\n65\n-------------------------------------\n34\ntopics out of\n65\n-------------------------------------\n35\ntopics out of\n65\n-------------------------------------\n36\ntopics out of\n65\n-------------------------------------\n37\ntopics out of\n65\n-------------------------------------\n38\ntopics out of\n65\n-------------------------------------\n39\ntopics out of\n65\n-------------------------------------\n40\ntopics out of\n65\n-------------------------------------\n41\ntopics out of\n65\n-------------------------------------\n42\ntopics out of\n65\n-------------------------------------\n43\ntopics out of\n65\n-------------------------------------\n44\ntopics out of\n65\n-------------------------------------\n45\ntopics out of\n65\n-------------------------------------\n46\ntopics out of\n65\n-------------------------------------\n47\ntopics out of\n65\n-------------------------------------\n48\ntopics out of\n65\n-------------------------------------\n49\ntopics out of\n65\n-------------------------------------\n50\ntopics out of\n65\n-------------------------------------\n51\ntopics out of\n65\n-------------------------------------\n52\ntopics out of\n65\n-------------------------------------\n53\ntopics out of\n65\n-------------------------------------\n54\ntopics out of\n65\n-------------------------------------\n55\ntopics out of\n65\n-------------------------------------\n56\ntopics out of\n65\n-------------------------------------\n57\ntopics out of\n65\n-------------------------------------\n58\ntopics out of\n65\n-------------------------------------\n59\ntopics out of\n65\n-------------------------------------\n60\ntopics out of\n65\n-------------------------------------\n61\ntopics out of\n65\n-------------------------------------\n62\ntopics out of\n65\n-------------------------------------\n63\ntopics out of\n65\n-------------------------------------\n64\ntopics out of\n65\n-------------------------------------\n65\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9697cc7e",
    "execution_millis": 14,
    "execution_start": 1613646449477,
    "cell_id": "00057-02ee4f52-d406-4dc5-8117-3f3f54db55c5",
    "deepnote_cell_type": "code"
   },
   "source": "#Save the results internally\nSensitivity_analysis_pd = pd.DataFrame.from_dict(Sensitivity_analysis)\n\n#Save the results externally\nSensitivity_analysis_pd.to_csv('Sensitivity_analysis_ALPHAsymmetric_3-65.csv', index=False)",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Alpha asymmetric",
   "metadata": {
    "tags": [],
    "cell_id": "00058-2d28e1f7-1f16-4389-9c0a-8af495cb2cc6",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4b44d1b7",
    "execution_millis": 11148223,
    "execution_start": 1613646449477,
    "cell_id": "00059-5418b2f2-62ef-4b75-91fd-bcda5445ff34",
    "deepnote_cell_type": "code"
   },
   "source": "#Compute coherence score for range of topics\nSensitivity_analysis = {'Topics': [],\n                 'Alpha': [],\n                 'Beta': [],\n                 'Coherence': [],\n                        'Perplexity': []\n                }\n\n#Topics(K):\ntopics_range = range(3,66,1)\n\n#Alpha:\nalpha = \"asymmetric\"\n\n#Beta:\nbeta = list(np.arange(0.01, 1, 0.3))\n\n#Number of passes:\nnum_passes = 2\n\n#Create training and holdout data\ntrain, test = train_test_split(processed_docs, test_size=0.2)\n\n# Training dictionary and BoW\ndictionary = gensim.corpora.Dictionary(train)\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n\nbow_corpus = [dictionary.doc2bow(doc) for doc in train]\n\n# tfidf = models.TfidfModel(bow_corpus)\n# corpus_tfidf = tfidf[bow_corpus]\n\n\n#Loop over the range of values\nfor k in topics_range:\n    print(k)\n    print(\"topics out of\")\n    print(max(topics_range))\n    print(\"-------------------------------------\")\n    \n    for b in beta:\n            \n        lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n                                           id2word=dictionary,\n                                           num_topics=k, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=num_passes,\n                                           alpha=alpha,\n                                           eta=b)\n        coherence_model_lda = CoherenceModel(model=lda_model, \n                                                       texts=test, \n                                                       dictionary=dictionary, \n                                                       coherence='c_v')\n        coherence_lda = coherence_model_lda.get_coherence()\n            \n        perplexity_lda = lda_model.log_perplexity(bow_corpus)\n            \n        Sensitivity_analysis[\"Topics\"].append(k)\n        Sensitivity_analysis[\"Alpha\"].append(alpha)\n        Sensitivity_analysis[\"Beta\"].append(b)\n        Sensitivity_analysis[\"Coherence\"].append(coherence_lda)\n        Sensitivity_analysis[\"Perplexity\"].append(perplexity_lda)\n            ",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n3\ntopics out of\n65\n-------------------------------------\n4\ntopics out of\n65\n-------------------------------------\n5\ntopics out of\n65\n-------------------------------------\n6\ntopics out of\n65\n-------------------------------------\n7\ntopics out of\n65\n-------------------------------------\n8\ntopics out of\n65\n-------------------------------------\n9\ntopics out of\n65\n-------------------------------------\n10\ntopics out of\n65\n-------------------------------------\n11\ntopics out of\n65\n-------------------------------------\n12\ntopics out of\n65\n-------------------------------------\n13\ntopics out of\n65\n-------------------------------------\n14\ntopics out of\n65\n-------------------------------------\n15\ntopics out of\n65\n-------------------------------------\n16\ntopics out of\n65\n-------------------------------------\n17\ntopics out of\n65\n-------------------------------------\n18\ntopics out of\n65\n-------------------------------------\n19\ntopics out of\n65\n-------------------------------------\n20\ntopics out of\n65\n-------------------------------------\n21\ntopics out of\n65\n-------------------------------------\n22\ntopics out of\n65\n-------------------------------------\n23\ntopics out of\n65\n-------------------------------------\n24\ntopics out of\n65\n-------------------------------------\n25\ntopics out of\n65\n-------------------------------------\n26\ntopics out of\n65\n-------------------------------------\n27\ntopics out of\n65\n-------------------------------------\n28\ntopics out of\n65\n-------------------------------------\n29\ntopics out of\n65\n-------------------------------------\n30\ntopics out of\n65\n-------------------------------------\n31\ntopics out of\n65\n-------------------------------------\n32\ntopics out of\n65\n-------------------------------------\n33\ntopics out of\n65\n-------------------------------------\n34\ntopics out of\n65\n-------------------------------------\n35\ntopics out of\n65\n-------------------------------------\n36\ntopics out of\n65\n-------------------------------------\n37\ntopics out of\n65\n-------------------------------------\n38\ntopics out of\n65\n-------------------------------------\n39\ntopics out of\n65\n-------------------------------------\n40\ntopics out of\n65\n-------------------------------------\n41\ntopics out of\n65\n-------------------------------------\n42\ntopics out of\n65\n-------------------------------------\n43\ntopics out of\n65\n-------------------------------------\n44\ntopics out of\n65\n-------------------------------------\n45\ntopics out of\n65\n-------------------------------------\n46\ntopics out of\n65\n-------------------------------------\n47\ntopics out of\n65\n-------------------------------------\n48\ntopics out of\n65\n-------------------------------------\n49\ntopics out of\n65\n-------------------------------------\n50\ntopics out of\n65\n-------------------------------------\n51\ntopics out of\n65\n-------------------------------------\n52\ntopics out of\n65\n-------------------------------------\n53\ntopics out of\n65\n-------------------------------------\n54\ntopics out of\n65\n-------------------------------------\n55\ntopics out of\n65\n-------------------------------------\n56\ntopics out of\n65\n-------------------------------------\n57\ntopics out of\n65\n-------------------------------------\n58\ntopics out of\n65\n-------------------------------------\n59\ntopics out of\n65\n-------------------------------------\n60\ntopics out of\n65\n-------------------------------------\n61\ntopics out of\n65\n-------------------------------------\n62\ntopics out of\n65\n-------------------------------------\n63\ntopics out of\n65\n-------------------------------------\n64\ntopics out of\n65\n-------------------------------------\n65\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n3\ntopics out of\n65\n-------------------------------------\n4\ntopics out of\n65\n-------------------------------------\n5\ntopics out of\n65\n-------------------------------------\n6\ntopics out of\n65\n-------------------------------------\n7\ntopics out of\n65\n-------------------------------------\n8\ntopics out of\n65\n-------------------------------------\n9\ntopics out of\n65\n-------------------------------------\n10\ntopics out of\n65\n-------------------------------------\n11\ntopics out of\n65\n-------------------------------------\n12\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "13\ntopics out of\n65\n-------------------------------------\n14\ntopics out of\n65\n-------------------------------------\n15\ntopics out of\n65\n-------------------------------------\n16\ntopics out of\n65\n-------------------------------------\n17\ntopics out of\n65\n-------------------------------------\n18\ntopics out of\n65\n-------------------------------------\n19\ntopics out of\n65\n-------------------------------------\n20\ntopics out of\n65\n-------------------------------------\n21\ntopics out of\n65\n-------------------------------------\n22\ntopics out of\n65\n-------------------------------------\n23\ntopics out of\n65\n-------------------------------------\n24\ntopics out of\n65\n-------------------------------------\n25\ntopics out of\n65\n-------------------------------------\n26\ntopics out of\n65\n-------------------------------------\n27\ntopics out of\n65\n-------------------------------------\n28\ntopics out of\n65\n-------------------------------------\n29\ntopics out of\n65\n-------------------------------------\n30\ntopics out of\n65\n-------------------------------------\n31\ntopics out of\n65\n-------------------------------------\n32\ntopics out of\n65\n-------------------------------------\n33\ntopics out of\n65\n-------------------------------------\n34\ntopics out of\n65\n-------------------------------------\n35\ntopics out of\n65\n-------------------------------------\n36\ntopics out of\n65\n-------------------------------------\n37\ntopics out of\n65\n-------------------------------------\n38\ntopics out of\n65\n-------------------------------------\n39\ntopics out of\n65\n-------------------------------------\n40\ntopics out of\n65\n-------------------------------------\n41\ntopics out of\n65\n-------------------------------------\n42\ntopics out of\n65\n-------------------------------------\n43\ntopics out of\n65\n-------------------------------------\n44\ntopics out of\n65\n-------------------------------------\n45\ntopics out of\n65\n-------------------------------------\n46\ntopics out of\n65\n-------------------------------------\n47\ntopics out of\n65\n-------------------------------------\n48\ntopics out of\n65\n-------------------------------------\n49\ntopics out of\n65\n-------------------------------------\n50\ntopics out of\n65\n-------------------------------------\n51\ntopics out of\n65\n-------------------------------------\n52\ntopics out of\n65\n-------------------------------------\n53\ntopics out of\n65\n-------------------------------------\n54\ntopics out of\n65\n-------------------------------------\n55\ntopics out of\n65\n-------------------------------------\n56\ntopics out of\n65\n-------------------------------------\n57\ntopics out of\n65\n-------------------------------------\n58\ntopics out of\n65\n-------------------------------------\n59\ntopics out of\n65\n-------------------------------------\n60\ntopics out of\n65\n-------------------------------------\n61\ntopics out of\n65\n-------------------------------------\n62\ntopics out of\n65\n-------------------------------------\n63\ntopics out of\n65\n-------------------------------------\n64\ntopics out of\n65\n-------------------------------------\n65\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "495b2b8d",
    "execution_millis": 12,
    "execution_start": 1613657597699,
    "cell_id": "00060-bdfd44e8-bd41-404d-bcdd-e42f7e042660",
    "deepnote_cell_type": "code"
   },
   "source": "#Save the results internally\nSensitivity_analysis_pd = pd.DataFrame.from_dict(Sensitivity_analysis)\n\n#Save the results externally\nSensitivity_analysis_pd.to_csv('Sensitivity_analysis_ALPHAasymmetric_3-65.csv', index=False)",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Alpha and Beta symmetric",
   "metadata": {
    "tags": [],
    "cell_id": "00061-b9c933c1-3d57-4643-a66c-94db64e44704",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a840181d",
    "execution_millis": 2571861,
    "execution_start": 1613657597699,
    "cell_id": "00062-a7f92755-0b57-4286-a98f-a0289ad36313",
    "deepnote_cell_type": "code"
   },
   "source": "#Compute coherence score for range of topics\nSensitivity_analysis = {'Topics': [],\n                 'Alpha': [],\n                 'Beta': [],\n                 'Coherence': [],\n                        'Perplexity': []\n                }\n\n#Topics(K):\ntopics_range = range(3,66,1)\n\n#Alpha:\nalpha = \"symmetric\"\n\n#Beta:\nbeta = \"symmetric\"\n\n#Number of passes:\nnum_passes = 2\n\n#Create training and holdout data\ntrain, test = train_test_split(processed_docs, test_size=0.2)\n\n# Training dictionary and BoW\ndictionary = gensim.corpora.Dictionary(train)\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n\nbow_corpus = [dictionary.doc2bow(doc) for doc in train]\n\n# tfidf = models.TfidfModel(bow_corpus)\n# corpus_tfidf = tfidf[bow_corpus]\n\n\n#Loop over the range of values\nfor k in topics_range:\n    print(k)\n    print(\"topics out of\")\n    print(max(topics_range))\n    print(\"-------------------------------------\")\n            \n    lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n                                           id2word=dictionary,\n                                           num_topics=k, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=num_passes,\n                                           alpha=alpha,\n                                           eta=beta)\n    coherence_model_lda = CoherenceModel(model=lda_model, \n                                                       texts=test, \n                                                       dictionary=dictionary, \n                                                       coherence='c_v')\n    coherence_lda = coherence_model_lda.get_coherence()\n            \n    perplexity_lda = lda_model.log_perplexity(bow_corpus)\n            \n    Sensitivity_analysis[\"Topics\"].append(k)\n    Sensitivity_analysis[\"Alpha\"].append(alpha)\n    Sensitivity_analysis[\"Beta\"].append(beta)\n    Sensitivity_analysis[\"Coherence\"].append(coherence_lda)\n    Sensitivity_analysis[\"Perplexity\"].append(perplexity_lda)\n            ",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n3\ntopics out of\n65\n-------------------------------------\n4\ntopics out of\n65\n-------------------------------------\n5\ntopics out of\n65\n-------------------------------------\n6\ntopics out of\n65\n-------------------------------------\n7\ntopics out of\n65\n-------------------------------------\n8\ntopics out of\n65\n-------------------------------------\n9\ntopics out of\n65\n-------------------------------------\n10\ntopics out of\n65\n-------------------------------------\n11\ntopics out of\n65\n-------------------------------------\n12\ntopics out of\n65\n-------------------------------------\n13\ntopics out of\n65\n-------------------------------------\n14\ntopics out of\n65\n-------------------------------------\n15\ntopics out of\n65\n-------------------------------------\n16\ntopics out of\n65\n-------------------------------------\n17\ntopics out of\n65\n-------------------------------------\n18\ntopics out of\n65\n-------------------------------------\n19\ntopics out of\n65\n-------------------------------------\n20\ntopics out of\n65\n-------------------------------------\n21\ntopics out of\n65\n-------------------------------------\n22\ntopics out of\n65\n-------------------------------------\n23\ntopics out of\n65\n-------------------------------------\n24\ntopics out of\n65\n-------------------------------------\n25\ntopics out of\n65\n-------------------------------------\n26\ntopics out of\n65\n-------------------------------------\n27\ntopics out of\n65\n-------------------------------------\n28\ntopics out of\n65\n-------------------------------------\n29\ntopics out of\n65\n-------------------------------------\n30\ntopics out of\n65\n-------------------------------------\n31\ntopics out of\n65\n-------------------------------------\n32\ntopics out of\n65\n-------------------------------------\n33\ntopics out of\n65\n-------------------------------------\n34\ntopics out of\n65\n-------------------------------------\n35\ntopics out of\n65\n-------------------------------------\n36\ntopics out of\n65\n-------------------------------------\n37\ntopics out of\n65\n-------------------------------------\n38\ntopics out of\n65\n-------------------------------------\n39\ntopics out of\n65\n-------------------------------------\n40\ntopics out of\n65\n-------------------------------------\n41\ntopics out of\n65\n-------------------------------------\n42\ntopics out of\n65\n-------------------------------------\n43\ntopics out of\n65\n-------------------------------------\n44\ntopics out of\n65\n-------------------------------------\n45\ntopics out of\n65\n-------------------------------------\n46\ntopics out of\n65\n-------------------------------------\n47\ntopics out of\n65\n-------------------------------------\n48\ntopics out of\n65\n-------------------------------------\n49\ntopics out of\n65\n-------------------------------------\n50\ntopics out of\n65\n-------------------------------------\n51\ntopics out of\n65\n-------------------------------------\n52\ntopics out of\n65\n-------------------------------------\n53\ntopics out of\n65\n-------------------------------------\n54\ntopics out of\n65\n-------------------------------------\n55\ntopics out of\n65\n-------------------------------------\n56\ntopics out of\n65\n-------------------------------------\n57\ntopics out of\n65\n-------------------------------------\n58\ntopics out of\n65\n-------------------------------------\n59\ntopics out of\n65\n-------------------------------------\n60\ntopics out of\n65\n-------------------------------------\n61\ntopics out of\n65\n-------------------------------------\n62\ntopics out of\n65\n-------------------------------------\n63\ntopics out of\n65\n-------------------------------------\n64\ntopics out of\n65\n-------------------------------------\n65\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n3\ntopics out of\n65\n-------------------------------------\n4\ntopics out of\n65\n-------------------------------------\n5\ntopics out of\n65\n-------------------------------------\n6\ntopics out of\n65\n-------------------------------------\n7\ntopics out of\n65\n-------------------------------------\n8\ntopics out of\n65\n-------------------------------------\n9\ntopics out of\n65\n-------------------------------------\n10\ntopics out of\n65\n-------------------------------------\n11\ntopics out of\n65\n-------------------------------------\n12\ntopics out of\n65\n-------------------------------------\n13\ntopics out of\n65\n-------------------------------------\n14\ntopics out of\n65\n-------------------------------------\n15\ntopics out of\n65\n-------------------------------------\n16\ntopics out of\n65\n-------------------------------------\n17\ntopics out of\n65\n-------------------------------------\n18\ntopics out of\n65\n-------------------------------------\n19\ntopics out of\n65\n-------------------------------------\n20\ntopics out of\n65\n-------------------------------------\n21\ntopics out of\n65\n-------------------------------------\n22\ntopics out of\n65\n-------------------------------------\n23\ntopics out of\n65\n-------------------------------------\n24\ntopics out of\n65\n-------------------------------------\n25\ntopics out of\n65\n-------------------------------------\n26\ntopics out of\n65\n-------------------------------------\n27\ntopics out of\n65\n-------------------------------------\n28\ntopics out of\n65\n-------------------------------------\n29\ntopics out of\n65\n-------------------------------------\n30\ntopics out of\n65\n-------------------------------------\n31\ntopics out of\n65\n-------------------------------------\n32\ntopics out of\n65\n-------------------------------------\n33\ntopics out of\n65\n-------------------------------------\n34\ntopics out of\n65\n-------------------------------------\n35\ntopics out of\n65\n-------------------------------------\n36\ntopics out of\n65\n-------------------------------------\n37\ntopics out of\n65\n-------------------------------------\n38\ntopics out of\n65\n-------------------------------------\n39\ntopics out of\n65\n-------------------------------------\n40\ntopics out of\n65\n-------------------------------------\n41\ntopics out of\n65\n-------------------------------------\n42\ntopics out of\n65\n-------------------------------------\n43\ntopics out of\n65\n-------------------------------------\n44\ntopics out of\n65\n-------------------------------------\n45\ntopics out of\n65\n-------------------------------------\n46\ntopics out of\n65\n-------------------------------------\n47\ntopics out of\n65\n-------------------------------------\n48\ntopics out of\n65\n-------------------------------------\n49\ntopics out of\n65\n-------------------------------------\n50\ntopics out of\n65\n-------------------------------------\n51\ntopics out of\n65\n-------------------------------------\n52\ntopics out of\n65\n-------------------------------------\n53\ntopics out of\n65\n-------------------------------------\n54\ntopics out of\n65\n-------------------------------------\n55\ntopics out of\n65\n-------------------------------------\n56\ntopics out of\n65\n-------------------------------------\n57\ntopics out of\n65\n-------------------------------------\n58\ntopics out of\n65\n-------------------------------------\n59\ntopics out of\n65\n-------------------------------------\n60\ntopics out of\n65\n-------------------------------------\n61\ntopics out of\n65\n-------------------------------------\n62\ntopics out of\n65\n-------------------------------------\n63\ntopics out of\n65\n-------------------------------------\n64\ntopics out of\n65\n-------------------------------------\n65\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7f9214f7",
    "execution_millis": 10,
    "execution_start": 1613660169559,
    "cell_id": "00063-5966b014-1a51-481d-bdf9-6a8ab2910d7b",
    "deepnote_cell_type": "code"
   },
   "source": "#Save the results internally\nSensitivity_analysis_pd = pd.DataFrame.from_dict(Sensitivity_analysis)\n\n#Save the results externally\nSensitivity_analysis_pd.to_csv('Sensitivity_analysis_symmetric_3-65.csv', index=False)",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Beta symmetric",
   "metadata": {
    "tags": [],
    "cell_id": "00064-d1b1c01e-31c4-469b-92c2-a47ced27ce11",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e07eb29c",
    "execution_millis": 10810350,
    "execution_start": 1613660169559,
    "cell_id": "00065-8216799c-937d-4f5e-b697-40fbdb4453cf",
    "deepnote_cell_type": "code"
   },
   "source": "#Compute coherence score for range of topics\nSensitivity_analysis = {'Topics': [],\n                 'Alpha': [],\n                 'Beta': [],\n                 'Coherence': [],\n                        'Perplexity': []\n                }\n\n#Topics(K):\ntopics_range = range(3,66,1)\n\n#Alpha:\nalpha = list(np.arange(0.01, 1, 0.3))\n\n#Beta:\nbeta = \"symmetric\"\n\n#Number of passes:\nnum_passes = 2\n\n#Create training and holdout data\ntrain, test = train_test_split(processed_docs, test_size=0.2)\n\n# Training dictionary and BoW\ndictionary = gensim.corpora.Dictionary(train)\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n\nbow_corpus = [dictionary.doc2bow(doc) for doc in train]\n\n# tfidf = models.TfidfModel(bow_corpus)\n# corpus_tfidf = tfidf[bow_corpus]\n\n\n#Loop over the range of values\nfor k in topics_range:\n    print(k)\n    print(\"topics out of\")\n    print(max(topics_range))\n    print(\"-------------------------------------\")\n    for a in alpha:\n            \n        lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n                                           id2word=dictionary,\n                                           num_topics=k, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=num_passes,\n                                           alpha=a,\n                                           eta=beta)\n        coherence_model_lda = CoherenceModel(model=lda_model, \n                                                       texts=test, \n                                                       dictionary=dictionary, \n                                                       coherence='c_v')\n        coherence_lda = coherence_model_lda.get_coherence()\n            \n        perplexity_lda = lda_model.log_perplexity(bow_corpus)\n            \n        Sensitivity_analysis[\"Topics\"].append(k)\n        Sensitivity_analysis[\"Alpha\"].append(a)\n        Sensitivity_analysis[\"Beta\"].append(beta)\n        Sensitivity_analysis[\"Coherence\"].append(coherence_lda)\n        Sensitivity_analysis[\"Perplexity\"].append(perplexity_lda)\n            \n            ",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n3\ntopics out of\n65\n-------------------------------------\n4\ntopics out of\n65\n-------------------------------------\n5\ntopics out of\n65\n-------------------------------------\n6\ntopics out of\n65\n-------------------------------------\n7\ntopics out of\n65\n-------------------------------------\n8\ntopics out of\n65\n-------------------------------------\n9\ntopics out of\n65\n-------------------------------------\n10\ntopics out of\n65\n-------------------------------------\n11\ntopics out of\n65\n-------------------------------------\n12\ntopics out of\n65\n-------------------------------------\n13\ntopics out of\n65\n-------------------------------------\n14\ntopics out of\n65\n-------------------------------------\n15\ntopics out of\n65\n-------------------------------------\n16\ntopics out of\n65\n-------------------------------------\n17\ntopics out of\n65\n-------------------------------------\n18\ntopics out of\n65\n-------------------------------------\n19\ntopics out of\n65\n-------------------------------------\n20\ntopics out of\n65\n-------------------------------------\n21\ntopics out of\n65\n-------------------------------------\n22\ntopics out of\n65\n-------------------------------------\n23\ntopics out of\n65\n-------------------------------------\n24\ntopics out of\n65\n-------------------------------------\n25\ntopics out of\n65\n-------------------------------------\n26\ntopics out of\n65\n-------------------------------------\n27\ntopics out of\n65\n-------------------------------------\n28\ntopics out of\n65\n-------------------------------------\n29\ntopics out of\n65\n-------------------------------------\n30\ntopics out of\n65\n-------------------------------------\n31\ntopics out of\n65\n-------------------------------------\n32\ntopics out of\n65\n-------------------------------------\n33\ntopics out of\n65\n-------------------------------------\n34\ntopics out of\n65\n-------------------------------------\n35\ntopics out of\n65\n-------------------------------------\n36\ntopics out of\n65\n-------------------------------------\n37\ntopics out of\n65\n-------------------------------------\n38\ntopics out of\n65\n-------------------------------------\n39\ntopics out of\n65\n-------------------------------------\n40\ntopics out of\n65\n-------------------------------------\n41\ntopics out of\n65\n-------------------------------------\n42\ntopics out of\n65\n-------------------------------------\n43\ntopics out of\n65\n-------------------------------------\n44\ntopics out of\n65\n-------------------------------------\n45\ntopics out of\n65\n-------------------------------------\n46\ntopics out of\n65\n-------------------------------------\n47\ntopics out of\n65\n-------------------------------------\n48\ntopics out of\n65\n-------------------------------------\n49\ntopics out of\n65\n-------------------------------------\n50\ntopics out of\n65\n-------------------------------------\n51\ntopics out of\n65\n-------------------------------------\n52\ntopics out of\n65\n-------------------------------------\n53\ntopics out of\n65\n-------------------------------------\n54\ntopics out of\n65\n-------------------------------------\n55\ntopics out of\n65\n-------------------------------------\n56\ntopics out of\n65\n-------------------------------------\n57\ntopics out of\n65\n-------------------------------------\n58\ntopics out of\n65\n-------------------------------------\n59\ntopics out of\n65\n-------------------------------------\n60\ntopics out of\n65\n-------------------------------------\n61\ntopics out of\n65\n-------------------------------------\n62\ntopics out of\n65\n-------------------------------------\n63\ntopics out of\n65\n-------------------------------------\n64\ntopics out of\n65\n-------------------------------------\n65\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n3\ntopics out of\n65\n-------------------------------------\n4\ntopics out of\n65\n-------------------------------------\n5\ntopics out of\n65\n-------------------------------------\n6\ntopics out of\n65\n-------------------------------------\n7\ntopics out of\n65\n-------------------------------------\n8\ntopics out of\n65\n-------------------------------------\n9\ntopics out of\n65\n-------------------------------------\n10\ntopics out of\n65\n-------------------------------------\n11\ntopics out of\n65\n-------------------------------------\n12\ntopics out of\n65\n-------------------------------------\n13\ntopics out of\n65\n-------------------------------------\n14\ntopics out of\n65\n-------------------------------------\n15\ntopics out of\n65\n-------------------------------------\n16\ntopics out of\n65\n-------------------------------------\n17\ntopics out of\n65\n-------------------------------------\n18\ntopics out of\n65\n-------------------------------------\n19\ntopics out of\n65\n-------------------------------------\n20\ntopics out of\n65\n-------------------------------------\n21\ntopics out of\n65\n-------------------------------------\n22\ntopics out of\n65\n-------------------------------------\n23\ntopics out of\n65\n-------------------------------------\n24\ntopics out of\n65\n-------------------------------------\n25\ntopics out of\n65\n-------------------------------------\n26\ntopics out of\n65\n-------------------------------------\n27\ntopics out of\n65\n-------------------------------------\n28\ntopics out of\n65\n-------------------------------------\n29\ntopics out of\n65\n-------------------------------------\n30\ntopics out of\n65\n-------------------------------------\n31\ntopics out of\n65\n-------------------------------------\n32\ntopics out of\n65\n-------------------------------------\n33\ntopics out of\n65\n-------------------------------------\n34\ntopics out of\n65\n-------------------------------------\n35\ntopics out of\n65\n-------------------------------------\n36\ntopics out of\n65\n-------------------------------------\n37\ntopics out of\n65\n-------------------------------------\n38\ntopics out of\n65\n-------------------------------------\n39\ntopics out of\n65\n-------------------------------------\n40\ntopics out of\n65\n-------------------------------------\n41\ntopics out of\n65\n-------------------------------------\n42\ntopics out of\n65\n-------------------------------------\n43\ntopics out of\n65\n-------------------------------------\n44\ntopics out of\n65\n-------------------------------------\n45\ntopics out of\n65\n-------------------------------------\n46\ntopics out of\n65\n-------------------------------------\n47\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "48\ntopics out of\n65\n-------------------------------------\n49\ntopics out of\n65\n-------------------------------------\n50\ntopics out of\n65\n-------------------------------------\n51\ntopics out of\n65\n-------------------------------------\n52\ntopics out of\n65\n-------------------------------------\n53\ntopics out of\n65\n-------------------------------------\n54\ntopics out of\n65\n-------------------------------------\n55\ntopics out of\n65\n-------------------------------------\n56\ntopics out of\n65\n-------------------------------------\n57\ntopics out of\n65\n-------------------------------------\n58\ntopics out of\n65\n-------------------------------------\n59\ntopics out of\n65\n-------------------------------------\n60\ntopics out of\n65\n-------------------------------------\n61\ntopics out of\n65\n-------------------------------------\n62\ntopics out of\n65\n-------------------------------------\n63\ntopics out of\n65\n-------------------------------------\n64\ntopics out of\n65\n-------------------------------------\n65\ntopics out of\n65\n-------------------------------------\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1b05b659",
    "execution_millis": 42,
    "execution_start": 1613670979908,
    "cell_id": "00066-67b93fbe-dcff-4fd5-b2b8-52d951b06182",
    "deepnote_cell_type": "code"
   },
   "source": "#Save the results internally\nSensitivity_analysis_pd = pd.DataFrame.from_dict(Sensitivity_analysis)\n\n#Save the results externally\nSensitivity_analysis_pd.to_csv('Sensitivity_analysis_BETAsymmetric_3-65.csv', index=False)",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Generate best model based on sensitivity analysis",
   "metadata": {
    "cell_id": "00069-7ccad078-b5d2-47e4-a052-0baa7632df2b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb2ad651",
    "execution_millis": 216447,
    "execution_start": 1614409735934,
    "cell_id": "00070-9fb5ce2f-25a2-4fb2-8c55-2a403d820f27",
    "deepnote_cell_type": "code"
   },
   "source": "# Dictionary and BoW\ndictionary = gensim.corpora.Dictionary(processed_docs)\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n\ncorpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n\nk = 57\nnum_passes = 10\na = 0.01\nb = 0.61\n\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                           id2word=dictionary,\n                                           num_topics=k, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=num_passes,\n                                           alpha=a,\n                                           eta=b)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": true,
    "source_hash": "20361617",
    "cell_id": "00075-bd6b8657-75d3-48d8-bf64-760f55a0412e",
    "deepnote_cell_type": "code"
   },
   "source": "# plt.plot(Sensitivity_analysis_pd[\"Topics\"], Sensitivity_analysis_pd[\"Coherence\"])\n# plt.xlabel('K topics')\n# plt.ylabel('Coherence score (C_v)')\n# plt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Save model",
   "metadata": {
    "cell_id": "00077-f990c75f-28eb-46a3-818b-276000bd3e9b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f9dfc238",
    "execution_millis": 1,
    "execution_start": 1614410092602,
    "cell_id": "00078-a87bb805-2809-470a-90db-94450e443eed",
    "deepnote_cell_type": "code"
   },
   "source": "#Save model to disk (no need to use pickle module)\nlda_model.save('lda_model.model')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## To load the model",
   "metadata": {
    "cell_id": "00079-c3406acf-da9c-48d7-8dbc-f1f7baa7f142",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": true,
    "source_hash": "a4381231",
    "cell_id": "00080-98974172-0c1d-44c0-93d5-92eceb5f62c5",
    "deepnote_cell_type": "code"
   },
   "source": "# # lda_model_tfidf =  models.LdaModel.load('lda_model_tfidf.model')\n\n# #To show topics\n# lda_visualization = pyLDAvis.gensim.prepare(lda_model_tfidf, corpus_tfidf, dictionary, sort_topics=False)\n\n# pyLDAvis.display(lda_visualization)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# PT3: Topic labelling",
   "metadata": {
    "cell_id": "00081-4339c8a8-aa23-4b85-9f80-67d7fb457d7b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Record the distance between topics",
   "metadata": {
    "cell_id": "00099-40a688cf-4015-4e96-b9ad-6d8f9124d75c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Record the words in each topic",
   "metadata": {
    "cell_id": "00100-37416968-bbef-48ec-9d86-35f63ab266fc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f3bc44b4",
    "execution_millis": 66,
    "execution_start": 1614090716096,
    "cell_id": "00101-5ce0e9de-4ee1-4232-931d-ce7e94ec1a7c",
    "deepnote_cell_type": "code"
   },
   "source": "num_topics = 57\nnum_words = 20\n\nWords_in_topics = {'Topics': [],\n                 'Words': [],\n                 'Probability': []}\n\nwords = lda_model.show_topics(formatted=False,num_words=num_words, num_topics = num_topics)\n# words\n\nfor i in range(num_topics):\n    for n in range(num_words):\n        word_topic = words[i][1][n][0]\n        word_prob = words[i][1][n][1]\n\n        Words_in_topics[\"Topics\"].append(i)\n        Words_in_topics[\"Words\"].append(word_topic)\n        Words_in_topics[\"Probability\"].append(word_prob)\n\nWords_in_topics = pd.DataFrame(Words_in_topics)",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 18,
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "row_count": 1140,
       "column_count": 3,
       "columns": [
        {
         "name": "Topics",
         "dtype": "int64",
         "stats": {
          "unique_count": 57,
          "nan_count": 0,
          "min": 0,
          "max": 56,
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 5.6,
            "count": 120
           },
           {
            "bin_start": 5.6,
            "bin_end": 11.2,
            "count": 120
           },
           {
            "bin_start": 11.2,
            "bin_end": 16.799999999999997,
            "count": 100
           },
           {
            "bin_start": 16.799999999999997,
            "bin_end": 22.4,
            "count": 120
           },
           {
            "bin_start": 22.4,
            "bin_end": 28,
            "count": 100
           },
           {
            "bin_start": 28,
            "bin_end": 33.599999999999994,
            "count": 120
           },
           {
            "bin_start": 33.599999999999994,
            "bin_end": 39.199999999999996,
            "count": 120
           },
           {
            "bin_start": 39.199999999999996,
            "bin_end": 44.8,
            "count": 100
           },
           {
            "bin_start": 44.8,
            "bin_end": 50.4,
            "count": 120
           },
           {
            "bin_start": 50.4,
            "bin_end": 56,
            "count": 120
           }
          ]
         }
        },
        {
         "name": "Words",
         "dtype": "object",
         "stats": {
          "unique_count": 1122,
          "nan_count": 0,
          "categories": [
           {
            "name": "tinna",
            "count": 11
           },
           {
            "name": "chessi",
            "count": 3
           },
           {
            "name": "1120 others",
            "count": 1126
           }
          ]
         }
        },
        {
         "name": "Probability",
         "dtype": "float64",
         "stats": {
          "unique_count": 1134,
          "nan_count": 0,
          "min": 0.00013295520329847932,
          "max": 0.8161613345146179,
          "histogram": [
           {
            "bin_start": 0.00013295520329847932,
            "bin_end": 0.08173579313443043,
            "count": 1048
           },
           {
            "bin_start": 0.08173579313443043,
            "bin_end": 0.16333863106556237,
            "count": 56
           },
           {
            "bin_start": 0.16333863106556237,
            "bin_end": 0.24494146899669433,
            "count": 16
           },
           {
            "bin_start": 0.24494146899669433,
            "bin_end": 0.32654430692782627,
            "count": 5
           },
           {
            "bin_start": 0.32654430692782627,
            "bin_end": 0.4081471448589582,
            "count": 7
           },
           {
            "bin_start": 0.4081471448589582,
            "bin_end": 0.4897499827900902,
            "count": 2
           },
           {
            "bin_start": 0.4897499827900902,
            "bin_end": 0.5713528207212221,
            "count": 3
           },
           {
            "bin_start": 0.5713528207212221,
            "bin_end": 0.652955658652354,
            "count": 1
           },
           {
            "bin_start": 0.652955658652354,
            "bin_end": 0.734558496583486,
            "count": 0
           },
           {
            "bin_start": 0.734558496583486,
            "bin_end": 0.8161613345146179,
            "count": 2
           }
          ]
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "rows_top": [
        {
         "Topics": 0,
         "Words": "fuck",
         "Probability": 0.7425365447998047,
         "_deepnote_index_column": 0
        },
        {
         "Topics": 0,
         "Words": "comment",
         "Probability": 0.08593292534351349,
         "_deepnote_index_column": 1
        },
        {
         "Topics": 0,
         "Words": "bitch",
         "Probability": 0.02391643635928631,
         "_deepnote_index_column": 2
        },
        {
         "Topics": 0,
         "Words": "piec",
         "Probability": 0.004984482191503048,
         "_deepnote_index_column": 3
        },
        {
         "Topics": 0,
         "Words": "stupid",
         "Probability": 0.004158255644142628,
         "_deepnote_index_column": 4
        },
        {
         "Topics": 0,
         "Words": "tinna",
         "Probability": 0.0023873145692050457,
         "_deepnote_index_column": 5
        },
        {
         "Topics": 0,
         "Words": "christ",
         "Probability": 0.00226391339674592,
         "_deepnote_index_column": 6
        },
        {
         "Topics": 0,
         "Words": "samantha",
         "Probability": 0.0015407621394842863,
         "_deepnote_index_column": 7
        },
        {
         "Topics": 0,
         "Words": "chessi",
         "Probability": 0.0009838673286139965,
         "_deepnote_index_column": 8
        },
        {
         "Topics": 0,
         "Words": "nika",
         "Probability": 0.0008650522795505822,
         "_deepnote_index_column": 9
        },
        {
         "Topics": 0,
         "Words": "nowaday",
         "Probability": 0.0006627488764934242,
         "_deepnote_index_column": 10
        },
        {
         "Topics": 0,
         "Words": "jedi",
         "Probability": 0.0005714218132197857,
         "_deepnote_index_column": 11
        },
        {
         "Topics": 0,
         "Words": "machin",
         "Probability": 0.0005691007245332003,
         "_deepnote_index_column": 12
        },
        {
         "Topics": 0,
         "Words": "peg",
         "Probability": 0.0005485321744345129,
         "_deepnote_index_column": 13
        },
        {
         "Topics": 0,
         "Words": "sake",
         "Probability": 0.0005016423529013991,
         "_deepnote_index_column": 14
        },
        {
         "Topics": 0,
         "Words": "deepli",
         "Probability": 0.00036042090505361557,
         "_deepnote_index_column": 15
        },
        {
         "Topics": 0,
         "Words": "zone",
         "Probability": 0.00035809478140436113,
         "_deepnote_index_column": 16
        },
        {
         "Topics": 0,
         "Words": "swimsuit",
         "Probability": 0.0003072312683798373,
         "_deepnote_index_column": 17
        },
        {
         "Topics": 0,
         "Words": "demon",
         "Probability": 0.00027061012224294245,
         "_deepnote_index_column": 18
        },
        {
         "Topics": 0,
         "Words": "cuck",
         "Probability": 0.00020451025920920074,
         "_deepnote_index_column": 19
        },
        {
         "Topics": 1,
         "Words": "woman",
         "Probability": 0.19811251759529114,
         "_deepnote_index_column": 20
        },
        {
         "Topics": 1,
         "Words": "squirt",
         "Probability": 0.12459356337785721,
         "_deepnote_index_column": 21
        },
        {
         "Topics": 1,
         "Words": "perfect",
         "Probability": 0.07596538215875626,
         "_deepnote_index_column": 22
        },
        {
         "Topics": 1,
         "Words": "year",
         "Probability": 0.057766836136579514,
         "_deepnote_index_column": 23
        },
        {
         "Topics": 1,
         "Words": "hear",
         "Probability": 0.05190448835492134,
         "_deepnote_index_column": 24
        },
        {
         "Topics": 1,
         "Words": "imagin",
         "Probability": 0.04832401126623154,
         "_deepnote_index_column": 25
        },
        {
         "Topics": 1,
         "Words": "probabl",
         "Probability": 0.0452486127614975,
         "_deepnote_index_column": 26
        },
        {
         "Topics": 1,
         "Words": "special",
         "Probability": 0.02738214284181595,
         "_deepnote_index_column": 27
        },
        {
         "Topics": 1,
         "Words": "remind",
         "Probability": 0.025521712377667427,
         "_deepnote_index_column": 28
        },
        {
         "Topics": 1,
         "Words": "pleas",
         "Probability": 0.013516255654394627,
         "_deepnote_index_column": 29
        },
        {
         "Topics": 1,
         "Words": "certain",
         "Probability": 0.013459360226988792,
         "_deepnote_index_column": 30
        },
        {
         "Topics": 1,
         "Words": "bell",
         "Probability": 0.009153851307928562,
         "_deepnote_index_column": 31
        },
        {
         "Topics": 1,
         "Words": "distract",
         "Probability": 0.008875060826539993,
         "_deepnote_index_column": 32
        },
        {
         "Topics": 1,
         "Words": "schoolgirl",
         "Probability": 0.0076282150112092495,
         "_deepnote_index_column": 33
        },
        {
         "Topics": 1,
         "Words": "terribl",
         "Probability": 0.007147246040403843,
         "_deepnote_index_column": 34
        },
        {
         "Topics": 1,
         "Words": "featur",
         "Probability": 0.00554805900901556,
         "_deepnote_index_column": 35
        },
        {
         "Topics": 1,
         "Words": "wink",
         "Probability": 0.004947839770466089,
         "_deepnote_index_column": 36
        },
        {
         "Topics": 1,
         "Words": "tinna",
         "Probability": 0.00453687459230423,
         "_deepnote_index_column": 37
        },
        {
         "Topics": 1,
         "Words": "pigtail",
         "Probability": 0.0038610075134783983,
         "_deepnote_index_column": 38
        },
        {
         "Topics": 1,
         "Words": "godess",
         "Probability": 0.0035937947686761618,
         "_deepnote_index_column": 39
        },
        {
         "Topics": 2,
         "Words": "come",
         "Probability": 0.23360414803028107,
         "_deepnote_index_column": 40
        },
        {
         "Topics": 2,
         "Words": "turn",
         "Probability": 0.10240621119737625,
         "_deepnote_index_column": 41
        },
        {
         "Topics": 2,
         "Words": "hair",
         "Probability": 0.08307498693466187,
         "_deepnote_index_column": 42
        },
        {
         "Topics": 2,
         "Words": "life",
         "Probability": 0.07665950059890747,
         "_deepnote_index_column": 43
        },
        {
         "Topics": 2,
         "Words": "join",
         "Probability": 0.03185631334781647,
         "_deepnote_index_column": 44
        },
        {
         "Topics": 2,
         "Words": "eat",
         "Probability": 0.029913686215877533,
         "_deepnote_index_column": 45
        },
        {
         "Topics": 2,
         "Words": "teen",
         "Probability": 0.026378430426120758,
         "_deepnote_index_column": 46
        },
        {
         "Topics": 2,
         "Words": "weird",
         "Probability": 0.019829535856842995,
         "_deepnote_index_column": 47
        },
        {
         "Topics": 2,
         "Words": "stick",
         "Probability": 0.018817638978362083,
         "_deepnote_index_column": 48
        },
        {
         "Topics": 2,
         "Words": "stand",
         "Probability": 0.018318813294172287,
         "_deepnote_index_column": 49
        },
        {
         "Topics": 2,
         "Words": "congratul",
         "Probability": 0.013741116970777512,
         "_deepnote_index_column": 50
        },
        {
         "Topics": 2,
         "Words": "reason",
         "Probability": 0.011863733641803265,
         "_deepnote_index_column": 51
        },
        {
         "Topics": 2,
         "Words": "rate",
         "Probability": 0.011006176471710205,
         "_deepnote_index_column": 52
        },
        {
         "Topics": 2,
         "Words": "step",
         "Probability": 0.009994390420615673,
         "_deepnote_index_column": 53
        },
        {
         "Topics": 2,
         "Words": "matter",
         "Probability": 0.009900465607643127,
         "_deepnote_index_column": 54
        },
        {
         "Topics": 2,
         "Words": "didnt",
         "Probability": 0.008772427216172218,
         "_deepnote_index_column": 55
        },
        {
         "Topics": 2,
         "Words": "phenomen",
         "Probability": 0.008691666647791862,
         "_deepnote_index_column": 56
        },
        {
         "Topics": 2,
         "Words": "sign",
         "Probability": 0.0079902159050107,
         "_deepnote_index_column": 57
        },
        {
         "Topics": 2,
         "Words": "biggest",
         "Probability": 0.007557142060250044,
         "_deepnote_index_column": 58
        },
        {
         "Topics": 2,
         "Words": "wonderful",
         "Probability": 0.00726713752374053,
         "_deepnote_index_column": 59
        },
        {
         "Topics": 3,
         "Words": "hell",
         "Probability": 0.08155055344104767,
         "_deepnote_index_column": 60
        },
        {
         "Topics": 3,
         "Words": "massag",
         "Probability": 0.05874691903591156,
         "_deepnote_index_column": 61
        },
        {
         "Topics": 3,
         "Words": "clean",
         "Probability": 0.04408687353134155,
         "_deepnote_index_column": 62
        },
        {
         "Topics": 3,
         "Words": "holi",
         "Probability": 0.040830399841070175,
         "_deepnote_index_column": 63
        },
        {
         "Topics": 3,
         "Words": "break",
         "Probability": 0.03734651207923889,
         "_deepnote_index_column": 64
        },
        {
         "Topics": 3,
         "Words": "twice",
         "Probability": 0.02981356903910637,
         "_deepnote_index_column": 65
        },
        {
         "Topics": 3,
         "Words": "morn",
         "Probability": 0.027906037867069244,
         "_deepnote_index_column": 66
        },
        {
         "Topics": 3,
         "Words": "train",
         "Probability": 0.02341792918741703,
         "_deepnote_index_column": 67
        },
        {
         "Topics": 3,
         "Words": "pair",
         "Probability": 0.02072414942085743,
         "_deepnote_index_column": 68
        },
        {
         "Topics": 3,
         "Words": "nasti",
         "Probability": 0.019514033570885658,
         "_deepnote_index_column": 69
        },
        {
         "Topics": 3,
         "Words": "nut",
         "Probability": 0.0188821479678154,
         "_deepnote_index_column": 70
        },
        {
         "Topics": 3,
         "Words": "magnific",
         "Probability": 0.01860194280743599,
         "_deepnote_index_column": 71
        },
        {
         "Topics": 3,
         "Words": "belli",
         "Probability": 0.017128070816397667,
         "_deepnote_index_column": 72
        },
        {
         "Topics": 3,
         "Words": "thigh",
         "Probability": 0.014337384141981602,
         "_deepnote_index_column": 73
        },
        {
         "Topics": 3,
         "Words": "heart",
         "Probability": 0.013761148788034916,
         "_deepnote_index_column": 74
        },
        {
         "Topics": 3,
         "Words": "tabl",
         "Probability": 0.011272923089563847,
         "_deepnote_index_column": 75
        },
        {
         "Topics": 3,
         "Words": "uniqu",
         "Probability": 0.011174451559782028,
         "_deepnote_index_column": 76
        },
        {
         "Topics": 3,
         "Words": "mess",
         "Probability": 0.009871061891317368,
         "_deepnote_index_column": 77
        },
        {
         "Topics": 3,
         "Words": "cock_sucker",
         "Probability": 0.009067664854228497,
         "_deepnote_index_column": 78
        },
        {
         "Topics": 3,
         "Words": "soul",
         "Probability": 0.009034481830894947,
         "_deepnote_index_column": 79
        },
        {
         "Topics": 4,
         "Words": "girl",
         "Probability": 0.643075168132782,
         "_deepnote_index_column": 80
        },
        {
         "Topics": 4,
         "Words": "cum",
         "Probability": 0.07678068429231644,
         "_deepnote_index_column": 81
        },
        {
         "Topics": 4,
         "Words": "action",
         "Probability": 0.021234851330518723,
         "_deepnote_index_column": 82
        },
        {
         "Topics": 4,
         "Words": "attract",
         "Probability": 0.015209567733108997,
         "_deepnote_index_column": 83
        },
        {
         "Topics": 4,
         "Words": "meet",
         "Probability": 0.013156969100236893,
         "_deepnote_index_column": 84
        },
        {
         "Topics": 4,
         "Words": "class",
         "Probability": 0.009491387754678726,
         "_deepnote_index_column": 85
        },
        {
         "Topics": 4,
         "Words": "tittyfuck",
         "Probability": 0.008772417902946472,
         "_deepnote_index_column": 86
        },
        {
         "Topics": 4,
         "Words": "actor",
         "Probability": 0.006723165977746248,
         "_deepnote_index_column": 87
        },
        {
         "Topics": 4,
         "Words": "everytim",
         "Probability": 0.006038794759660959,
         "_deepnote_index_column": 88
        },
        {
         "Topics": 4,
         "Words": "vibe",
         "Probability": 0.00566148292273283,
         "_deepnote_index_column": 89
        },
        {
         "Topics": 4,
         "Words": "wast",
         "Probability": 0.0044727330096066,
         "_deepnote_index_column": 90
        },
        {
         "Topics": 4,
         "Words": "short_hair",
         "Probability": 0.002985958941280842,
         "_deepnote_index_column": 91
        },
        {
         "Topics": 4,
         "Words": "asian",
         "Probability": 0.0027341400273144245,
         "_deepnote_index_column": 92
        },
        {
         "Topics": 4,
         "Words": "brave",
         "Probability": 0.002433880465105176,
         "_deepnote_index_column": 93
        },
        {
         "Topics": 4,
         "Words": "tinna",
         "Probability": 0.001635921187698841,
         "_deepnote_index_column": 94
        },
        {
         "Topics": 4,
         "Words": "blue_eye",
         "Probability": 0.0013445538934320211,
         "_deepnote_index_column": 95
        },
        {
         "Topics": 4,
         "Words": "sweater",
         "Probability": 0.0012035939143970609,
         "_deepnote_index_column": 96
        },
        {
         "Topics": 4,
         "Words": "swedish",
         "Probability": 0.0004924633540213108,
         "_deepnote_index_column": 97
        },
        {
         "Topics": 4,
         "Words": "haircut",
         "Probability": 0.0004836828156840056,
         "_deepnote_index_column": 98
        },
        {
         "Topics": 4,
         "Words": "hoodi",
         "Probability": 0.00045720196794718504,
         "_deepnote_index_column": 99
        },
        {
         "Topics": 5,
         "Words": "hard",
         "Probability": 0.3568001389503479,
         "_deepnote_index_column": 100
        },
        {
         "Topics": 5,
         "Words": "hole",
         "Probability": 0.054934073239564896,
         "_deepnote_index_column": 101
        },
        {
         "Topics": 5,
         "Words": "leav",
         "Probability": 0.05460299178957939,
         "_deepnote_index_column": 102
        },
        {
         "Topics": 5,
         "Words": "tini",
         "Probability": 0.02343951351940632,
         "_deepnote_index_column": 103
        },
        {
         "Topics": 5,
         "Words": "ruin",
         "Probability": 0.019676972180604935,
         "_deepnote_index_column": 104
        },
        {
         "Topics": 5,
         "Words": "instant",
         "Probability": 0.018026288598775864,
         "_deepnote_index_column": 105
        },
        {
         "Topics": 5,
         "Words": "express",
         "Probability": 0.015113234519958496,
         "_deepnote_index_column": 106
        },
        {
         "Topics": 5,
         "Words": "background",
         "Probability": 0.012470944784581661,
         "_deepnote_index_column": 107
        },
        {
         "Topics": 5,
         "Words": "speak",
         "Probability": 0.011396492831408978,
         "_deepnote_index_column": 108
        },
        {
         "Topics": 5,
         "Words": "beg",
         "Probability": 0.009542963467538357,
         "_deepnote_index_column": 109
        },
        {
         "Topics": 5,
         "Words": "bend",
         "Probability": 0.009217438288033009,
         "_deepnote_index_column": 110
        },
        {
         "Topics": 5,
         "Words": "manag",
         "Probability": 0.008766749873757362,
         "_deepnote_index_column": 111
        },
        {
         "Topics": 5,
         "Words": "wrap",
         "Probability": 0.008269378915429115,
         "_deepnote_index_column": 112
        },
        {
         "Topics": 5,
         "Words": "nip",
         "Probability": 0.007903298363089561,
         "_deepnote_index_column": 113
        },
        {
         "Topics": 5,
         "Words": "chines",
         "Probability": 0.007385565899312496,
         "_deepnote_index_column": 114
        },
        {
         "Topics": 5,
         "Words": "arous",
         "Probability": 0.007206919603049755,
         "_deepnote_index_column": 115
        },
        {
         "Topics": 5,
         "Words": "congrat",
         "Probability": 0.0063702561892569065,
         "_deepnote_index_column": 116
        },
        {
         "Topics": 5,
         "Words": "neighbor",
         "Probability": 0.006042323540896177,
         "_deepnote_index_column": 117
        },
        {
         "Topics": 5,
         "Words": "vielen_dank",
         "Probability": 0.00559545774012804,
         "_deepnote_index_column": 118
        },
        {
         "Topics": 5,
         "Words": "feminin",
         "Probability": 0.005543178413063288,
         "_deepnote_index_column": 119
        },
        {
         "Topics": 6,
         "Words": "share",
         "Probability": 0.17955471575260162,
         "_deepnote_index_column": 120
        },
        {
         "Topics": 6,
         "Words": "view",
         "Probability": 0.11401807516813278,
         "_deepnote_index_column": 121
        },
        {
         "Topics": 6,
         "Words": "wear",
         "Probability": 0.0627078115940094,
         "_deepnote_index_column": 122
        },
        {
         "Topics": 6,
         "Words": "dress",
         "Probability": 0.03584182634949684,
         "_deepnote_index_column": 123
        },
        {
         "Topics": 6,
         "Words": "nake",
         "Probability": 0.028250673785805702,
         "_deepnote_index_column": 124
        },
        {
         "Topics": 6,
         "Words": "usual",
         "Probability": 0.026526635512709618,
         "_deepnote_index_column": 125
        },
        {
         "Topics": 6,
         "Words": "shower",
         "Probability": 0.021127721294760704,
         "_deepnote_index_column": 126
        },
        {
         "Topics": 6,
         "Words": "piss",
         "Probability": 0.018211474642157555,
         "_deepnote_index_column": 127
        },
        {
         "Topics": 6,
         "Words": "satisfi",
         "Probability": 0.015692729502916336,
         "_deepnote_index_column": 128
        },
        {
         "Topics": 6,
         "Words": "photo",
         "Probability": 0.015591050498187542,
         "_deepnote_index_column": 129
        },
        {
         "Topics": 6,
         "Words": "near",
         "Probability": 0.01539786346256733,
         "_deepnote_index_column": 130
        },
        {
         "Topics": 6,
         "Words": "suggest",
         "Probability": 0.013766903430223465,
         "_deepnote_index_column": 131
        },
        {
         "Topics": 6,
         "Words": "fan",
         "Probability": 0.013654272072017193,
         "_deepnote_index_column": 132
        },
        {
         "Topics": 6,
         "Words": "attent",
         "Probability": 0.013431725092232227,
         "_deepnote_index_column": 133
        },
        {
         "Topics": 6,
         "Words": "request",
         "Probability": 0.011951138265430927,
         "_deepnote_index_column": 134
        },
        {
         "Topics": 6,
         "Words": "half",
         "Probability": 0.0117581095546484,
         "_deepnote_index_column": 135
        },
        {
         "Topics": 6,
         "Words": "phat",
         "Probability": 0.009665858931839466,
         "_deepnote_index_column": 136
        },
        {
         "Topics": 6,
         "Words": "convers",
         "Probability": 0.008988816291093826,
         "_deepnote_index_column": 137
        },
        {
         "Topics": 6,
         "Words": "wall",
         "Probability": 0.008360037580132484,
         "_deepnote_index_column": 138
        },
        {
         "Topics": 6,
         "Words": "rich",
         "Probability": 0.00818458292633295,
         "_deepnote_index_column": 139
        },
        {
         "Topics": 7,
         "Words": "like",
         "Probability": 0.513089120388031,
         "_deepnote_index_column": 140
        },
        {
         "Topics": 7,
         "Words": "bodi",
         "Probability": 0.26251450181007385,
         "_deepnote_index_column": 141
        },
        {
         "Topics": 7,
         "Words": "boob",
         "Probability": 0.05604589357972145,
         "_deepnote_index_column": 142
        },
        {
         "Topics": 7,
         "Words": "moan",
         "Probability": 0.037485744804143906,
         "_deepnote_index_column": 143
        },
        {
         "Topics": 7,
         "Words": "smoke",
         "Probability": 0.009701911360025406,
         "_deepnote_index_column": 144
        },
        {
         "Topics": 7,
         "Words": "freak",
         "Probability": 0.009330963715910912,
         "_deepnote_index_column": 145
        },
        {
         "Topics": 7,
         "Words": "beautiful",
         "Probability": 0.005826791748404503,
         "_deepnote_index_column": 146
        },
        {
         "Topics": 7,
         "Words": "earth",
         "Probability": 0.0037671932950615883,
         "_deepnote_index_column": 147
        },
        {
         "Topics": 7,
         "Words": "tinna",
         "Probability": 0.0030252975411713123,
         "_deepnote_index_column": 148
        },
        {
         "Topics": 7,
         "Words": "chessi",
         "Probability": 0.0007082417141646147,
         "_deepnote_index_column": 149
        },
        {
         "Topics": 7,
         "Words": "fli",
         "Probability": 0.00035136754740960896,
         "_deepnote_index_column": 150
        },
        {
         "Topics": 7,
         "Words": "smaller",
         "Probability": 0.00034299210528843105,
         "_deepnote_index_column": 151
        },
        {
         "Topics": 7,
         "Words": "tatoo",
         "Probability": 0.00023727228108327836,
         "_deepnote_index_column": 152
        },
        {
         "Topics": 7,
         "Words": "interraci",
         "Probability": 0.00023091777984518558,
         "_deepnote_index_column": 153
        },
        {
         "Topics": 7,
         "Words": "kate",
         "Probability": 0.00022038172755856067,
         "_deepnote_index_column": 154
        },
        {
         "Topics": 7,
         "Words": "demon",
         "Probability": 0.0002065204462269321,
         "_deepnote_index_column": 155
        },
        {
         "Topics": 7,
         "Words": "martina",
         "Probability": 0.0001901257928693667,
         "_deepnote_index_column": 156
        },
        {
         "Topics": 7,
         "Words": "cuddl",
         "Probability": 0.00017383100930601358,
         "_deepnote_index_column": 157
        },
        {
         "Topics": 7,
         "Words": "tomorrow",
         "Probability": 0.00016538190538994968,
         "_deepnote_index_column": 158
        },
        {
         "Topics": 7,
         "Words": "particular",
         "Probability": 0.00013295520329847932,
         "_deepnote_index_column": 159
        },
        {
         "Topics": 8,
         "Words": "suck",
         "Probability": 0.23556683957576752,
         "_deepnote_index_column": 160
        },
        {
         "Topics": 8,
         "Words": "sure",
         "Probability": 0.10800332576036453,
         "_deepnote_index_column": 161
        },
        {
         "Topics": 8,
         "Words": "swallow",
         "Probability": 0.077690988779068,
         "_deepnote_index_column": 162
        },
        {
         "Topics": 8,
         "Words": "skill",
         "Probability": 0.06748248636722565,
         "_deepnote_index_column": 163
        },
        {
         "Topics": 8,
         "Words": "talk",
         "Probability": 0.044141411781311035,
         "_deepnote_index_column": 164
        },
        {
         "Topics": 8,
         "Words": "deep",
         "Probability": 0.030170241370797157,
         "_deepnote_index_column": 165
        }
       ],
       "rows_bottom": [
        {
         "Topics": 48,
         "Words": "outsid",
         "Probability": 0.014893781393766403,
         "_deepnote_index_column": 973
        },
        {
         "Topics": 48,
         "Words": "nail",
         "Probability": 0.011901343241333961,
         "_deepnote_index_column": 974
        },
        {
         "Topics": 48,
         "Words": "forev",
         "Probability": 0.011079381220042706,
         "_deepnote_index_column": 975
        },
        {
         "Topics": 48,
         "Words": "tender",
         "Probability": 0.01081794686615467,
         "_deepnote_index_column": 976
        },
        {
         "Topics": 48,
         "Words": "gross",
         "Probability": 0.010381372645497322,
         "_deepnote_index_column": 977
        },
        {
         "Topics": 48,
         "Words": "trust",
         "Probability": 0.010183081962168217,
         "_deepnote_index_column": 978
        },
        {
         "Topics": 48,
         "Words": "custom",
         "Probability": 0.008067158050835133,
         "_deepnote_index_column": 979
        },
        {
         "Topics": 49,
         "Words": "scene",
         "Probability": 0.1319693922996521,
         "_deepnote_index_column": 980
        },
        {
         "Topics": 49,
         "Words": "incred",
         "Probability": 0.07687699049711227,
         "_deepnote_index_column": 981
        },
        {
         "Topics": 49,
         "Words": "natur",
         "Probability": 0.07664746046066284,
         "_deepnote_index_column": 982
        },
        {
         "Topics": 49,
         "Words": "posit",
         "Probability": 0.07342954725027084,
         "_deepnote_index_column": 983
        },
        {
         "Topics": 49,
         "Words": "liter",
         "Probability": 0.05471421405673027,
         "_deepnote_index_column": 984
        },
        {
         "Topics": 49,
         "Words": "model",
         "Probability": 0.03316373750567436,
         "_deepnote_index_column": 985
        },
        {
         "Topics": 49,
         "Words": "extrem",
         "Probability": 0.02633408084511757,
         "_deepnote_index_column": 986
        },
        {
         "Topics": 49,
         "Words": "fall",
         "Probability": 0.025590643286705017,
         "_deepnote_index_column": 987
        },
        {
         "Topics": 49,
         "Words": "revers_cowgirl",
         "Probability": 0.021965267136693,
         "_deepnote_index_column": 988
        },
        {
         "Topics": 49,
         "Words": "divin",
         "Probability": 0.01817183569073677,
         "_deepnote_index_column": 989
        },
        {
         "Topics": 49,
         "Words": "oral",
         "Probability": 0.016134370118379593,
         "_deepnote_index_column": 990
        },
        {
         "Topics": 49,
         "Words": "chat",
         "Probability": 0.015226495452225208,
         "_deepnote_index_column": 991
        },
        {
         "Topics": 49,
         "Words": "genuin",
         "Probability": 0.01506350003182888,
         "_deepnote_index_column": 992
        },
        {
         "Topics": 49,
         "Words": "expect",
         "Probability": 0.012698485516011715,
         "_deepnote_index_column": 993
        },
        {
         "Topics": 49,
         "Words": "foreplay",
         "Probability": 0.011977766640484333,
         "_deepnote_index_column": 994
        },
        {
         "Topics": 49,
         "Words": "jesus_christ",
         "Probability": 0.009167690761387348,
         "_deepnote_index_column": 995
        },
        {
         "Topics": 49,
         "Words": "jizz",
         "Probability": 0.00833272933959961,
         "_deepnote_index_column": 996
        },
        {
         "Topics": 49,
         "Words": "goal",
         "Probability": 0.007716361433267593,
         "_deepnote_index_column": 997
        },
        {
         "Topics": 49,
         "Words": "pee",
         "Probability": 0.007401145529001951,
         "_deepnote_index_column": 998
        },
        {
         "Topics": 49,
         "Words": "curvi",
         "Probability": 0.006091046612709761,
         "_deepnote_index_column": 999
        },
        {
         "Topics": 50,
         "Words": "tit",
         "Probability": 0.35582593083381653,
         "_deepnote_index_column": 1000
        },
        {
         "Topics": 50,
         "Words": "babe",
         "Probability": 0.16114692389965057,
         "_deepnote_index_column": 1001
        },
        {
         "Topics": 50,
         "Words": "delici",
         "Probability": 0.05933209881186485,
         "_deepnote_index_column": 1002
        },
        {
         "Topics": 50,
         "Words": "tight",
         "Probability": 0.05897684022784233,
         "_deepnote_index_column": 1003
        },
        {
         "Topics": 50,
         "Words": "ador",
         "Probability": 0.03225293755531311,
         "_deepnote_index_column": 1004
        },
        {
         "Topics": 50,
         "Words": "open",
         "Probability": 0.02358877845108509,
         "_deepnote_index_column": 1005
        },
        {
         "Topics": 50,
         "Words": "straight",
         "Probability": 0.020134760066866875,
         "_deepnote_index_column": 1006
        },
        {
         "Topics": 50,
         "Words": "intens",
         "Probability": 0.01956673339009285,
         "_deepnote_index_column": 1007
        },
        {
         "Topics": 50,
         "Words": "honey",
         "Probability": 0.015121987089514732,
         "_deepnote_index_column": 1008
        },
        {
         "Topics": 50,
         "Words": "rest",
         "Probability": 0.011490825563669205,
         "_deepnote_index_column": 1009
        },
        {
         "Topics": 50,
         "Words": "edg",
         "Probability": 0.010794905945658684,
         "_deepnote_index_column": 1010
        },
        {
         "Topics": 50,
         "Words": "hang",
         "Probability": 0.009564216248691082,
         "_deepnote_index_column": 1011
        },
        {
         "Topics": 50,
         "Words": "clit",
         "Probability": 0.008870666846632957,
         "_deepnote_index_column": 1012
        },
        {
         "Topics": 50,
         "Words": "rock_hard",
         "Probability": 0.007014057133346796,
         "_deepnote_index_column": 1013
        },
        {
         "Topics": 50,
         "Words": "squeez",
         "Probability": 0.006466780789196491,
         "_deepnote_index_column": 1014
        },
        {
         "Topics": 50,
         "Words": "glad_enjoy",
         "Probability": 0.005665282253175974,
         "_deepnote_index_column": 1015
        },
        {
         "Topics": 50,
         "Words": "lick_clean",
         "Probability": 0.005351715255528688,
         "_deepnote_index_column": 1016
        },
        {
         "Topics": 50,
         "Words": "slide",
         "Probability": 0.005326246377080679,
         "_deepnote_index_column": 1017
        },
        {
         "Topics": 50,
         "Words": "plug",
         "Probability": 0.001803655526600778,
         "_deepnote_index_column": 1018
        },
        {
         "Topics": 50,
         "Words": "tinna",
         "Probability": 0.0011081879492849112,
         "_deepnote_index_column": 1019
        },
        {
         "Topics": 51,
         "Words": "go",
         "Probability": 0.16203080117702484,
         "_deepnote_index_column": 1020
        },
        {
         "Topics": 51,
         "Words": "take",
         "Probability": 0.09174554795026779,
         "_deepnote_index_column": 1021
        },
        {
         "Topics": 51,
         "Words": "asshol",
         "Probability": 0.07776354253292084,
         "_deepnote_index_column": 1022
        },
        {
         "Topics": 51,
         "Words": "minut",
         "Probability": 0.05733916908502579,
         "_deepnote_index_column": 1023
        },
        {
         "Topics": 51,
         "Words": "fact",
         "Probability": 0.02439180202782154,
         "_deepnote_index_column": 1024
        },
        {
         "Topics": 51,
         "Words": "quick",
         "Probability": 0.023500829935073853,
         "_deepnote_index_column": 1025
        },
        {
         "Topics": 51,
         "Words": "level",
         "Probability": 0.02297823317348957,
         "_deepnote_index_column": 1026
        },
        {
         "Topics": 51,
         "Words": "favorit",
         "Probability": 0.019953515380620956,
         "_deepnote_index_column": 1027
        },
        {
         "Topics": 51,
         "Words": "milk",
         "Probability": 0.017612388357520103,
         "_deepnote_index_column": 1028
        },
        {
         "Topics": 51,
         "Words": "today",
         "Probability": 0.017127487808465958,
         "_deepnote_index_column": 1029
        },
        {
         "Topics": 51,
         "Words": "isnt",
         "Probability": 0.015524467453360558,
         "_deepnote_index_column": 1030
        },
        {
         "Topics": 51,
         "Words": "link",
         "Probability": 0.013790512457489967,
         "_deepnote_index_column": 1031
        },
        {
         "Topics": 51,
         "Words": "dislik",
         "Probability": 0.012827284634113312,
         "_deepnote_index_column": 1032
        },
        {
         "Topics": 51,
         "Words": "doubt",
         "Probability": 0.012533305212855339,
         "_deepnote_index_column": 1033
        },
        {
         "Topics": 51,
         "Words": "disappoint",
         "Probability": 0.012139041908085346,
         "_deepnote_index_column": 1034
        },
        {
         "Topics": 51,
         "Words": "tina",
         "Probability": 0.01206221990287304,
         "_deepnote_index_column": 1035
        },
        {
         "Topics": 51,
         "Words": "past",
         "Probability": 0.011867910623550415,
         "_deepnote_index_column": 1036
        },
        {
         "Topics": 51,
         "Words": "push",
         "Probability": 0.011737912893295288,
         "_deepnote_index_column": 1037
        },
        {
         "Topics": 51,
         "Words": "basic",
         "Probability": 0.009035659022629261,
         "_deepnote_index_column": 1038
        },
        {
         "Topics": 51,
         "Words": "pretend",
         "Probability": 0.008469508960843086,
         "_deepnote_index_column": 1039
        },
        {
         "Topics": 52,
         "Words": "feel",
         "Probability": 0.1655532419681549,
         "_deepnote_index_column": 1040
        },
        {
         "Topics": 52,
         "Words": "tast",
         "Probability": 0.056187283247709274,
         "_deepnote_index_column": 1041
        },
        {
         "Topics": 52,
         "Words": "soon",
         "Probability": 0.056073639541864395,
         "_deepnote_index_column": 1042
        },
        {
         "Topics": 52,
         "Words": "throat",
         "Probability": 0.039904359728097916,
         "_deepnote_index_column": 1043
        },
        {
         "Topics": 52,
         "Words": "juici",
         "Probability": 0.034138862043619156,
         "_deepnote_index_column": 1044
        },
        {
         "Topics": 52,
         "Words": "sexual",
         "Probability": 0.032911691814661026,
         "_deepnote_index_column": 1045
        },
        {
         "Topics": 52,
         "Words": "white",
         "Probability": 0.03195574879646301,
         "_deepnote_index_column": 1046
        },
        {
         "Topics": 52,
         "Words": "chanc",
         "Probability": 0.024788642302155495,
         "_deepnote_index_column": 1047
        },
        {
         "Topics": 52,
         "Words": "power",
         "Probability": 0.021401215344667435,
         "_deepnote_index_column": 1048
        },
        {
         "Topics": 52,
         "Words": "line",
         "Probability": 0.020904384553432465,
         "_deepnote_index_column": 1049
        },
        {
         "Topics": 52,
         "Words": "wake",
         "Probability": 0.01833982579410076,
         "_deepnote_index_column": 1050
        },
        {
         "Topics": 52,
         "Words": "high",
         "Probability": 0.015862487256526947,
         "_deepnote_index_column": 1051
        },
        {
         "Topics": 52,
         "Words": "husband",
         "Probability": 0.014889116398990154,
         "_deepnote_index_column": 1052
        },
        {
         "Topics": 52,
         "Words": "gentl",
         "Probability": 0.01445352379232645,
         "_deepnote_index_column": 1053
        },
        {
         "Topics": 52,
         "Words": "sit",
         "Probability": 0.012307003140449524,
         "_deepnote_index_column": 1054
        },
        {
         "Topics": 52,
         "Words": "soak",
         "Probability": 0.009867331944406033,
         "_deepnote_index_column": 1055
        },
        {
         "Topics": 52,
         "Words": "ring",
         "Probability": 0.009725657291710377,
         "_deepnote_index_column": 1056
        },
        {
         "Topics": 52,
         "Words": "destroy",
         "Probability": 0.008857861161231995,
         "_deepnote_index_column": 1057
        },
        {
         "Topics": 52,
         "Words": "extra",
         "Probability": 0.008796380832791328,
         "_deepnote_index_column": 1058
        },
        {
         "Topics": 52,
         "Words": "merci",
         "Probability": 0.008592113852500916,
         "_deepnote_index_column": 1059
        },
        {
         "Topics": 53,
         "Words": "show",
         "Probability": 0.05989256501197815,
         "_deepnote_index_column": 1060
        },
        {
         "Topics": 53,
         "Words": "thumb",
         "Probability": 0.053592585027217865,
         "_deepnote_index_column": 1061
        },
        {
         "Topics": 53,
         "Words": "sweeti",
         "Probability": 0.031559158116579056,
         "_deepnote_index_column": 1062
        },
        {
         "Topics": 53,
         "Words": "impress",
         "Probability": 0.02599913999438286,
         "_deepnote_index_column": 1063
        },
        {
         "Topics": 53,
         "Words": "luck",
         "Probability": 0.025124119594693184,
         "_deepnote_index_column": 1064
        },
        {
         "Topics": 53,
         "Words": "money",
         "Probability": 0.02308407984673977,
         "_deepnote_index_column": 1065
        },
        {
         "Topics": 53,
         "Words": "boss",
         "Probability": 0.019786229357123375,
         "_deepnote_index_column": 1066
        },
        {
         "Topics": 53,
         "Words": "master",
         "Probability": 0.01970190368592739,
         "_deepnote_index_column": 1067
        },
        {
         "Topics": 53,
         "Words": "origin",
         "Probability": 0.018243368715047836,
         "_deepnote_index_column": 1068
        },
        {
         "Topics": 53,
         "Words": "ask",
         "Probability": 0.018119391053915024,
         "_deepnote_index_column": 1069
        },
        {
         "Topics": 53,
         "Words": "creativ",
         "Probability": 0.017595823854207993,
         "_deepnote_index_column": 1070
        },
        {
         "Topics": 53,
         "Words": "name",
         "Probability": 0.017522217705845833,
         "_deepnote_index_column": 1071
        },
        {
         "Topics": 53,
         "Words": "teach",
         "Probability": 0.015919411554932594,
         "_deepnote_index_column": 1072
        },
        {
         "Topics": 53,
         "Words": "mate",
         "Probability": 0.01359432004392147,
         "_deepnote_index_column": 1073
        },
        {
         "Topics": 53,
         "Words": "crush",
         "Probability": 0.012279980815947056,
         "_deepnote_index_column": 1074
        },
        {
         "Topics": 53,
         "Words": "serv",
         "Probability": 0.012054176069796085,
         "_deepnote_index_column": 1075
        },
        {
         "Topics": 53,
         "Words": "boner",
         "Probability": 0.010939769446849823,
         "_deepnote_index_column": 1076
        },
        {
         "Topics": 53,
         "Words": "worth",
         "Probability": 0.010869025252759457,
         "_deepnote_index_column": 1077
        },
        {
         "Topics": 53,
         "Words": "bite",
         "Probability": 0.01032471563667059,
         "_deepnote_index_column": 1078
        },
        {
         "Topics": 53,
         "Words": "culo",
         "Probability": 0.010131532326340675,
         "_deepnote_index_column": 1079
        },
        {
         "Topics": 54,
         "Words": "lucki",
         "Probability": 0.20260412991046906,
         "_deepnote_index_column": 1080
        },
        {
         "Topics": 54,
         "Words": "women",
         "Probability": 0.0839926153421402,
         "_deepnote_index_column": 1081
        },
        {
         "Topics": 54,
         "Words": "boyfriend",
         "Probability": 0.06430739909410477,
         "_deepnote_index_column": 1082
        },
        {
         "Topics": 54,
         "Words": "true",
         "Probability": 0.05447782948613167,
         "_deepnote_index_column": 1083
        },
        {
         "Topics": 54,
         "Words": "touch",
         "Probability": 0.050041552633047104,
         "_deepnote_index_column": 1084
        },
        {
         "Topics": 54,
         "Words": "plus",
         "Probability": 0.032227952033281326,
         "_deepnote_index_column": 1085
        },
        {
         "Topics": 54,
         "Words": "marri",
         "Probability": 0.027447275817394257,
         "_deepnote_index_column": 1086
        },
        {
         "Topics": 54,
         "Words": "male",
         "Probability": 0.018258772790431976,
         "_deepnote_index_column": 1087
        },
        {
         "Topics": 54,
         "Words": "creat",
         "Probability": 0.014796222560107708,
         "_deepnote_index_column": 1088
        },
        {
         "Topics": 54,
         "Words": "save",
         "Probability": 0.01452899444848299,
         "_deepnote_index_column": 1089
        },
        {
         "Topics": 54,
         "Words": "pregnant",
         "Probability": 0.012279754504561424,
         "_deepnote_index_column": 1090
        },
        {
         "Topics": 54,
         "Words": "tit_bounc",
         "Probability": 0.010993191041052341,
         "_deepnote_index_column": 1091
        },
        {
         "Topics": 54,
         "Words": "immedi",
         "Probability": 0.010327850468456745,
         "_deepnote_index_column": 1092
        },
        {
         "Topics": 54,
         "Words": "kriss",
         "Probability": 0.009484442882239819,
         "_deepnote_index_column": 1093
        },
        {
         "Topics": 54,
         "Words": "redhead",
         "Probability": 0.009301535785198212,
         "_deepnote_index_column": 1094
        },
        {
         "Topics": 54,
         "Words": "relat",
         "Probability": 0.008792709559202194,
         "_deepnote_index_column": 1095
        },
        {
         "Topics": 54,
         "Words": "simp",
         "Probability": 0.008662001229822636,
         "_deepnote_index_column": 1096
        },
        {
         "Topics": 54,
         "Words": "reaction",
         "Probability": 0.008233442902565002,
         "_deepnote_index_column": 1097
        },
        {
         "Topics": 54,
         "Words": "oop",
         "Probability": 0.0063499328680336475,
         "_deepnote_index_column": 1098
        },
        {
         "Topics": 54,
         "Words": "climax",
         "Probability": 0.006313555408269167,
         "_deepnote_index_column": 1099
        },
        {
         "Topics": 55,
         "Words": "cock",
         "Probability": 0.39761948585510254,
         "_deepnote_index_column": 1100
        },
        {
         "Topics": 55,
         "Words": "creampi",
         "Probability": 0.12684349715709686,
         "_deepnote_index_column": 1101
        },
        {
         "Topics": 55,
         "Words": "masturb",
         "Probability": 0.056671109050512314,
         "_deepnote_index_column": 1102
        },
        {
         "Topics": 55,
         "Words": "ball",
         "Probability": 0.042802829295396805,
         "_deepnote_index_column": 1103
        },
        {
         "Topics": 55,
         "Words": "explod",
         "Probability": 0.025997919961810112,
         "_deepnote_index_column": 1104
        },
        {
         "Topics": 55,
         "Words": "slut",
         "Probability": 0.024008851498365402,
         "_deepnote_index_column": 1105
        },
        {
         "Topics": 55,
         "Words": "lesbian",
         "Probability": 0.015398732386529446,
         "_deepnote_index_column": 1106
        },
        {
         "Topics": 55,
         "Words": "throb",
         "Probability": 0.013107724487781525,
         "_deepnote_index_column": 1107
        },
        {
         "Topics": 55,
         "Words": "cheat",
         "Probability": 0.013021859340369701,
         "_deepnote_index_column": 1108
        },
        {
         "Topics": 55,
         "Words": "bomb",
         "Probability": 0.00853061769157648,
         "_deepnote_index_column": 1109
        },
        {
         "Topics": 55,
         "Words": "resist",
         "Probability": 0.007671321276575327,
         "_deepnote_index_column": 1110
        },
        {
         "Topics": 55,
         "Words": "riski",
         "Probability": 0.006464056670665741,
         "_deepnote_index_column": 1111
        },
        {
         "Topics": 55,
         "Words": "team",
         "Probability": 0.0063971239142119884,
         "_deepnote_index_column": 1112
        },
        {
         "Topics": 55,
         "Words": "grip",
         "Probability": 0.005528187844902277,
         "_deepnote_index_column": 1113
        },
        {
         "Topics": 55,
         "Words": "circumcis",
         "Probability": 0.005494921933859587,
         "_deepnote_index_column": 1114
        },
        {
         "Topics": 55,
         "Words": "toy",
         "Probability": 0.005214767996221781,
         "_deepnote_index_column": 1115
        },
        {
         "Topics": 55,
         "Words": "holi_crap",
         "Probability": 0.0051133823581039906,
         "_deepnote_index_column": 1116
        },
        {
         "Topics": 55,
         "Words": "hmmmm",
         "Probability": 0.004551284946501255,
         "_deepnote_index_column": 1117
        },
        {
         "Topics": 55,
         "Words": "pure_perfect",
         "Probability": 0.003183824708685279,
         "_deepnote_index_column": 1118
        },
        {
         "Topics": 55,
         "Words": "wont",
         "Probability": 0.002681673737242818,
         "_deepnote_index_column": 1119
        },
        {
         "Topics": 56,
         "Words": "work",
         "Probability": 0.2090732306241989,
         "_deepnote_index_column": 1120
        },
        {
         "Topics": 56,
         "Words": "vid",
         "Probability": 0.12050429731607437,
         "_deepnote_index_column": 1121
        },
        {
         "Topics": 56,
         "Words": "stun",
         "Probability": 0.07369600981473923,
         "_deepnote_index_column": 1122
        },
        {
         "Topics": 56,
         "Words": "sock",
         "Probability": 0.061494361609220505,
         "_deepnote_index_column": 1123
        },
        {
         "Topics": 56,
         "Words": "stock",
         "Probability": 0.05348607897758484,
         "_deepnote_index_column": 1124
        },
        {
         "Topics": 56,
         "Words": "site",
         "Probability": 0.031270455569028854,
         "_deepnote_index_column": 1125
        },
        {
         "Topics": 56,
         "Words": "possibl",
         "Probability": 0.026593009009957314,
         "_deepnote_index_column": 1126
        },
        {
         "Topics": 56,
         "Words": "chick",
         "Probability": 0.026046147570014,
         "_deepnote_index_column": 1127
        },
        {
         "Topics": 56,
         "Words": "pornstar",
         "Probability": 0.02294362708926201,
         "_deepnote_index_column": 1128
        },
        {
         "Topics": 56,
         "Words": "long_time",
         "Probability": 0.01811322383582592,
         "_deepnote_index_column": 1129
        },
        {
         "Topics": 56,
         "Words": "russian",
         "Probability": 0.014550713822245598,
         "_deepnote_index_column": 1130
        },
        {
         "Topics": 56,
         "Words": "heel",
         "Probability": 0.014117555692791939,
         "_deepnote_index_column": 1131
        },
        {
         "Topics": 56,
         "Words": "omfg",
         "Probability": 0.013514791615307331,
         "_deepnote_index_column": 1132
        },
        {
         "Topics": 56,
         "Words": "cunt",
         "Probability": 0.01261436939239502,
         "_deepnote_index_column": 1133
        },
        {
         "Topics": 56,
         "Words": "die",
         "Probability": 0.012481997720897198,
         "_deepnote_index_column": 1134
        },
        {
         "Topics": 56,
         "Words": "suit",
         "Probability": 0.010922722518444061,
         "_deepnote_index_column": 1135
        },
        {
         "Topics": 56,
         "Words": "facesit",
         "Probability": 0.00968137662857771,
         "_deepnote_index_column": 1136
        },
        {
         "Topics": 56,
         "Words": "unreal",
         "Probability": 0.006671618204563856,
         "_deepnote_index_column": 1137
        },
        {
         "Topics": 56,
         "Words": "feedback",
         "Probability": 0.005868339911103249,
         "_deepnote_index_column": 1138
        },
        {
         "Topics": 56,
         "Words": "attack",
         "Probability": 0.004492853302508593,
         "_deepnote_index_column": 1139
        }
       ]
      },
      "text/plain": "      Topics     Words  Probability\n0          0      fuck     0.742537\n1          0   comment     0.085933\n2          0     bitch     0.023916\n3          0      piec     0.004984\n4          0    stupid     0.004158\n...      ...       ...          ...\n1135      56      suit     0.010923\n1136      56   facesit     0.009681\n1137      56    unreal     0.006672\n1138      56  feedback     0.005868\n1139      56    attack     0.004493\n\n[1140 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topics</th>\n      <th>Words</th>\n      <th>Probability</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>fuck</td>\n      <td>0.742537</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>comment</td>\n      <td>0.085933</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>bitch</td>\n      <td>0.023916</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>piec</td>\n      <td>0.004984</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>stupid</td>\n      <td>0.004158</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1135</th>\n      <td>56</td>\n      <td>suit</td>\n      <td>0.010923</td>\n    </tr>\n    <tr>\n      <th>1136</th>\n      <td>56</td>\n      <td>facesit</td>\n      <td>0.009681</td>\n    </tr>\n    <tr>\n      <th>1137</th>\n      <td>56</td>\n      <td>unreal</td>\n      <td>0.006672</td>\n    </tr>\n    <tr>\n      <th>1138</th>\n      <td>56</td>\n      <td>feedback</td>\n      <td>0.005868</td>\n    </tr>\n    <tr>\n      <th>1139</th>\n      <td>56</td>\n      <td>attack</td>\n      <td>0.004493</td>\n    </tr>\n  </tbody>\n</table>\n<p>1140 rows × 3 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "85fca68",
    "execution_millis": 26,
    "execution_start": 1614090716141,
    "cell_id": "00102-4d83cc8e-279e-4ae9-9b28-0cafe39fb58e",
    "deepnote_cell_type": "code"
   },
   "source": "Words_in_topics.to_csv(\"words_in_topics.csv\")",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Compute the Hellinger distance",
   "metadata": {
    "cell_id": "00104-30873601-7ae1-46f1-9f99-8d51a2c9e0eb",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b23d186b",
    "execution_millis": 369,
    "execution_start": 1614091024167,
    "cell_id": "00103-ab735bbb-f574-4fed-a808-ae60cb98fc55",
    "deepnote_cell_type": "code"
   },
   "source": "#Processing function for Hellinger distance\ntopic_distribution = {\"Topics\": [],\n              \"Prob_distribution\": []}\n\nfor i in range(num_topics):\n    words_topic = Words_in_topics.loc[(Words_in_topics.Topics==i), [\"Words\"]].values.tolist()\n    words_topic = sum(words_topic, [])\n    \n    prob_dist = lda_model.id2word.doc2bow(words_topic)  \n    \n    topic_distribution[\"Topics\"].append(i)\n    topic_distribution[\"Prob_distribution\"].append(prob_dist)\n\ntopic_distribution = pd.DataFrame(topic_distribution)",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 21,
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "row_count": 57,
       "column_count": 2,
       "columns": [
        {
         "name": "Topics",
         "dtype": "int64",
         "stats": {
          "unique_count": 57,
          "nan_count": 0,
          "min": 0,
          "max": 56,
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 5.6,
            "count": 6
           },
           {
            "bin_start": 5.6,
            "bin_end": 11.2,
            "count": 6
           },
           {
            "bin_start": 11.2,
            "bin_end": 16.799999999999997,
            "count": 5
           },
           {
            "bin_start": 16.799999999999997,
            "bin_end": 22.4,
            "count": 6
           },
           {
            "bin_start": 22.4,
            "bin_end": 28,
            "count": 5
           },
           {
            "bin_start": 28,
            "bin_end": 33.599999999999994,
            "count": 6
           },
           {
            "bin_start": 33.599999999999994,
            "bin_end": 39.199999999999996,
            "count": 6
           },
           {
            "bin_start": 39.199999999999996,
            "bin_end": 44.8,
            "count": 5
           },
           {
            "bin_start": 44.8,
            "bin_end": 50.4,
            "count": 6
           },
           {
            "bin_start": 50.4,
            "bin_end": 56,
            "count": 6
           }
          ]
         }
        },
        {
         "name": "Prob_distribution",
         "dtype": "object",
         "stats": {
          "unique_count": 57,
          "nan_count": 0,
          "categories": [
           {
            "name": "[(27, 1), (40, 1), (266, 1), (659, 1), (963, 1), (1076, 1), (1150, 1), (1444, 1), (1519, 1), (1577, 1), (1580, 1), (1801, 1), (1828, 1), (1865, 1), (1891, 1), (1893, 1), (2017, 1), (2271, 1), (2298, 1), (2301, 1)]",
            "count": 1
           },
           {
            "name": "[(37, 1), (76, 1), (78, 1), (79, 1), (270, 1), (313, 1), (314, 1), (338, 1), (473, 1), (595, 1), (686, 1), (687, 1), (688, 1), (917, 1), (1185, 1), (1286, 1), (1392, 1), (1845, 1), (2056, 1), (2301, 1)]",
            "count": 1
           },
           {
            "name": "55 others",
            "count": 55
           }
          ]
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "rows_top": [
        {
         "Topics": 0,
         "Prob_distribution": "[(27, 1), (40, 1), (266, 1), (659, 1), (963, 1), (1076, 1), (1150, 1), (1444, 1), (1519, 1), (1577, 1), (1580, 1), (1801, 1), (1828, 1), (1865, 1), (1891, 1), (1893, 1), (2017, 1), (2271, 1), (2298, 1), (2301, 1)]",
         "_deepnote_index_column": 0
        },
        {
         "Topics": 1,
         "Prob_distribution": "[(37, 1), (76, 1), (78, 1), (79, 1), (270, 1), (313, 1), (314, 1), (338, 1), (473, 1), (595, 1), (686, 1), (687, 1), (688, 1), (917, 1), (1185, 1), (1286, 1), (1392, 1), (1845, 1), (2056, 1), (2301, 1)]",
         "_deepnote_index_column": 1
        },
        {
         "Topics": 2,
         "Prob_distribution": "[(44, 1), (131, 1), (185, 1), (350, 1), (351, 1), (352, 1), (365, 1), (366, 1), (466, 1), (508, 1), (512, 1), (628, 1), (723, 1), (724, 1), (750, 1), (1318, 1), (1339, 1), (1343, 1), (1356, 1), (1387, 1)]",
         "_deepnote_index_column": 2
        },
        {
         "Topics": 3,
         "Prob_distribution": "[(348, 1), (382, 1), (383, 1), (384, 1), (456, 1), (623, 1), (799, 1), (1077, 1), (1223, 1), (1234, 1), (1236, 1), (1265, 1), (1331, 1), (1450, 1), (1459, 1), (1486, 1), (1711, 1), (1771, 1), (2040, 1), (2121, 1)]",
         "_deepnote_index_column": 3
        },
        {
         "Topics": 4,
         "Prob_distribution": "[(21, 1), (75, 1), (319, 1), (381, 1), (387, 1), (388, 1), (443, 1), (639, 1), (763, 1), (778, 1), (806, 1), (852, 1), (1497, 1), (1849, 1), (2007, 1), (2036, 1), (2075, 1), (2276, 1), (2284, 1), (2301, 1)]",
         "_deepnote_index_column": 4
        },
        {
         "Topics": 5,
         "Prob_distribution": "[(59, 1), (123, 1), (237, 1), (241, 1), (536, 1), (547, 1), (692, 1), (816, 1), (879, 1), (1048, 1), (1071, 1), (1221, 1), (1282, 1), (1308, 1), (1338, 1), (1347, 1), (1382, 1), (1774, 1), (1810, 1), (2295, 1)]",
         "_deepnote_index_column": 5
        },
        {
         "Topics": 6,
         "Prob_distribution": "[(140, 1), (149, 1), (178, 1), (318, 1), (336, 1), (434, 1), (480, 1), (482, 1), (483, 1), (484, 1), (487, 1), (488, 1), (510, 1), (1104, 1), (1326, 1), (1416, 1), (1534, 1), (1574, 1), (1678, 1), (1787, 1)]",
         "_deepnote_index_column": 6
        },
        {
         "Topics": 7,
         "Prob_distribution": "[(17, 1), (20, 1), (46, 1), (52, 1), (191, 1), (216, 1), (326, 1), (659, 1), (697, 1), (1017, 1), (1076, 1), (1469, 1), (1524, 1), (1704, 1), (1718, 1), (1807, 1), (1830, 1), (2093, 1), (2299, 1), (2301, 1)]",
         "_deepnote_index_column": 7
        },
        {
         "Topics": 8,
         "Prob_distribution": "[(93, 1), (141, 1), (170, 1), (200, 1), (378, 1), (577, 1), (792, 1), (874, 1), (875, 1), (1090, 1), (1121, 1), (1137, 1), (1152, 1), (1167, 1), (1436, 1), (1478, 1), (1628, 1), (1631, 1), (1636, 1), (1732, 1)]",
         "_deepnote_index_column": 8
        },
        {
         "Topics": 9,
         "Prob_distribution": "[(114, 1), (304, 1), (305, 1), (345, 1), (346, 1), (347, 1), (432, 1), (433, 1), (475, 1), (759, 1), (831, 1), (887, 1), (948, 1), (1016, 1), (1055, 1), (1088, 1), (1163, 1), (1441, 1), (1539, 1), (2118, 1)]",
         "_deepnote_index_column": 9
        },
        {
         "Topics": 10,
         "Prob_distribution": "[(84, 1), (104, 1), (105, 1), (107, 1), (193, 1), (494, 1), (495, 1), (496, 1), (497, 1), (498, 1), (499, 1), (521, 1), (674, 1), (822, 1), (1124, 1), (1183, 1), (1184, 1), (1555, 1), (1806, 1), (1977, 1)]",
         "_deepnote_index_column": 10
        },
        {
         "Topics": 11,
         "Prob_distribution": "[(26, 1), (45, 1), (71, 1), (72, 1), (73, 1), (218, 1), (361, 1), (477, 1), (556, 1), (558, 1), (559, 1), (560, 1), (609, 1), (829, 1), (838, 1), (1078, 1), (1528, 1), (1537, 1), (2030, 1), (2105, 1)]",
         "_deepnote_index_column": 11
        },
        {
         "Topics": 12,
         "Prob_distribution": "[(164, 1), (198, 1), (199, 1), (227, 1), (229, 1), (232, 1), (262, 1), (376, 1), (506, 1), (507, 1), (631, 1), (634, 1), (734, 1), (735, 1), (736, 1), (737, 1), (959, 1), (960, 1), (1353, 1), (1892, 1)]",
         "_deepnote_index_column": 12
        },
        {
         "Topics": 13,
         "Prob_distribution": "[(190, 1), (339, 1), (444, 1), (826, 1), (837, 1), (876, 1), (1179, 1), (1248, 1), (1461, 1), (1466, 1), (1485, 1), (1791, 1), (1848, 1), (1877, 1), (1894, 1), (1912, 1), (1915, 1), (1990, 1), (2106, 1), (2234, 1)]",
         "_deepnote_index_column": 13
        },
        {
         "Topics": 14,
         "Prob_distribution": "[(62, 1), (66, 1), (82, 1), (154, 1), (158, 1), (201, 1), (489, 1), (530, 1), (534, 1), (578, 1), (699, 1), (1027, 1), (1357, 1), (1400, 1), (1603, 1), (1638, 1), (1867, 1), (2045, 1), (2054, 1), (2301, 1)]",
         "_deepnote_index_column": 14
        },
        {
         "Topics": 15,
         "Prob_distribution": "[(4, 1), (102, 1), (223, 1), (273, 1), (274, 1), (289, 1), (321, 1), (333, 1), (335, 1), (341, 1), (380, 1), (470, 1), (780, 1), (782, 1), (783, 1), (846, 1), (991, 1), (1047, 1), (1259, 1), (1811, 1)]",
         "_deepnote_index_column": 15
        },
        {
         "Topics": 16,
         "Prob_distribution": "[(5, 1), (83, 1), (116, 1), (129, 1), (152, 1), (253, 1), (300, 1), (308, 1), (649, 1), (677, 1), (678, 1), (679, 1), (680, 1), (850, 1), (1072, 1), (1203, 1), (1325, 1), (1522, 1), (1526, 1), (1556, 1)]",
         "_deepnote_index_column": 16
        },
        {
         "Topics": 17,
         "Prob_distribution": "[(19, 1), (182, 1), (224, 1), (392, 1), (455, 1), (522, 1), (562, 1), (582, 1), (608, 1), (615, 1), (616, 1), (691, 1), (840, 1), (1054, 1), (1172, 1), (1222, 1), (1327, 1), (1351, 1), (1454, 1), (2200, 1)]",
         "_deepnote_index_column": 17
        },
        {
         "Topics": 18,
         "Prob_distribution": "[(2, 1), (264, 1), (280, 1), (545, 1), (732, 1), (794, 1), (866, 1), (889, 1), (1059, 1), (1067, 1), (1212, 1), (1290, 1), (1319, 1), (1412, 1), (1600, 1), (1639, 1), (1751, 1), (1760, 1), (1826, 1), (2025, 1)]",
         "_deepnote_index_column": 18
        },
        {
         "Topics": 19,
         "Prob_distribution": "[(41, 1), (80, 1), (174, 1), (396, 1), (406, 1), (453, 1), (698, 1), (742, 1), (772, 1), (773, 1), (775, 1), (776, 1), (898, 1), (899, 1), (962, 1), (967, 1), (1109, 1), (1131, 1), (1244, 1), (1464, 1)]",
         "_deepnote_index_column": 19
        },
        {
         "Topics": 20,
         "Prob_distribution": "[(0, 1), (29, 1), (139, 1), (181, 1), (294, 1), (309, 1), (371, 1), (372, 1), (398, 1), (415, 1), (539, 1), (607, 1), (767, 1), (1348, 1), (1398, 1), (1647, 1), (1755, 1), (1815, 1), (1895, 1), (2301, 1)]",
         "_deepnote_index_column": 20
        },
        {
         "Topics": 21,
         "Prob_distribution": "[(32, 1), (96, 1), (144, 1), (147, 1), (240, 1), (259, 1), (358, 1), (523, 1), (555, 1), (581, 1), (585, 1), (661, 1), (897, 1), (996, 1), (1020, 1), (1031, 1), (1287, 1), (1853, 1), (2029, 1), (2256, 1)]",
         "_deepnote_index_column": 21
        },
        {
         "Topics": 22,
         "Prob_distribution": "[(31, 1), (121, 1), (245, 1), (421, 1), (422, 1), (423, 1), (424, 1), (425, 1), (427, 1), (469, 1), (514, 1), (567, 1), (795, 1), (1207, 1), (1208, 1), (1310, 1), (1411, 1), (1433, 1), (2148, 1), (2301, 1)]",
         "_deepnote_index_column": 22
        },
        {
         "Topics": 23,
         "Prob_distribution": "[(166, 1), (177, 1), (249, 1), (360, 1), (511, 1), (529, 1), (638, 1), (640, 1), (641, 1), (731, 1), (779, 1), (862, 1), (970, 1), (999, 1), (1074, 1), (1228, 1), (1229, 1), (1424, 1), (1434, 1), (1814, 1)]",
         "_deepnote_index_column": 23
        },
        {
         "Topics": 24,
         "Prob_distribution": "[(18, 1), (23, 1), (173, 1), (390, 1), (393, 1), (394, 1), (472, 1), (478, 1), (524, 1), (693, 1), (851, 1), (860, 1), (1018, 1), (1423, 1), (1627, 1), (1763, 1), (1779, 1), (1918, 1), (1967, 1), (2155, 1)]",
         "_deepnote_index_column": 24
        },
        {
         "Topics": 25,
         "Prob_distribution": "[(42, 1), (101, 1), (263, 1), (281, 1), (316, 1), (327, 1), (363, 1), (412, 1), (413, 1), (414, 1), (622, 1), (626, 1), (632, 1), (704, 1), (865, 1), (903, 1), (1158, 1), (1403, 1), (1719, 1), (2260, 1)]",
         "_deepnote_index_column": 25
        },
        {
         "Topics": 26,
         "Prob_distribution": "[(63, 1), (205, 1), (527, 1), (592, 1), (593, 1), (770, 1), (791, 1), (813, 1), (856, 1), (976, 1), (989, 1), (1272, 1), (1379, 1), (1513, 1), (1559, 1), (1668, 1), (1839, 1), (1841, 1), (2272, 1), (2301, 1)]",
         "_deepnote_index_column": 26
        },
        {
         "Topics": 27,
         "Prob_distribution": "[(67, 1), (85, 1), (337, 1), (404, 1), (405, 1), (501, 1), (517, 1), (662, 1), (683, 1), (823, 1), (888, 1), (942, 1), (953, 1), (1002, 1), (1111, 1), (1151, 1), (1306, 1), (1473, 1), (1840, 1), (1949, 1)]",
         "_deepnote_index_column": 27
        },
        {
         "Topics": 28,
         "Prob_distribution": "[(25, 1), (143, 1), (272, 1), (322, 1), (573, 1), (574, 1), (575, 1), (629, 1), (665, 1), (863, 1), (1009, 1), (1010, 1), (1243, 1), (1323, 1), (1350, 1), (1396, 1), (1626, 1), (1660, 1), (2223, 1), (2301, 1)]",
         "_deepnote_index_column": 28
        },
        {
         "Topics": 29,
         "Prob_distribution": "[(56, 1), (57, 1), (60, 1), (61, 1), (130, 1), (132, 1), (368, 1), (785, 1), (805, 1), (1143, 1), (1173, 1), (1181, 1), (1226, 1), (1330, 1), (1365, 1), (1482, 1), (1582, 1), (1583, 1), (2046, 1), (2164, 1)]",
         "_deepnote_index_column": 29
        },
        {
         "Topics": 30,
         "Prob_distribution": "[(86, 1), (267, 1), (296, 1), (297, 1), (298, 1), (299, 1), (342, 1), (343, 1), (554, 1), (572, 1), (598, 1), (600, 1), (669, 1), (904, 1), (958, 1), (1039, 1), (1049, 1), (1190, 1), (1586, 1), (1808, 1)]",
         "_deepnote_index_column": 30
        },
        {
         "Topics": 31,
         "Prob_distribution": "[(329, 1), (515, 1), (751, 1), (752, 1), (771, 1), (836, 1), (1026, 1), (1038, 1), (1314, 1), (1370, 1), (1568, 1), (1614, 1), (1662, 1), (1685, 1), (1687, 1), (1691, 1), (1692, 1), (1693, 1), (1935, 1), (2055, 1)]",
         "_deepnote_index_column": 31
        },
        {
         "Topics": 32,
         "Prob_distribution": "[(36, 1), (68, 1), (157, 1), (195, 1), (235, 1), (278, 1), (440, 1), (590, 1), (657, 1), (684, 1), (1129, 1), (1165, 1), (1241, 1), (1245, 1), (1376, 1), (1507, 1), (1939, 1), (1998, 1), (2093, 1), (2167, 1)]",
         "_deepnote_index_column": 32
        },
        {
         "Topics": 33,
         "Prob_distribution": "[(89, 1), (163, 1), (175, 1), (189, 1), (207, 1), (210, 1), (214, 1), (225, 1), (311, 1), (315, 1), (377, 1), (431, 1), (436, 1), (520, 1), (706, 1), (892, 1), (1420, 1), (1493, 1), (1736, 1), (2042, 1)]",
         "_deepnote_index_column": 33
        },
        {
         "Topics": 34,
         "Prob_distribution": "[(111, 1), (212, 1), (268, 1), (419, 1), (460, 1), (1023, 1), (1046, 1), (1114, 1), (1133, 1), (1418, 1), (1496, 1), (1538, 1), (1575, 1), (1633, 1), (1780, 1), (1938, 1), (2221, 1), (2226, 1), (2235, 1), (2245, 1)]",
         "_deepnote_index_column": 34
        },
        {
         "Topics": 35,
         "Prob_distribution": "[(34, 1), (87, 1), (171, 1), (203, 1), (226, 1), (295, 1), (486, 1), (538, 1), (710, 1), (1635, 1), (1883, 1), (2017, 1), (2216, 1), (2242, 1), (2243, 1), (2244, 1), (2247, 1), (2296, 1), (2298, 1), (2301, 1)]",
         "_deepnote_index_column": 35
        },
        {
         "Topics": 36,
         "Prob_distribution": "[(77, 1), (208, 1), (217, 1), (276, 1), (277, 1), (370, 1), (553, 1), (580, 1), (643, 1), (655, 1), (696, 1), (746, 1), (801, 1), (1328, 1), (1361, 1), (1369, 1), (1371, 1), (1816, 1), (1986, 1), (2008, 1)]",
         "_deepnote_index_column": 36
        },
        {
         "Topics": 37,
         "Prob_distribution": "[(356, 1), (399, 1), (400, 1), (402, 1), (533, 1), (541, 1), (694, 1), (726, 1), (1085, 1), (1215, 1), (1304, 1), (1320, 1), (1340, 1), (1597, 1), (1605, 1), (1747, 1), (1835, 1), (2052, 1), (2088, 1), (2131, 1)]",
         "_deepnote_index_column": 37
        },
        {
         "Topics": 38,
         "Prob_distribution": "[(74, 1), (103, 1), (167, 1), (220, 1), (445, 1), (596, 1), (646, 1), (707, 1), (748, 1), (749, 1), (951, 1), (1069, 1), (1148, 1), (1186, 1), (1307, 1), (1385, 1), (1386, 1), (1504, 1), (1543, 1), (2147, 1)]",
         "_deepnote_index_column": 38
        },
        {
         "Topics": 39,
         "Prob_distribution": "[(20, 1), (22, 1), (51, 1), (122, 1), (202, 1), (271, 1), (410, 1), (411, 1), (446, 1), (449, 1), (500, 1), (652, 1), (654, 1), (712, 1), (768, 1), (1262, 1), (1499, 1), (1525, 1), (1714, 1), (1753, 1)]",
         "_deepnote_index_column": 39
        },
        {
         "Topics": 40,
         "Prob_distribution": "[(215, 1), (306, 1), (454, 1), (769, 1), (784, 1), (880, 1), (910, 1), (918, 1), (919, 1), (921, 1), (1024, 1), (1128, 1), (1440, 1), (1593, 1), (1612, 1), (1884, 1), (1897, 1), (2138, 1), (2257, 1), (2287, 1)]",
         "_deepnote_index_column": 40
        },
        {
         "Topics": 41,
         "Prob_distribution": "[(90, 1), (91, 1), (92, 1), (94, 1), (95, 1), (110, 1), (290, 1), (437, 1), (535, 1), (668, 1), (743, 1), (945, 1), (972, 1), (1130, 1), (1177, 1), (1191, 1), (1487, 1), (1569, 1), (1589, 1), (1671, 1)]",
         "_deepnote_index_column": 41
        },
        {
         "Topics": 42,
         "Prob_distribution": "[(100, 1), (155, 1), (239, 1), (287, 1), (328, 1), (395, 1), (656, 1), (881, 1), (900, 1), (901, 1), (902, 1), (993, 1), (1021, 1), (1125, 1), (1204, 1), (1462, 1), (1530, 1), (1776, 1), (2044, 1), (2182, 1)]",
         "_deepnote_index_column": 42
        },
        {
         "Topics": 43,
         "Prob_distribution": "[(120, 1), (265, 1), (302, 1), (467, 1), (468, 1), (896, 1), (950, 1), (957, 1), (997, 1), (998, 1), (1136, 1), (1268, 1), (1333, 1), (1367, 1), (1381, 1), (1455, 1), (1550, 1), (1551, 1), (1786, 1), (2050, 1)]",
         "_deepnote_index_column": 43
        },
        {
         "Topics": 44,
         "Prob_distribution": "[(10, 1), (50, 1), (55, 1), (209, 1), (222, 1), (261, 1), (291, 1), (509, 1), (543, 1), (544, 1), (579, 1), (762, 1), (807, 1), (882, 1), (1004, 1), (1087, 1), (1101, 1), (1548, 1), (1621, 1), (2020, 1)]",
         "_deepnote_index_column": 44
        },
        {
         "Topics": 45,
         "Prob_distribution": "[(159, 1), (161, 1), (162, 1), (172, 1), (355, 1), (408, 1), (418, 1), (448, 1), (685, 1), (756, 1), (858, 1), (895, 1), (1246, 1), (1258, 1), (1266, 1), (1332, 1), (1458, 1), (1489, 1), (1987, 1), (2236, 1)]",
         "_deepnote_index_column": 45
        },
        {
         "Topics": 46,
         "Prob_distribution": "[(70, 1), (115, 1), (409, 1), (490, 1), (550, 1), (551, 1), (552, 1), (659, 1), (969, 1), (1073, 1), (1083, 1), (1247, 1), (1252, 1), (1253, 1), (1283, 1), (1406, 1), (1545, 1), (1609, 1), (1673, 1), (1824, 1)]",
         "_deepnote_index_column": 46
        },
        {
         "Topics": 47,
         "Prob_distribution": "[(206, 1), (353, 1), (354, 1), (397, 1), (428, 1), (450, 1), (576, 1), (808, 1), (809, 1), (1007, 1), (1036, 1), (1060, 1), (1141, 1), (1242, 1), (1296, 1), (1467, 1), (1701, 1), (2278, 1), (2279, 1), (2280, 1)]",
         "_deepnote_index_column": 47
        },
        {
         "Topics": 48,
         "Prob_distribution": "[(137, 1), (138, 1), (146, 1), (242, 1), (243, 1), (258, 1), (526, 1), (568, 1), (650, 1), (651, 1), (653, 1), (664, 1), (667, 1), (689, 1), (733, 1), (886, 1), (905, 1), (1269, 1), (1395, 1), (2059, 1)]",
         "_deepnote_index_column": 48
        },
        {
         "Topics": 49,
         "Prob_distribution": "[(39, 1), (97, 1), (98, 1), (124, 1), (184, 1), (186, 1), (187, 1), (503, 1), (504, 1), (528, 1), (537, 1), (625, 1), (790, 1), (1118, 1), (1123, 1), (1206, 1), (1477, 1), (1790, 1), (1795, 1), (1882, 1)]",
         "_deepnote_index_column": 49
        },
        {
         "Topics": 50,
         "Prob_distribution": "[(54, 1), (112, 1), (127, 1), (176, 1), (252, 1), (426, 1), (441, 1), (442, 1), (462, 1), (513, 1), (566, 1), (611, 1), (617, 1), (620, 1), (1135, 1), (1209, 1), (1624, 1), (1822, 1), (1957, 1), (2301, 1)]",
         "_deepnote_index_column": 50
        },
        {
         "Topics": 51,
         "Prob_distribution": "[(48, 1), (153, 1), (211, 1), (293, 1), (340, 1), (439, 1), (518, 1), (519, 1), (695, 1), (754, 1), (825, 1), (1056, 1), (1097, 1), (1219, 1), (1251, 1), (1312, 1), (1508, 1), (1561, 1), (1985, 1), (2247, 1)]",
         "_deepnote_index_column": 51
        },
        {
         "Topics": 52,
         "Prob_distribution": "[(8, 1), (9, 1), (12, 1), (64, 1), (99, 1), (168, 1), (282, 1), (286, 1), (357, 1), (386, 1), (401, 1), (663, 1), (715, 1), (716, 1), (861, 1), (894, 1), (955, 1), (1322, 1), (1342, 1), (2051, 1)]",
         "_deepnote_index_column": 52
        },
        {
         "Topics": 53,
         "Prob_distribution": "[(106, 1), (148, 1), (279, 1), (584, 1), (817, 1), (873, 1), (978, 1), (981, 1), (985, 1), (987, 1), (988, 1), (1075, 1), (1086, 1), (1159, 1), (1162, 1), (1359, 1), (1481, 1), (1503, 1), (1585, 1), (1733, 1)]",
         "_deepnote_index_column": 53
        },
        {
         "Topics": 54,
         "Prob_distribution": "[(228, 1), (234, 1), (288, 1), (332, 1), (642, 1), (786, 1), (818, 1), (820, 1), (848, 1), (884, 1), (971, 1), (983, 1), (984, 1), (1476, 1), (1509, 1), (1812, 1), (1916, 1), (2043, 1), (2208, 1), (2300, 1)]",
         "_deepnote_index_column": 54
        },
        {
         "Topics": 55,
         "Prob_distribution": "[(58, 1), (135, 1), (165, 1), (260, 1), (292, 1), (391, 1), (479, 1), (675, 1), (702, 1), (811, 1), (1094, 1), (1120, 1), (1127, 1), (1227, 1), (1324, 1), (1438, 1), (1474, 1), (1871, 1), (2145, 1), (2238, 1)]",
         "_deepnote_index_column": 55
        },
        {
         "Topics": 56,
         "Prob_distribution": "[(133, 1), (150, 1), (307, 1), (325, 1), (334, 1), (374, 1), (463, 1), (708, 1), (857, 1), (869, 1), (871, 1), (994, 1), (995, 1), (1006, 1), (1025, 1), (1040, 1), (1062, 1), (1138, 1), (1349, 1), (2002, 1)]",
         "_deepnote_index_column": 56
        }
       ],
       "rows_bottom": null
      },
      "text/plain": "    Topics                                  Prob_distribution\n0        0  [(27, 1), (40, 1), (266, 1), (659, 1), (963, 1...\n1        1  [(37, 1), (76, 1), (78, 1), (79, 1), (270, 1),...\n2        2  [(44, 1), (131, 1), (185, 1), (350, 1), (351, ...\n3        3  [(348, 1), (382, 1), (383, 1), (384, 1), (456,...\n4        4  [(21, 1), (75, 1), (319, 1), (381, 1), (387, 1...\n5        5  [(59, 1), (123, 1), (237, 1), (241, 1), (536, ...\n6        6  [(140, 1), (149, 1), (178, 1), (318, 1), (336,...\n7        7  [(17, 1), (20, 1), (46, 1), (52, 1), (191, 1),...\n8        8  [(93, 1), (141, 1), (170, 1), (200, 1), (378, ...\n9        9  [(114, 1), (304, 1), (305, 1), (345, 1), (346,...\n10      10  [(84, 1), (104, 1), (105, 1), (107, 1), (193, ...\n11      11  [(26, 1), (45, 1), (71, 1), (72, 1), (73, 1), ...\n12      12  [(164, 1), (198, 1), (199, 1), (227, 1), (229,...\n13      13  [(190, 1), (339, 1), (444, 1), (826, 1), (837,...\n14      14  [(62, 1), (66, 1), (82, 1), (154, 1), (158, 1)...\n15      15  [(4, 1), (102, 1), (223, 1), (273, 1), (274, 1...\n16      16  [(5, 1), (83, 1), (116, 1), (129, 1), (152, 1)...\n17      17  [(19, 1), (182, 1), (224, 1), (392, 1), (455, ...\n18      18  [(2, 1), (264, 1), (280, 1), (545, 1), (732, 1...\n19      19  [(41, 1), (80, 1), (174, 1), (396, 1), (406, 1...\n20      20  [(0, 1), (29, 1), (139, 1), (181, 1), (294, 1)...\n21      21  [(32, 1), (96, 1), (144, 1), (147, 1), (240, 1...\n22      22  [(31, 1), (121, 1), (245, 1), (421, 1), (422, ...\n23      23  [(166, 1), (177, 1), (249, 1), (360, 1), (511,...\n24      24  [(18, 1), (23, 1), (173, 1), (390, 1), (393, 1...\n25      25  [(42, 1), (101, 1), (263, 1), (281, 1), (316, ...\n26      26  [(63, 1), (205, 1), (527, 1), (592, 1), (593, ...\n27      27  [(67, 1), (85, 1), (337, 1), (404, 1), (405, 1...\n28      28  [(25, 1), (143, 1), (272, 1), (322, 1), (573, ...\n29      29  [(56, 1), (57, 1), (60, 1), (61, 1), (130, 1),...\n30      30  [(86, 1), (267, 1), (296, 1), (297, 1), (298, ...\n31      31  [(329, 1), (515, 1), (751, 1), (752, 1), (771,...\n32      32  [(36, 1), (68, 1), (157, 1), (195, 1), (235, 1...\n33      33  [(89, 1), (163, 1), (175, 1), (189, 1), (207, ...\n34      34  [(111, 1), (212, 1), (268, 1), (419, 1), (460,...\n35      35  [(34, 1), (87, 1), (171, 1), (203, 1), (226, 1...\n36      36  [(77, 1), (208, 1), (217, 1), (276, 1), (277, ...\n37      37  [(356, 1), (399, 1), (400, 1), (402, 1), (533,...\n38      38  [(74, 1), (103, 1), (167, 1), (220, 1), (445, ...\n39      39  [(20, 1), (22, 1), (51, 1), (122, 1), (202, 1)...\n40      40  [(215, 1), (306, 1), (454, 1), (769, 1), (784,...\n41      41  [(90, 1), (91, 1), (92, 1), (94, 1), (95, 1), ...\n42      42  [(100, 1), (155, 1), (239, 1), (287, 1), (328,...\n43      43  [(120, 1), (265, 1), (302, 1), (467, 1), (468,...\n44      44  [(10, 1), (50, 1), (55, 1), (209, 1), (222, 1)...\n45      45  [(159, 1), (161, 1), (162, 1), (172, 1), (355,...\n46      46  [(70, 1), (115, 1), (409, 1), (490, 1), (550, ...\n47      47  [(206, 1), (353, 1), (354, 1), (397, 1), (428,...\n48      48  [(137, 1), (138, 1), (146, 1), (242, 1), (243,...\n49      49  [(39, 1), (97, 1), (98, 1), (124, 1), (184, 1)...\n50      50  [(54, 1), (112, 1), (127, 1), (176, 1), (252, ...\n51      51  [(48, 1), (153, 1), (211, 1), (293, 1), (340, ...\n52      52  [(8, 1), (9, 1), (12, 1), (64, 1), (99, 1), (1...\n53      53  [(106, 1), (148, 1), (279, 1), (584, 1), (817,...\n54      54  [(228, 1), (234, 1), (288, 1), (332, 1), (642,...\n55      55  [(58, 1), (135, 1), (165, 1), (260, 1), (292, ...\n56      56  [(133, 1), (150, 1), (307, 1), (325, 1), (334,...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topics</th>\n      <th>Prob_distribution</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>[(27, 1), (40, 1), (266, 1), (659, 1), (963, 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>[(37, 1), (76, 1), (78, 1), (79, 1), (270, 1),...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>[(44, 1), (131, 1), (185, 1), (350, 1), (351, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[(348, 1), (382, 1), (383, 1), (384, 1), (456,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[(21, 1), (75, 1), (319, 1), (381, 1), (387, 1...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>[(59, 1), (123, 1), (237, 1), (241, 1), (536, ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>[(140, 1), (149, 1), (178, 1), (318, 1), (336,...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>[(17, 1), (20, 1), (46, 1), (52, 1), (191, 1),...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>[(93, 1), (141, 1), (170, 1), (200, 1), (378, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>[(114, 1), (304, 1), (305, 1), (345, 1), (346,...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>[(84, 1), (104, 1), (105, 1), (107, 1), (193, ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>[(26, 1), (45, 1), (71, 1), (72, 1), (73, 1), ...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>[(164, 1), (198, 1), (199, 1), (227, 1), (229,...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>[(190, 1), (339, 1), (444, 1), (826, 1), (837,...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>[(62, 1), (66, 1), (82, 1), (154, 1), (158, 1)...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>[(4, 1), (102, 1), (223, 1), (273, 1), (274, 1...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>[(5, 1), (83, 1), (116, 1), (129, 1), (152, 1)...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>[(19, 1), (182, 1), (224, 1), (392, 1), (455, ...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>[(2, 1), (264, 1), (280, 1), (545, 1), (732, 1...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>[(41, 1), (80, 1), (174, 1), (396, 1), (406, 1...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>[(0, 1), (29, 1), (139, 1), (181, 1), (294, 1)...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>[(32, 1), (96, 1), (144, 1), (147, 1), (240, 1...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>[(31, 1), (121, 1), (245, 1), (421, 1), (422, ...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>[(166, 1), (177, 1), (249, 1), (360, 1), (511,...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>[(18, 1), (23, 1), (173, 1), (390, 1), (393, 1...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>[(42, 1), (101, 1), (263, 1), (281, 1), (316, ...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>[(63, 1), (205, 1), (527, 1), (592, 1), (593, ...</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>[(67, 1), (85, 1), (337, 1), (404, 1), (405, 1...</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>[(25, 1), (143, 1), (272, 1), (322, 1), (573, ...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>[(56, 1), (57, 1), (60, 1), (61, 1), (130, 1),...</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>30</td>\n      <td>[(86, 1), (267, 1), (296, 1), (297, 1), (298, ...</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>31</td>\n      <td>[(329, 1), (515, 1), (751, 1), (752, 1), (771,...</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>[(36, 1), (68, 1), (157, 1), (195, 1), (235, 1...</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>33</td>\n      <td>[(89, 1), (163, 1), (175, 1), (189, 1), (207, ...</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>34</td>\n      <td>[(111, 1), (212, 1), (268, 1), (419, 1), (460,...</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>35</td>\n      <td>[(34, 1), (87, 1), (171, 1), (203, 1), (226, 1...</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>36</td>\n      <td>[(77, 1), (208, 1), (217, 1), (276, 1), (277, ...</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>37</td>\n      <td>[(356, 1), (399, 1), (400, 1), (402, 1), (533,...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>38</td>\n      <td>[(74, 1), (103, 1), (167, 1), (220, 1), (445, ...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>39</td>\n      <td>[(20, 1), (22, 1), (51, 1), (122, 1), (202, 1)...</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>40</td>\n      <td>[(215, 1), (306, 1), (454, 1), (769, 1), (784,...</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>41</td>\n      <td>[(90, 1), (91, 1), (92, 1), (94, 1), (95, 1), ...</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>42</td>\n      <td>[(100, 1), (155, 1), (239, 1), (287, 1), (328,...</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>43</td>\n      <td>[(120, 1), (265, 1), (302, 1), (467, 1), (468,...</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>44</td>\n      <td>[(10, 1), (50, 1), (55, 1), (209, 1), (222, 1)...</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>45</td>\n      <td>[(159, 1), (161, 1), (162, 1), (172, 1), (355,...</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>46</td>\n      <td>[(70, 1), (115, 1), (409, 1), (490, 1), (550, ...</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>47</td>\n      <td>[(206, 1), (353, 1), (354, 1), (397, 1), (428,...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>48</td>\n      <td>[(137, 1), (138, 1), (146, 1), (242, 1), (243,...</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>49</td>\n      <td>[(39, 1), (97, 1), (98, 1), (124, 1), (184, 1)...</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>50</td>\n      <td>[(54, 1), (112, 1), (127, 1), (176, 1), (252, ...</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>51</td>\n      <td>[(48, 1), (153, 1), (211, 1), (293, 1), (340, ...</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>52</td>\n      <td>[(8, 1), (9, 1), (12, 1), (64, 1), (99, 1), (1...</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>53</td>\n      <td>[(106, 1), (148, 1), (279, 1), (584, 1), (817,...</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>54</td>\n      <td>[(228, 1), (234, 1), (288, 1), (332, 1), (642,...</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>55</td>\n      <td>[(58, 1), (135, 1), (165, 1), (260, 1), (292, ...</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>56</td>\n      <td>[(133, 1), (150, 1), (307, 1), (325, 1), (334,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "deepnote_to_be_reexecuted": false,
    "source_hash": "557c53be",
    "execution_millis": 6935,
    "execution_start": 1614091031319,
    "cell_id": "00105-af5c5688-ad02-424b-9229-46431f94a6c7",
    "deepnote_cell_type": "code"
   },
   "source": "topic_distances = {\"from\": [],\n                  \"to\": [],\n                  \"Hellinger_distance\": []}\n\nfor i in range(num_topics):\n    for y in range(num_topics):\n        hellinger_distance = hellinger(sum(topic_distribution.loc[(topic_distribution.Topics == i), [\"Prob_distribution\"]].values.tolist(),[]),\n                                      sum(topic_distribution.loc[(topic_distribution.Topics == y), [\"Prob_distribution\"]].values.tolist(),[]))\n        \n        topic_distances[\"from\"].append(i)\n        topic_distances[\"to\"].append(y)\n        topic_distances[\"Hellinger_distance\"].append(hellinger_distance)\n\ntopic_distances = pd.DataFrame(topic_distances)",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 22,
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "row_count": 3249,
       "column_count": 3,
       "columns": [
        {
         "name": "from",
         "dtype": "int64",
         "stats": {
          "unique_count": 57,
          "nan_count": 0,
          "min": 0,
          "max": 56,
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 5.6,
            "count": 342
           },
           {
            "bin_start": 5.6,
            "bin_end": 11.2,
            "count": 342
           },
           {
            "bin_start": 11.2,
            "bin_end": 16.799999999999997,
            "count": 285
           },
           {
            "bin_start": 16.799999999999997,
            "bin_end": 22.4,
            "count": 342
           },
           {
            "bin_start": 22.4,
            "bin_end": 28,
            "count": 285
           },
           {
            "bin_start": 28,
            "bin_end": 33.599999999999994,
            "count": 342
           },
           {
            "bin_start": 33.599999999999994,
            "bin_end": 39.199999999999996,
            "count": 342
           },
           {
            "bin_start": 39.199999999999996,
            "bin_end": 44.8,
            "count": 285
           },
           {
            "bin_start": 44.8,
            "bin_end": 50.4,
            "count": 342
           },
           {
            "bin_start": 50.4,
            "bin_end": 56,
            "count": 342
           }
          ]
         }
        },
        {
         "name": "to",
         "dtype": "int64",
         "stats": {
          "unique_count": 57,
          "nan_count": 0,
          "min": 0,
          "max": 56,
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 5.6,
            "count": 342
           },
           {
            "bin_start": 5.6,
            "bin_end": 11.2,
            "count": 342
           },
           {
            "bin_start": 11.2,
            "bin_end": 16.799999999999997,
            "count": 285
           },
           {
            "bin_start": 16.799999999999997,
            "bin_end": 22.4,
            "count": 342
           },
           {
            "bin_start": 22.4,
            "bin_end": 28,
            "count": 285
           },
           {
            "bin_start": 28,
            "bin_end": 33.599999999999994,
            "count": 342
           },
           {
            "bin_start": 33.599999999999994,
            "bin_end": 39.199999999999996,
            "count": 342
           },
           {
            "bin_start": 39.199999999999996,
            "bin_end": 44.8,
            "count": 285
           },
           {
            "bin_start": 44.8,
            "bin_end": 50.4,
            "count": 342
           },
           {
            "bin_start": 50.4,
            "bin_end": 56,
            "count": 342
           }
          ]
         }
        },
        {
         "name": "Hellinger_distance",
         "dtype": "float64",
         "stats": {
          "unique_count": 1597,
          "nan_count": 0,
          "min": 0,
          "max": 50.45853566243293,
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 5.045853566243293,
            "count": 57
           },
           {
            "bin_start": 5.045853566243293,
            "bin_end": 10.091707132486587,
            "count": 254
           },
           {
            "bin_start": 10.091707132486587,
            "bin_end": 15.13756069872988,
            "count": 864
           },
           {
            "bin_start": 15.13756069872988,
            "bin_end": 20.183414264973173,
            "count": 814
           },
           {
            "bin_start": 20.183414264973173,
            "bin_end": 25.229267831216468,
            "count": 540
           },
           {
            "bin_start": 25.229267831216468,
            "bin_end": 30.27512139745976,
            "count": 352
           },
           {
            "bin_start": 30.27512139745976,
            "bin_end": 35.32097496370305,
            "count": 182
           },
           {
            "bin_start": 35.32097496370305,
            "bin_end": 40.366828529946346,
            "count": 120
           },
           {
            "bin_start": 40.366828529946346,
            "bin_end": 45.41268209618964,
            "count": 46
           },
           {
            "bin_start": 45.41268209618964,
            "bin_end": 50.45853566243293,
            "count": 20
           }
          ]
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "rows_top": [
        {
         "from": 0,
         "to": 0,
         "Hellinger_distance": 0,
         "_deepnote_index_column": 0
        },
        {
         "from": 0,
         "to": 1,
         "Hellinger_distance": 39.22929418696471,
         "_deepnote_index_column": 1
        },
        {
         "from": 0,
         "to": 2,
         "Hellinger_distance": 41.57454906710709,
         "_deepnote_index_column": 2
        },
        {
         "from": 0,
         "to": 3,
         "Hellinger_distance": 21.161725014951625,
         "_deepnote_index_column": 3
        },
        {
         "from": 0,
         "to": 4,
         "Hellinger_distance": 25.185514245704283,
         "_deepnote_index_column": 4
        },
        {
         "from": 0,
         "to": 5,
         "Hellinger_distance": 22.224030968076875,
         "_deepnote_index_column": 5
        },
        {
         "from": 0,
         "to": 6,
         "Hellinger_distance": 38.42330870855102,
         "_deepnote_index_column": 6
        },
        {
         "from": 0,
         "to": 7,
         "Hellinger_distance": 29.822767390003154,
         "_deepnote_index_column": 7
        },
        {
         "from": 0,
         "to": 8,
         "Hellinger_distance": 23.922950050108952,
         "_deepnote_index_column": 8
        },
        {
         "from": 0,
         "to": 9,
         "Hellinger_distance": 35.66785229262956,
         "_deepnote_index_column": 9
        },
        {
         "from": 0,
         "to": 10,
         "Hellinger_distance": 40.34149220663495,
         "_deepnote_index_column": 10
        },
        {
         "from": 0,
         "to": 11,
         "Hellinger_distance": 43.34267051678725,
         "_deepnote_index_column": 11
        },
        {
         "from": 0,
         "to": 12,
         "Hellinger_distance": 45.78247917584041,
         "_deepnote_index_column": 12
        },
        {
         "from": 0,
         "to": 13,
         "Hellinger_distance": 12.112909403140305,
         "_deepnote_index_column": 13
        },
        {
         "from": 0,
         "to": 14,
         "Hellinger_distance": 32.997664614289015,
         "_deepnote_index_column": 14
        },
        {
         "from": 0,
         "to": 15,
         "Hellinger_distance": 47.08500791712577,
         "_deepnote_index_column": 15
        },
        {
         "from": 0,
         "to": 16,
         "Hellinger_distance": 41.071179616826825,
         "_deepnote_index_column": 16
        },
        {
         "from": 0,
         "to": 17,
         "Hellinger_distance": 33.47162277201349,
         "_deepnote_index_column": 17
        },
        {
         "from": 0,
         "to": 18,
         "Hellinger_distance": 16.92023956381095,
         "_deepnote_index_column": 18
        },
        {
         "from": 0,
         "to": 19,
         "Hellinger_distance": 34.1277478795555,
         "_deepnote_index_column": 19
        },
        {
         "from": 0,
         "to": 20,
         "Hellinger_distance": 37.957652393920846,
         "_deepnote_index_column": 20
        },
        {
         "from": 0,
         "to": 21,
         "Hellinger_distance": 37.85443363676089,
         "_deepnote_index_column": 21
        },
        {
         "from": 0,
         "to": 22,
         "Hellinger_distance": 36.426328009655585,
         "_deepnote_index_column": 22
        },
        {
         "from": 0,
         "to": 23,
         "Hellinger_distance": 31.58887735277348,
         "_deepnote_index_column": 23
        },
        {
         "from": 0,
         "to": 24,
         "Hellinger_distance": 28.89353495152249,
         "_deepnote_index_column": 24
        },
        {
         "from": 0,
         "to": 25,
         "Hellinger_distance": 41.82349419100496,
         "_deepnote_index_column": 25
        },
        {
         "from": 0,
         "to": 26,
         "Hellinger_distance": 18.598658294540883,
         "_deepnote_index_column": 26
        },
        {
         "from": 0,
         "to": 27,
         "Hellinger_distance": 29.589012462296694,
         "_deepnote_index_column": 27
        },
        {
         "from": 0,
         "to": 28,
         "Hellinger_distance": 24.48477099206783,
         "_deepnote_index_column": 28
        },
        {
         "from": 0,
         "to": 29,
         "Hellinger_distance": 32.160101621818015,
         "_deepnote_index_column": 29
        },
        {
         "from": 0,
         "to": 30,
         "Hellinger_distance": 41.51551052164761,
         "_deepnote_index_column": 30
        },
        {
         "from": 0,
         "to": 31,
         "Hellinger_distance": 19.279876370115964,
         "_deepnote_index_column": 31
        },
        {
         "from": 0,
         "to": 32,
         "Hellinger_distance": 29.021884550516372,
         "_deepnote_index_column": 32
        },
        {
         "from": 0,
         "to": 33,
         "Hellinger_distance": 50.45853566243293,
         "_deepnote_index_column": 33
        },
        {
         "from": 0,
         "to": 34,
         "Hellinger_distance": 12.424556041339962,
         "_deepnote_index_column": 34
        },
        {
         "from": 0,
         "to": 35,
         "Hellinger_distance": 25.04636995203977,
         "_deepnote_index_column": 35
        },
        {
         "from": 0,
         "to": 36,
         "Hellinger_distance": 32.381391870659904,
         "_deepnote_index_column": 36
        },
        {
         "from": 0,
         "to": 37,
         "Hellinger_distance": 21.721783558011502,
         "_deepnote_index_column": 37
        },
        {
         "from": 0,
         "to": 38,
         "Hellinger_distance": 26.94872477060453,
         "_deepnote_index_column": 38
        },
        {
         "from": 0,
         "to": 39,
         "Hellinger_distance": 42.862383717516934,
         "_deepnote_index_column": 39
        },
        {
         "from": 0,
         "to": 40,
         "Hellinger_distance": 18.520891661707083,
         "_deepnote_index_column": 40
        },
        {
         "from": 0,
         "to": 41,
         "Hellinger_distance": 40.03047450153756,
         "_deepnote_index_column": 41
        },
        {
         "from": 0,
         "to": 42,
         "Hellinger_distance": 26.322145208012135,
         "_deepnote_index_column": 42
        },
        {
         "from": 0,
         "to": 43,
         "Hellinger_distance": 19.933951298759112,
         "_deepnote_index_column": 43
        },
        {
         "from": 0,
         "to": 44,
         "Hellinger_distance": 39.764028925391905,
         "_deepnote_index_column": 44
        },
        {
         "from": 0,
         "to": 45,
         "Hellinger_distance": 31.491726031067387,
         "_deepnote_index_column": 45
        },
        {
         "from": 0,
         "to": 46,
         "Hellinger_distance": 22.859256393274052,
         "_deepnote_index_column": 46
        },
        {
         "from": 0,
         "to": 47,
         "Hellinger_distance": 25.292843263421627,
         "_deepnote_index_column": 47
        },
        {
         "from": 0,
         "to": 48,
         "Hellinger_distance": 41.28934098373874,
         "_deepnote_index_column": 48
        },
        {
         "from": 0,
         "to": 49,
         "Hellinger_distance": 40.81086501831312,
         "_deepnote_index_column": 49
        },
        {
         "from": 0,
         "to": 50,
         "Hellinger_distance": 39.28913690817291,
         "_deepnote_index_column": 50
        },
        {
         "from": 0,
         "to": 51,
         "Hellinger_distance": 29.11818275194667,
         "_deepnote_index_column": 51
        },
        {
         "from": 0,
         "to": 52,
         "Hellinger_distance": 49.565972541248286,
         "_deepnote_index_column": 52
        },
        {
         "from": 0,
         "to": 53,
         "Hellinger_distance": 21.8860198864032,
         "_deepnote_index_column": 53
        },
        {
         "from": 0,
         "to": 54,
         "Hellinger_distance": 22.157566139967557,
         "_deepnote_index_column": 54
        },
        {
         "from": 0,
         "to": 55,
         "Hellinger_distance": 27.337317455692666,
         "_deepnote_index_column": 55
        },
        {
         "from": 0,
         "to": 56,
         "Hellinger_distance": 33.03288476052573,
         "_deepnote_index_column": 56
        },
        {
         "from": 1,
         "to": 0,
         "Hellinger_distance": 39.22929418696471,
         "_deepnote_index_column": 57
        },
        {
         "from": 1,
         "to": 1,
         "Hellinger_distance": 0,
         "_deepnote_index_column": 58
        },
        {
         "from": 1,
         "to": 2,
         "Hellinger_distance": 14.879665400722178,
         "_deepnote_index_column": 59
        },
        {
         "from": 1,
         "to": 3,
         "Hellinger_distance": 28.001175522724328,
         "_deepnote_index_column": 60
        },
        {
         "from": 1,
         "to": 4,
         "Hellinger_distance": 21.649042050138135,
         "_deepnote_index_column": 61
        },
        {
         "from": 1,
         "to": 5,
         "Hellinger_distance": 19.177014451642155,
         "_deepnote_index_column": 62
        },
        {
         "from": 1,
         "to": 6,
         "Hellinger_distance": 12.727317972283009,
         "_deepnote_index_column": 63
        },
        {
         "from": 1,
         "to": 7,
         "Hellinger_distance": 19.86598864101544,
         "_deepnote_index_column": 64
        },
        {
         "from": 1,
         "to": 8,
         "Hellinger_distance": 19.611683094586898,
         "_deepnote_index_column": 65
        },
        {
         "from": 1,
         "to": 9,
         "Hellinger_distance": 14.623932569063413,
         "_deepnote_index_column": 66
        },
        {
         "from": 1,
         "to": 10,
         "Hellinger_distance": 8.836382586830725,
         "_deepnote_index_column": 67
        },
        {
         "from": 1,
         "to": 11,
         "Hellinger_distance": 9.275371022040561,
         "_deepnote_index_column": 68
        },
        {
         "from": 1,
         "to": 12,
         "Hellinger_distance": 16.940194830451365,
         "_deepnote_index_column": 69
        },
        {
         "from": 1,
         "to": 13,
         "Hellinger_distance": 39.21371188415245,
         "_deepnote_index_column": 70
        },
        {
         "from": 1,
         "to": 14,
         "Hellinger_distance": 13.717212128912923,
         "_deepnote_index_column": 71
        },
        {
         "from": 1,
         "to": 15,
         "Hellinger_distance": 17.2550589463011,
         "_deepnote_index_column": 72
        },
        {
         "from": 1,
         "to": 16,
         "Hellinger_distance": 9.904419843916088,
         "_deepnote_index_column": 73
        },
        {
         "from": 1,
         "to": 17,
         "Hellinger_distance": 14.454139997957792,
         "_deepnote_index_column": 74
        },
        {
         "from": 1,
         "to": 18,
         "Hellinger_distance": 26.820312535755438,
         "_deepnote_index_column": 75
        },
        {
         "from": 1,
         "to": 19,
         "Hellinger_distance": 18.996594898102522,
         "_deepnote_index_column": 76
        },
        {
         "from": 1,
         "to": 20,
         "Hellinger_distance": 10.275819980428546,
         "_deepnote_index_column": 77
        },
        {
         "from": 1,
         "to": 21,
         "Hellinger_distance": 6.803855519847696,
         "_deepnote_index_column": 78
        },
        {
         "from": 1,
         "to": 22,
         "Hellinger_distance": 12.318544359340693,
         "_deepnote_index_column": 79
        },
        {
         "from": 1,
         "to": 23,
         "Hellinger_distance": 16.840543219380105,
         "_deepnote_index_column": 80
        },
        {
         "from": 1,
         "to": 24,
         "Hellinger_distance": 14.338583739663585,
         "_deepnote_index_column": 81
        },
        {
         "from": 1,
         "to": 25,
         "Hellinger_distance": 11.782285014361221,
         "_deepnote_index_column": 82
        },
        {
         "from": 1,
         "to": 26,
         "Hellinger_distance": 25.95982607149779,
         "_deepnote_index_column": 83
        },
        {
         "from": 1,
         "to": 27,
         "Hellinger_distance": 15.458023110285009,
         "_deepnote_index_column": 84
        },
        {
         "from": 1,
         "to": 28,
         "Hellinger_distance": 17.009689959763833,
         "_deepnote_index_column": 85
        },
        {
         "from": 1,
         "to": 29,
         "Hellinger_distance": 16.960401799081204,
         "_deepnote_index_column": 86
        },
        {
         "from": 1,
         "to": 30,
         "Hellinger_distance": 14.144979062035414,
         "_deepnote_index_column": 87
        },
        {
         "from": 1,
         "to": 31,
         "Hellinger_distance": 37.74805585135276,
         "_deepnote_index_column": 88
        },
        {
         "from": 1,
         "to": 32,
         "Hellinger_distance": 14.06311051547916,
         "_deepnote_index_column": 89
        },
        {
         "from": 1,
         "to": 33,
         "Hellinger_distance": 15.24530828782261,
         "_deepnote_index_column": 90
        },
        {
         "from": 1,
         "to": 34,
         "Hellinger_distance": 32.285808088974505,
         "_deepnote_index_column": 91
        },
        {
         "from": 1,
         "to": 35,
         "Hellinger_distance": 32.99299896673652,
         "_deepnote_index_column": 92
        },
        {
         "from": 1,
         "to": 36,
         "Hellinger_distance": 11.808284326316679,
         "_deepnote_index_column": 93
        },
        {
         "from": 1,
         "to": 37,
         "Hellinger_distance": 27.4393899604882,
         "_deepnote_index_column": 94
        },
        {
         "from": 1,
         "to": 38,
         "Hellinger_distance": 15.736854163803017,
         "_deepnote_index_column": 95
        },
        {
         "from": 1,
         "to": 39,
         "Hellinger_distance": 9.924773687682624,
         "_deepnote_index_column": 96
        },
        {
         "from": 1,
         "to": 40,
         "Hellinger_distance": 30.054556163793205,
         "_deepnote_index_column": 97
        },
        {
         "from": 1,
         "to": 41,
         "Hellinger_distance": 11.684693120053653,
         "_deepnote_index_column": 98
        },
        {
         "from": 1,
         "to": 42,
         "Hellinger_distance": 16.320737500861423,
         "_deepnote_index_column": 99
        },
        {
         "from": 1,
         "to": 43,
         "Hellinger_distance": 25.612015942786066,
         "_deepnote_index_column": 100
        },
        {
         "from": 1,
         "to": 44,
         "Hellinger_distance": 9.023449627815035,
         "_deepnote_index_column": 101
        },
        {
         "from": 1,
         "to": 45,
         "Hellinger_distance": 12.307998762135876,
         "_deepnote_index_column": 102
        },
        {
         "from": 1,
         "to": 46,
         "Hellinger_distance": 21.721257596567913,
         "_deepnote_index_column": 103
        },
        {
         "from": 1,
         "to": 47,
         "Hellinger_distance": 20.869406137548808,
         "_deepnote_index_column": 104
        },
        {
         "from": 1,
         "to": 48,
         "Hellinger_distance": 15.101778708948316,
         "_deepnote_index_column": 105
        },
        {
         "from": 1,
         "to": 49,
         "Hellinger_distance": 7.908401931664134,
         "_deepnote_index_column": 106
        },
        {
         "from": 1,
         "to": 50,
         "Hellinger_distance": 7.445416662788183,
         "_deepnote_index_column": 107
        },
        {
         "from": 1,
         "to": 51,
         "Hellinger_distance": 12.947654084277104,
         "_deepnote_index_column": 108
        },
        {
         "from": 1,
         "to": 52,
         "Hellinger_distance": 15.013550298326946,
         "_deepnote_index_column": 109
        },
        {
         "from": 1,
         "to": 53,
         "Hellinger_distance": 25.761494994066823,
         "_deepnote_index_column": 110
        },
        {
         "from": 1,
         "to": 54,
         "Hellinger_distance": 22.675694827659594,
         "_deepnote_index_column": 111
        },
        {
         "from": 1,
         "to": 55,
         "Hellinger_distance": 13.883079135518342,
         "_deepnote_index_column": 112
        },
        {
         "from": 1,
         "to": 56,
         "Hellinger_distance": 18.05831848882504,
         "_deepnote_index_column": 113
        },
        {
         "from": 2,
         "to": 0,
         "Hellinger_distance": 41.57454906710709,
         "_deepnote_index_column": 114
        },
        {
         "from": 2,
         "to": 1,
         "Hellinger_distance": 14.879665400722178,
         "_deepnote_index_column": 115
        },
        {
         "from": 2,
         "to": 2,
         "Hellinger_distance": 0,
         "_deepnote_index_column": 116
        },
        {
         "from": 2,
         "to": 3,
         "Hellinger_distance": 29.20973293342789,
         "_deepnote_index_column": 117
        },
        {
         "from": 2,
         "to": 4,
         "Hellinger_distance": 26.52007575862553,
         "_deepnote_index_column": 118
        },
        {
         "from": 2,
         "to": 5,
         "Hellinger_distance": 22.184493211619262,
         "_deepnote_index_column": 119
        },
        {
         "from": 2,
         "to": 6,
         "Hellinger_distance": 11.363584338182548,
         "_deepnote_index_column": 120
        },
        {
         "from": 2,
         "to": 7,
         "Hellinger_distance": 28.798427356600502,
         "_deepnote_index_column": 121
        },
        {
         "from": 2,
         "to": 8,
         "Hellinger_distance": 21.133499488758428,
         "_deepnote_index_column": 122
        },
        {
         "from": 2,
         "to": 9,
         "Hellinger_distance": 12.774837350668857,
         "_deepnote_index_column": 123
        },
        {
         "from": 2,
         "to": 10,
         "Hellinger_distance": 12.086597626000781,
         "_deepnote_index_column": 124
        },
        {
         "from": 2,
         "to": 11,
         "Hellinger_distance": 15.321161255839561,
         "_deepnote_index_column": 125
        },
        {
         "from": 2,
         "to": 12,
         "Hellinger_distance": 11.966865006805929,
         "_deepnote_index_column": 126
        },
        {
         "from": 2,
         "to": 13,
         "Hellinger_distance": 39.93212108412779,
         "_deepnote_index_column": 127
        },
        {
         "from": 2,
         "to": 14,
         "Hellinger_distance": 21.394505885496415,
         "_deepnote_index_column": 128
        },
        {
         "from": 2,
         "to": 15,
         "Hellinger_distance": 10.433801236893508,
         "_deepnote_index_column": 129
        },
        {
         "from": 2,
         "to": 16,
         "Hellinger_distance": 10.81889369230648,
         "_deepnote_index_column": 130
        },
        {
         "from": 2,
         "to": 17,
         "Hellinger_distance": 11.87195663386277,
         "_deepnote_index_column": 131
        },
        {
         "from": 2,
         "to": 18,
         "Hellinger_distance": 27.196716250329224,
         "_deepnote_index_column": 132
        },
        {
         "from": 2,
         "to": 19,
         "Hellinger_distance": 12.63392545892693,
         "_deepnote_index_column": 133
        },
        {
         "from": 2,
         "to": 20,
         "Hellinger_distance": 16.76808433947793,
         "_deepnote_index_column": 134
        },
        {
         "from": 2,
         "to": 21,
         "Hellinger_distance": 13.673903501717149,
         "_deepnote_index_column": 135
        },
        {
         "from": 2,
         "to": 22,
         "Hellinger_distance": 13.200448342335314,
         "_deepnote_index_column": 136
        },
        {
         "from": 2,
         "to": 23,
         "Hellinger_distance": 13.053400338646943,
         "_deepnote_index_column": 137
        },
        {
         "from": 2,
         "to": 24,
         "Hellinger_distance": 18.846091525012344,
         "_deepnote_index_column": 138
        },
        {
         "from": 2,
         "to": 25,
         "Hellinger_distance": 10.204470629409224,
         "_deepnote_index_column": 139
        },
        {
         "from": 2,
         "to": 26,
         "Hellinger_distance": 27.043213428994875,
         "_deepnote_index_column": 140
        },
        {
         "from": 2,
         "to": 27,
         "Hellinger_distance": 14.361211579223859,
         "_deepnote_index_column": 141
        },
        {
         "from": 2,
         "to": 28,
         "Hellinger_distance": 20.23339563520811,
         "_deepnote_index_column": 142
        },
        {
         "from": 2,
         "to": 29,
         "Hellinger_distance": 24.286814954827516,
         "_deepnote_index_column": 143
        },
        {
         "from": 2,
         "to": 30,
         "Hellinger_distance": 8.944107625094052,
         "_deepnote_index_column": 144
        },
        {
         "from": 2,
         "to": 31,
         "Hellinger_distance": 37.08153360067554,
         "_deepnote_index_column": 145
        },
        {
         "from": 2,
         "to": 32,
         "Hellinger_distance": 20.485773642307347,
         "_deepnote_index_column": 146
        },
        {
         "from": 2,
         "to": 33,
         "Hellinger_distance": 13.467080318291634,
         "_deepnote_index_column": 147
        },
        {
         "from": 2,
         "to": 34,
         "Hellinger_distance": 35.26496288646577,
         "_deepnote_index_column": 148
        },
        {
         "from": 2,
         "to": 35,
         "Hellinger_distance": 39.1604006852381,
         "_deepnote_index_column": 149
        },
        {
         "from": 2,
         "to": 36,
         "Hellinger_distance": 14.952404406435981,
         "_deepnote_index_column": 150
        },
        {
         "from": 2,
         "to": 37,
         "Hellinger_distance": 29.45426073262353,
         "_deepnote_index_column": 151
        },
        {
         "from": 2,
         "to": 38,
         "Hellinger_distance": 17.510747801528,
         "_deepnote_index_column": 152
        },
        {
         "from": 2,
         "to": 39,
         "Hellinger_distance": 10.876100908905135,
         "_deepnote_index_column": 153
        },
        {
         "from": 2,
         "to": 40,
         "Hellinger_distance": 30.517386778187287,
         "_deepnote_index_column": 154
        },
        {
         "from": 2,
         "to": 41,
         "Hellinger_distance": 15.531121009640858,
         "_deepnote_index_column": 155
        },
        {
         "from": 2,
         "to": 42,
         "Hellinger_distance": 18.984406157806475,
         "_deepnote_index_column": 156
        },
        {
         "from": 2,
         "to": 43,
         "Hellinger_distance": 25.420566799059486,
         "_deepnote_index_column": 157
        },
        {
         "from": 2,
         "to": 44,
         "Hellinger_distance": 11.550290444116452,
         "_deepnote_index_column": 158
        },
        {
         "from": 2,
         "to": 45,
         "Hellinger_distance": 16.87304596110974,
         "_deepnote_index_column": 159
        },
        {
         "from": 2,
         "to": 46,
         "Hellinger_distance": 20.670916711679478,
         "_deepnote_index_column": 160
        },
        {
         "from": 2,
         "to": 47,
         "Hellinger_distance": 23.56441898769613,
         "_deepnote_index_column": 161
        },
        {
         "from": 2,
         "to": 48,
         "Hellinger_distance": 11.820905710900982,
         "_deepnote_index_column": 162
        },
        {
         "from": 2,
         "to": 49,
         "Hellinger_distance": 13.347384444928613,
         "_deepnote_index_column": 163
        },
        {
         "from": 2,
         "to": 50,
         "Hellinger_distance": 12.790903533114069,
         "_deepnote_index_column": 164
        },
        {
         "from": 2,
         "to": 51,
         "Hellinger_distance": 16.09724667340817,
         "_deepnote_index_column": 165
        }
       ],
       "rows_bottom": [
        {
         "from": 54,
         "to": 4,
         "Hellinger_distance": 15.358013696615943,
         "_deepnote_index_column": 3082
        },
        {
         "from": 54,
         "to": 5,
         "Hellinger_distance": 11.70047130304447,
         "_deepnote_index_column": 3083
        },
        {
         "from": 54,
         "to": 6,
         "Hellinger_distance": 19.468628917293923,
         "_deepnote_index_column": 3084
        },
        {
         "from": 54,
         "to": 7,
         "Hellinger_distance": 22.72261968987327,
         "_deepnote_index_column": 3085
        },
        {
         "from": 54,
         "to": 8,
         "Hellinger_distance": 12.410627649789419,
         "_deepnote_index_column": 3086
        },
        {
         "from": 54,
         "to": 9,
         "Hellinger_distance": 19.1369505844775,
         "_deepnote_index_column": 3087
        },
        {
         "from": 54,
         "to": 10,
         "Hellinger_distance": 22.791439726077016,
         "_deepnote_index_column": 3088
        },
        {
         "from": 54,
         "to": 11,
         "Hellinger_distance": 27.548251432106206,
         "_deepnote_index_column": 3089
        },
        {
         "from": 54,
         "to": 12,
         "Hellinger_distance": 29.278577973838246,
         "_deepnote_index_column": 3090
        },
        {
         "from": 54,
         "to": 13,
         "Hellinger_distance": 19.325831188580626,
         "_deepnote_index_column": 3091
        },
        {
         "from": 54,
         "to": 14,
         "Hellinger_distance": 19.49211430155669,
         "_deepnote_index_column": 3092
        },
        {
         "from": 54,
         "to": 15,
         "Hellinger_distance": 31.613380059735167,
         "_deepnote_index_column": 3093
        },
        {
         "from": 54,
         "to": 16,
         "Hellinger_distance": 26.251268786360708,
         "_deepnote_index_column": 3094
        },
        {
         "from": 54,
         "to": 17,
         "Hellinger_distance": 17.995349576016768,
         "_deepnote_index_column": 3095
        },
        {
         "from": 54,
         "to": 18,
         "Hellinger_distance": 13.153998671511891,
         "_deepnote_index_column": 3096
        },
        {
         "from": 54,
         "to": 19,
         "Hellinger_distance": 22.157436446512374,
         "_deepnote_index_column": 3097
        },
        {
         "from": 54,
         "to": 20,
         "Hellinger_distance": 22.929242024916665,
         "_deepnote_index_column": 3098
        },
        {
         "from": 54,
         "to": 21,
         "Hellinger_distance": 22.00053052941051,
         "_deepnote_index_column": 3099
        },
        {
         "from": 54,
         "to": 22,
         "Hellinger_distance": 19.533202355168108,
         "_deepnote_index_column": 3100
        },
        {
         "from": 54,
         "to": 23,
         "Hellinger_distance": 15.586559961833279,
         "_deepnote_index_column": 3101
        },
        {
         "from": 54,
         "to": 24,
         "Hellinger_distance": 15.98828018777427,
         "_deepnote_index_column": 3102
        },
        {
         "from": 54,
         "to": 25,
         "Hellinger_distance": 25.269684681197518,
         "_deepnote_index_column": 3103
        },
        {
         "from": 54,
         "to": 26,
         "Hellinger_distance": 9.774024773434231,
         "_deepnote_index_column": 3104
        },
        {
         "from": 54,
         "to": 27,
         "Hellinger_distance": 15.677913019007942,
         "_deepnote_index_column": 3105
        },
        {
         "from": 54,
         "to": 28,
         "Hellinger_distance": 11.410776412366515,
         "_deepnote_index_column": 3106
        },
        {
         "from": 54,
         "to": 29,
         "Hellinger_distance": 22.271825455562475,
         "_deepnote_index_column": 3107
        },
        {
         "from": 54,
         "to": 30,
         "Hellinger_distance": 24.211622249843405,
         "_deepnote_index_column": 3108
        },
        {
         "from": 54,
         "to": 31,
         "Hellinger_distance": 18.4823855677168,
         "_deepnote_index_column": 3109
        },
        {
         "from": 54,
         "to": 32,
         "Hellinger_distance": 16.747465857747756,
         "_deepnote_index_column": 3110
        },
        {
         "from": 54,
         "to": 33,
         "Hellinger_distance": 32.12970328199782,
         "_deepnote_index_column": 3111
        },
        {
         "from": 54,
         "to": 34,
         "Hellinger_distance": 14.103107183159151,
         "_deepnote_index_column": 3112
        },
        {
         "from": 54,
         "to": 35,
         "Hellinger_distance": 25.949865096189804,
         "_deepnote_index_column": 3113
        },
        {
         "from": 54,
         "to": 36,
         "Hellinger_distance": 14.739985939696954,
         "_deepnote_index_column": 3114
        },
        {
         "from": 54,
         "to": 37,
         "Hellinger_distance": 10.301688357052134,
         "_deepnote_index_column": 3115
        },
        {
         "from": 54,
         "to": 38,
         "Hellinger_distance": 13.356297578316491,
         "_deepnote_index_column": 3116
        },
        {
         "from": 54,
         "to": 39,
         "Hellinger_distance": 27.347585081719654,
         "_deepnote_index_column": 3117
        },
        {
         "from": 54,
         "to": 40,
         "Hellinger_distance": 9.644607975421431,
         "_deepnote_index_column": 3118
        },
        {
         "from": 54,
         "to": 41,
         "Hellinger_distance": 25.208862471625718,
         "_deepnote_index_column": 3119
        },
        {
         "from": 54,
         "to": 42,
         "Hellinger_distance": 11.508347703975831,
         "_deepnote_index_column": 3120
        },
        {
         "from": 54,
         "to": 43,
         "Hellinger_distance": 10.608968256388822,
         "_deepnote_index_column": 3121
        },
        {
         "from": 54,
         "to": 44,
         "Hellinger_distance": 25.535995423375514,
         "_deepnote_index_column": 3122
        },
        {
         "from": 54,
         "to": 45,
         "Hellinger_distance": 15.085481527492698,
         "_deepnote_index_column": 3123
        },
        {
         "from": 54,
         "to": 46,
         "Hellinger_distance": 13.301648482641847,
         "_deepnote_index_column": 3124
        },
        {
         "from": 54,
         "to": 47,
         "Hellinger_distance": 9.246336473468512,
         "_deepnote_index_column": 3125
        },
        {
         "from": 54,
         "to": 48,
         "Hellinger_distance": 25.51692596989994,
         "_deepnote_index_column": 3126
        },
        {
         "from": 54,
         "to": 49,
         "Hellinger_distance": 24.640887759876655,
         "_deepnote_index_column": 3127
        },
        {
         "from": 54,
         "to": 50,
         "Hellinger_distance": 21.733619331513253,
         "_deepnote_index_column": 3128
        },
        {
         "from": 54,
         "to": 51,
         "Hellinger_distance": 14.132237591036615,
         "_deepnote_index_column": 3129
        },
        {
         "from": 54,
         "to": 52,
         "Hellinger_distance": 35.054286670789196,
         "_deepnote_index_column": 3130
        },
        {
         "from": 54,
         "to": 53,
         "Hellinger_distance": 13.101000780253125,
         "_deepnote_index_column": 3131
        },
        {
         "from": 54,
         "to": 54,
         "Hellinger_distance": 0,
         "_deepnote_index_column": 3132
        },
        {
         "from": 54,
         "to": 55,
         "Hellinger_distance": 13.667108759706185,
         "_deepnote_index_column": 3133
        },
        {
         "from": 54,
         "to": 56,
         "Hellinger_distance": 19.687471577282867,
         "_deepnote_index_column": 3134
        },
        {
         "from": 55,
         "to": 0,
         "Hellinger_distance": 27.337317455692666,
         "_deepnote_index_column": 3135
        },
        {
         "from": 55,
         "to": 1,
         "Hellinger_distance": 13.883079135518342,
         "_deepnote_index_column": 3136
        },
        {
         "from": 55,
         "to": 2,
         "Hellinger_distance": 19.213837202757215,
         "_deepnote_index_column": 3137
        },
        {
         "from": 55,
         "to": 3,
         "Hellinger_distance": 16.33811762757433,
         "_deepnote_index_column": 3138
        },
        {
         "from": 55,
         "to": 4,
         "Hellinger_distance": 13.972602268996987,
         "_deepnote_index_column": 3139
        },
        {
         "from": 55,
         "to": 5,
         "Hellinger_distance": 8.238855263153322,
         "_deepnote_index_column": 3140
        },
        {
         "from": 55,
         "to": 6,
         "Hellinger_distance": 16.501175486577466,
         "_deepnote_index_column": 3141
        },
        {
         "from": 55,
         "to": 7,
         "Hellinger_distance": 14.322488142399664,
         "_deepnote_index_column": 3142
        },
        {
         "from": 55,
         "to": 8,
         "Hellinger_distance": 9.907744597862877,
         "_deepnote_index_column": 3143
        },
        {
         "from": 55,
         "to": 9,
         "Hellinger_distance": 12.495285610593568,
         "_deepnote_index_column": 3144
        },
        {
         "from": 55,
         "to": 10,
         "Hellinger_distance": 16.29303511793164,
         "_deepnote_index_column": 3145
        },
        {
         "from": 55,
         "to": 11,
         "Hellinger_distance": 18.28578253906579,
         "_deepnote_index_column": 3146
        },
        {
         "from": 55,
         "to": 12,
         "Hellinger_distance": 21.645392534736825,
         "_deepnote_index_column": 3147
        },
        {
         "from": 55,
         "to": 13,
         "Hellinger_distance": 26.50523578751639,
         "_deepnote_index_column": 3148
        },
        {
         "from": 55,
         "to": 14,
         "Hellinger_distance": 11.020391263546806,
         "_deepnote_index_column": 3149
        },
        {
         "from": 55,
         "to": 15,
         "Hellinger_distance": 23.974256758184495,
         "_deepnote_index_column": 3150
        },
        {
         "from": 55,
         "to": 16,
         "Hellinger_distance": 16.644040292607993,
         "_deepnote_index_column": 3151
        },
        {
         "from": 55,
         "to": 17,
         "Hellinger_distance": 13.364123239589965,
         "_deepnote_index_column": 3152
        },
        {
         "from": 55,
         "to": 18,
         "Hellinger_distance": 15.76825570502663,
         "_deepnote_index_column": 3153
        },
        {
         "from": 55,
         "to": 19,
         "Hellinger_distance": 16.643236099280287,
         "_deepnote_index_column": 3154
        },
        {
         "from": 55,
         "to": 20,
         "Hellinger_distance": 15.994658891450168,
         "_deepnote_index_column": 3155
        },
        {
         "from": 55,
         "to": 21,
         "Hellinger_distance": 12.492377670734239,
         "_deepnote_index_column": 3156
        },
        {
         "from": 55,
         "to": 22,
         "Hellinger_distance": 14.520461661525902,
         "_deepnote_index_column": 3157
        },
        {
         "from": 55,
         "to": 23,
         "Hellinger_distance": 12.67139099768159,
         "_deepnote_index_column": 3158
        },
        {
         "from": 55,
         "to": 24,
         "Hellinger_distance": 10.175257076270015,
         "_deepnote_index_column": 3159
        },
        {
         "from": 55,
         "to": 25,
         "Hellinger_distance": 17.763466210710675,
         "_deepnote_index_column": 3160
        },
        {
         "from": 55,
         "to": 26,
         "Hellinger_distance": 14.65874485909695,
         "_deepnote_index_column": 3161
        },
        {
         "from": 55,
         "to": 27,
         "Hellinger_distance": 9.544237404796784,
         "_deepnote_index_column": 3162
        },
        {
         "from": 55,
         "to": 28,
         "Hellinger_distance": 7.5933063412270085,
         "_deepnote_index_column": 3163
        },
        {
         "from": 55,
         "to": 29,
         "Hellinger_distance": 11.806345742816356,
         "_deepnote_index_column": 3164
        },
        {
         "from": 55,
         "to": 30,
         "Hellinger_distance": 17.701671168281383,
         "_deepnote_index_column": 3165
        },
        {
         "from": 55,
         "to": 31,
         "Hellinger_distance": 25.58750976950052,
         "_deepnote_index_column": 3166
        },
        {
         "from": 55,
         "to": 32,
         "Hellinger_distance": 6.3613232456086415,
         "_deepnote_index_column": 3167
        },
        {
         "from": 55,
         "to": 33,
         "Hellinger_distance": 25.299621202372702,
         "_deepnote_index_column": 3168
        },
        {
         "from": 55,
         "to": 34,
         "Hellinger_distance": 20.131544570301948,
         "_deepnote_index_column": 3169
        },
        {
         "from": 55,
         "to": 35,
         "Hellinger_distance": 23.03324322652372,
         "_deepnote_index_column": 3170
        },
        {
         "from": 55,
         "to": 36,
         "Hellinger_distance": 8.788164248161165,
         "_deepnote_index_column": 3171
        },
        {
         "from": 55,
         "to": 37,
         "Hellinger_distance": 16.114011068366363,
         "_deepnote_index_column": 3172
        },
        {
         "from": 55,
         "to": 38,
         "Hellinger_distance": 8.232901142420044,
         "_deepnote_index_column": 3173
        },
        {
         "from": 55,
         "to": 39,
         "Hellinger_distance": 18.928179828820863,
         "_deepnote_index_column": 3174
        },
        {
         "from": 55,
         "to": 40,
         "Hellinger_distance": 19.2135520399015,
         "_deepnote_index_column": 3175
        },
        {
         "from": 55,
         "to": 41,
         "Hellinger_distance": 14.640736435829458,
         "_deepnote_index_column": 3176
        },
        {
         "from": 55,
         "to": 42,
         "Hellinger_distance": 6.377817166053925,
         "_deepnote_index_column": 3177
        },
        {
         "from": 55,
         "to": 43,
         "Hellinger_distance": 14.174519814719131,
         "_deepnote_index_column": 3178
        },
        {
         "from": 55,
         "to": 44,
         "Hellinger_distance": 15.237477332835866,
         "_deepnote_index_column": 3179
        },
        {
         "from": 55,
         "to": 45,
         "Hellinger_distance": 7.82195913300658,
         "_deepnote_index_column": 3180
        },
        {
         "from": 55,
         "to": 46,
         "Hellinger_distance": 11.338151357317551,
         "_deepnote_index_column": 3181
        },
        {
         "from": 55,
         "to": 47,
         "Hellinger_distance": 10.466433892675823,
         "_deepnote_index_column": 3182
        },
        {
         "from": 55,
         "to": 48,
         "Hellinger_distance": 18.2576259197307,
         "_deepnote_index_column": 3183
        },
        {
         "from": 55,
         "to": 49,
         "Hellinger_distance": 15.2934285195136,
         "_deepnote_index_column": 3184
        },
        {
         "from": 55,
         "to": 50,
         "Hellinger_distance": 15.261520090993981,
         "_deepnote_index_column": 3185
        },
        {
         "from": 55,
         "to": 51,
         "Hellinger_distance": 5.5440893167991545,
         "_deepnote_index_column": 3186
        },
        {
         "from": 55,
         "to": 52,
         "Hellinger_distance": 25.02878295311538,
         "_deepnote_index_column": 3187
        },
        {
         "from": 55,
         "to": 53,
         "Hellinger_distance": 16.87334515812157,
         "_deepnote_index_column": 3188
        },
        {
         "from": 55,
         "to": 54,
         "Hellinger_distance": 13.667108759706185,
         "_deepnote_index_column": 3189
        },
        {
         "from": 55,
         "to": 55,
         "Hellinger_distance": 0,
         "_deepnote_index_column": 3190
        },
        {
         "from": 55,
         "to": 56,
         "Hellinger_distance": 13.37298611157519,
         "_deepnote_index_column": 3191
        },
        {
         "from": 56,
         "to": 0,
         "Hellinger_distance": 33.03288476052573,
         "_deepnote_index_column": 3192
        },
        {
         "from": 56,
         "to": 1,
         "Hellinger_distance": 18.05831848882504,
         "_deepnote_index_column": 3193
        },
        {
         "from": 56,
         "to": 2,
         "Hellinger_distance": 15.2106383673187,
         "_deepnote_index_column": 3194
        },
        {
         "from": 56,
         "to": 3,
         "Hellinger_distance": 18.85132161529216,
         "_deepnote_index_column": 3195
        },
        {
         "from": 56,
         "to": 4,
         "Hellinger_distance": 23.27195720717424,
         "_deepnote_index_column": 3196
        },
        {
         "from": 56,
         "to": 5,
         "Hellinger_distance": 13.002970117718622,
         "_deepnote_index_column": 3197
        },
        {
         "from": 56,
         "to": 6,
         "Hellinger_distance": 16.259745879579434,
         "_deepnote_index_column": 3198
        },
        {
         "from": 56,
         "to": 7,
         "Hellinger_distance": 24.488819856422285,
         "_deepnote_index_column": 3199
        },
        {
         "from": 56,
         "to": 8,
         "Hellinger_distance": 13.429332620488934,
         "_deepnote_index_column": 3200
        },
        {
         "from": 56,
         "to": 9,
         "Hellinger_distance": 8.812446697160489,
         "_deepnote_index_column": 3201
        },
        {
         "from": 56,
         "to": 10,
         "Hellinger_distance": 16.005921186794552,
         "_deepnote_index_column": 3202
        },
        {
         "from": 56,
         "to": 11,
         "Hellinger_distance": 19.517145191651366,
         "_deepnote_index_column": 3203
        },
        {
         "from": 56,
         "to": 12,
         "Hellinger_distance": 13.952836763929716,
         "_deepnote_index_column": 3204
        },
        {
         "from": 56,
         "to": 13,
         "Hellinger_distance": 30.3998469168388,
         "_deepnote_index_column": 3205
        },
        {
         "from": 56,
         "to": 14,
         "Hellinger_distance": 20.059967108150666,
         "_deepnote_index_column": 3206
        },
        {
         "from": 56,
         "to": 15,
         "Hellinger_distance": 18.67699707688699,
         "_deepnote_index_column": 3207
        },
        {
         "from": 56,
         "to": 16,
         "Hellinger_distance": 16.130240538956336,
         "_deepnote_index_column": 3208
        },
        {
         "from": 56,
         "to": 17,
         "Hellinger_distance": 10.905620124696183,
         "_deepnote_index_column": 3209
        },
        {
         "from": 56,
         "to": 18,
         "Hellinger_distance": 20.26788941889057,
         "_deepnote_index_column": 3210
        },
        {
         "from": 56,
         "to": 19,
         "Hellinger_distance": 9.082577370668918,
         "_deepnote_index_column": 3211
        },
        {
         "from": 56,
         "to": 20,
         "Hellinger_distance": 21.377234774288734,
         "_deepnote_index_column": 3212
        },
        {
         "from": 56,
         "to": 21,
         "Hellinger_distance": 14.924545821861742,
         "_deepnote_index_column": 3213
        },
        {
         "from": 56,
         "to": 22,
         "Hellinger_distance": 16.124205046364203,
         "_deepnote_index_column": 3214
        },
        {
         "from": 56,
         "to": 23,
         "Hellinger_distance": 7.752585534330618,
         "_deepnote_index_column": 3215
        },
        {
         "from": 56,
         "to": 24,
         "Hellinger_distance": 18.24777364252117,
         "_deepnote_index_column": 3216
        },
        {
         "from": 56,
         "to": 25,
         "Hellinger_distance": 14.727870942344705,
         "_deepnote_index_column": 3217
        },
        {
         "from": 56,
         "to": 26,
         "Hellinger_distance": 19.974212521823265,
         "_deepnote_index_column": 3218
        },
        {
         "from": 56,
         "to": 27,
         "Hellinger_distance": 8.035708866741851,
         "_deepnote_index_column": 3219
        },
        {
         "from": 56,
         "to": 28,
         "Hellinger_distance": 14.744598839328074,
         "_deepnote_index_column": 3220
        },
        {
         "from": 56,
         "to": 29,
         "Hellinger_distance": 18.254884742360474,
         "_deepnote_index_column": 3221
        },
        {
         "from": 56,
         "to": 30,
         "Hellinger_distance": 12.093868027008714,
         "_deepnote_index_column": 3222
        },
        {
         "from": 56,
         "to": 31,
         "Hellinger_distance": 27.002687531024396,
         "_deepnote_index_column": 3223
        },
        {
         "from": 56,
         "to": 32,
         "Hellinger_distance": 17.153090583070725,
         "_deepnote_index_column": 3224
        },
        {
         "from": 56,
         "to": 33,
         "Hellinger_distance": 22.9810551569971,
         "_deepnote_index_column": 3225
        },
        {
         "from": 56,
         "to": 34,
         "Hellinger_distance": 27.112798810405952,
         "_deepnote_index_column": 3226
        },
        {
         "from": 56,
         "to": 35,
         "Hellinger_distance": 32.45983413608678,
         "_deepnote_index_column": 3227
        },
        {
         "from": 56,
         "to": 36,
         "Hellinger_distance": 12.890277088383955,
         "_deepnote_index_column": 3228
        },
        {
         "from": 56,
         "to": 37,
         "Hellinger_distance": 20.800852889688443,
         "_deepnote_index_column": 3229
        },
        {
         "from": 56,
         "to": 38,
         "Hellinger_distance": 10.373964972834102,
         "_deepnote_index_column": 3230
        },
        {
         "from": 56,
         "to": 39,
         "Hellinger_distance": 19.215064116665165,
         "_deepnote_index_column": 3231
        },
        {
         "from": 56,
         "to": 40,
         "Hellinger_distance": 23.96587905517366,
         "_deepnote_index_column": 3232
        },
        {
         "from": 56,
         "to": 41,
         "Hellinger_distance": 16.020656894034612,
         "_deepnote_index_column": 3233
        },
        {
         "from": 56,
         "to": 42,
         "Hellinger_distance": 11.642402636776612,
         "_deepnote_index_column": 3234
        },
        {
         "from": 56,
         "to": 43,
         "Hellinger_distance": 16.19490518362218,
         "_deepnote_index_column": 3235
        },
        {
         "from": 56,
         "to": 44,
         "Hellinger_distance": 14.950824135813734,
         "_deepnote_index_column": 3236
        },
        {
         "from": 56,
         "to": 45,
         "Hellinger_distance": 11.7966958279937,
         "_deepnote_index_column": 3237
        },
        {
         "from": 56,
         "to": 46,
         "Hellinger_distance": 12.306760533091094,
         "_deepnote_index_column": 3238
        },
        {
         "from": 56,
         "to": 47,
         "Hellinger_distance": 16.72895023631113,
         "_deepnote_index_column": 3239
        },
        {
         "from": 56,
         "to": 48,
         "Hellinger_distance": 11.096985231195875,
         "_deepnote_index_column": 3240
        },
        {
         "from": 56,
         "to": 49,
         "Hellinger_distance": 17.451005693738942,
         "_deepnote_index_column": 3241
        },
        {
         "from": 56,
         "to": 50,
         "Hellinger_distance": 17.5238564228925,
         "_deepnote_index_column": 3242
        },
        {
         "from": 56,
         "to": 51,
         "Hellinger_distance": 11.052852603799222,
         "_deepnote_index_column": 3243
        },
        {
         "from": 56,
         "to": 52,
         "Hellinger_distance": 23.280710785205084,
         "_deepnote_index_column": 3244
        },
        {
         "from": 56,
         "to": 53,
         "Hellinger_distance": 15.708146326854173,
         "_deepnote_index_column": 3245
        },
        {
         "from": 56,
         "to": 54,
         "Hellinger_distance": 19.687471577282867,
         "_deepnote_index_column": 3246
        },
        {
         "from": 56,
         "to": 55,
         "Hellinger_distance": 13.37298611157519,
         "_deepnote_index_column": 3247
        },
        {
         "from": 56,
         "to": 56,
         "Hellinger_distance": 0,
         "_deepnote_index_column": 3248
        }
       ]
      },
      "text/plain": "      from  to  Hellinger_distance\n0        0   0            0.000000\n1        0   1           39.229294\n2        0   2           41.574549\n3        0   3           21.161725\n4        0   4           25.185514\n...    ...  ..                 ...\n3244    56  52           23.280711\n3245    56  53           15.708146\n3246    56  54           19.687472\n3247    56  55           13.372986\n3248    56  56            0.000000\n\n[3249 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>from</th>\n      <th>to</th>\n      <th>Hellinger_distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>39.229294</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>41.574549</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>21.161725</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>25.185514</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3244</th>\n      <td>56</td>\n      <td>52</td>\n      <td>23.280711</td>\n    </tr>\n    <tr>\n      <th>3245</th>\n      <td>56</td>\n      <td>53</td>\n      <td>15.708146</td>\n    </tr>\n    <tr>\n      <th>3246</th>\n      <td>56</td>\n      <td>54</td>\n      <td>19.687472</td>\n    </tr>\n    <tr>\n      <th>3247</th>\n      <td>56</td>\n      <td>55</td>\n      <td>13.372986</td>\n    </tr>\n    <tr>\n      <th>3248</th>\n      <td>56</td>\n      <td>56</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>3249 rows × 3 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Save the distance between topics",
   "metadata": {
    "cell_id": "00106-f483032a-1487-453a-8fd9-75f5eace530a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "dc573ea7",
    "execution_millis": 47,
    "execution_start": 1614091041810,
    "cell_id": "00107-4964ef59-fff7-4de9-98b8-b3ad430f9132",
    "deepnote_cell_type": "code"
   },
   "source": "topic_distances.to_csv('Topic_distances.csv', index=False)",
   "outputs": [
    {
     "name": "stderr",
     "text": "/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Group the topics together based on their Hellinger distance and summarise them with a label based on their word distribution",
   "metadata": {
    "tags": [],
    "cell_id": "00060-7b79bbdb-3c09-4ad7-a5e6-17bdfe9d6141",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2514c728",
    "execution_millis": 0,
    "execution_start": 1614413675391,
    "cell_id": "00082-03edf4d3-663f-4de2-9add-eedf6275c078",
    "deepnote_cell_type": "code"
   },
   "source": "Topic1 = \"Duration\"\nTopic2 = \"Looks\"\nTopic3 = \"Cluster3\"\nTopic4 = \"Cluster4\"\nTopic5 = \"Cluster5\"\nTopic6 = \"Female_attribute\"\nTopic7 = \"Video\"\nTopic8 = \"Skills\"\nTopic9 = \"Intercourse\"\nTopic10 = \"Orgasm\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# PT4: Comments labelling",
   "metadata": {
    "cell_id": "00083-c3e05d29-459b-4a89-a306-fbe01e61de91",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Load the data",
   "metadata": {
    "tags": [],
    "cell_id": "00064-7d1a5ed2-5752-464e-9c7e-8131b2268360",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "deepnote_to_be_reexecuted": false,
    "source_hash": "661933d",
    "execution_millis": 5667,
    "execution_start": 1614413681039,
    "cell_id": "00085-65413944-fc4a-4260-9eeb-afb2e0f5180d",
    "deepnote_cell_type": "code"
   },
   "source": "final_data = pd.read_csv(\"ReadyForAnalysis.csv\")\ncols = data.columns\n\n# Add all of the other columns\nfinal_data = pd.DataFrame(final_data.loc[(final_data.clean==True) & (final_data.is_eng==True), [\"title\", \n                                                                             \"views\", \n                                                                             \"up_votes\", \n                                                                             \"down_votes\", \n                                                                             \"percent\", \n                                                                             \"author\",\n                                                                             \"author_subscriber\",\n                                                                             \"categories\",\n                                                                             \"tags\",\n                                                                             \"production\",\n                                                                             \"description\",\n                                                                             \"duration\",\n                                                                             \"upload_date\",\n                                                                             \"pornstars\",\n                                                                             \"download_urls\",\n                                                                             \"thumbnail_url\",\n                                                                             \"number_of_comment\",\n                                                                             \"url\",\n                                                                             \"error\",\n                                                                             \"repeat_n_times\",\n                                                                             \"comms\",\n                                                                             \"clean\",\n                                                                             \"is_eng\"\n                                                                            ]])\n\nfinal_data = final_data.dropna(subset=['comms'])\nfinal_data = final_data.reset_index(drop=True)\nfinal_data['index'] = final_data.index",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Label comments",
   "metadata": {
    "cell_id": "00087-22f1f43f-a5bc-4060-8e20-cdd01cb3c0e1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Create the labelling function",
   "metadata": {
    "cell_id": "00091-cc1aea69-982d-41b0-858a-5098e4c1192a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6ccd0cd4",
    "execution_millis": 1,
    "execution_start": 1614413722970,
    "cell_id": "00092-b8ef5f9b-8361-4402-af63-23a50daaa918",
    "deepnote_cell_type": "code"
   },
   "source": "def labelling(com):\n\n    #Process the new document\n    com = preprocess(com)\n    \n    #Compute matching topic\n    topics = lda_model.show_topics(formatted=True, num_topics=57, num_words=10)\n    label = pd.DataFrame([(el[0], round(el[1],2), topics[el[0]][1]) for el in lda_model[dictionary.doc2bow(com)]], \n             columns=['topic', 'weight', 'words in topic'])\n    label = label.sort_values(by=['weight'], ascending = False)\n    label = label.reset_index(drop=True)\n    \n    #Record topic and weight\n    topic_n_weight = []\n    \n    topic = label.loc[0, \"topic\"]\n    weight = label.loc[0, \"weight\"]\n    \n    topic_n_weight.append(topic)\n    topic_n_weight.append(weight)\n    \n    return(topic_n_weight)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Apply labelling function to every document in the dataset",
   "metadata": {
    "cell_id": "00093-a3935080-39a9-44c5-94f1-ba410e96fd7c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c4e4d8ad",
    "execution_millis": 1331547,
    "execution_start": 1614414087182,
    "cell_id": "00095-7a3c70e9-8538-4e48-b2e9-457d13049dc6",
    "deepnote_cell_type": "code"
   },
   "source": "#Create columns\nfinal_data[\"topic_nb\"] = range(len(final_data))\n# final_data[\"weight\"] = range(len(final_data))\n\n# Create the clusters\nclust_01 = [12, 15, 11, 20, 16, 21]\nclust_02 = [33, 52, 25, 30, 48, 41, 39, 50, 44, 49]\nclust_03 = [43, 53, 46, 47, 54]\nclust_04 = [18, 26, 37, 40]\nclust_05 = [56, 55, 45, 51]\nclust_06 = [14, 29, 17, 22, 19, 23, 27, 28, 38, 42, 36, 24, 32]\nclust_07 = [35, 13, 31, 34]\nclust_08 = [4, 7, 3, 5, 8]\nclust_09 = [0]\nclust_10 = [1, 2, 9, 6, 10]\n\n#Run the labelling function over each comment\nfor i in range(len(final_data)):\n    label = labelling(final_data[\"comms\"][i])\n    final_data[\"topic_nb\"][i] = label[0]\n#     final_data[\"weight\"][i] = label[1]\n    \n\n#Apply the true label\nfinal_data[\"topic_label\"] = range(len(final_data))\n\nfor i in range(len(final_data)):\n    if final_data[\"topic_nb\"][i] in clust_01:\n        final_data[\"topic_label\"][i] = Topic1\n\n    if final_data[\"topic_nb\"][i] in clust_02:\n        final_data[\"topic_label\"][i] = Topic2\n        \n    if final_data[\"topic_nb\"][i] in clust_03:\n        final_data[\"topic_label\"][i] = Topic3\n    \n    if final_data[\"topic_nb\"][i] in clust_04:\n        final_data[\"topic_label\"][i] = Topic4\n    \n    if final_data[\"topic_nb\"][i] in clust_05:\n        final_data[\"topic_label\"][i] = Topic5\n    \n    if final_data[\"topic_nb\"][i] in clust_06:\n        final_data[\"topic_label\"][i] = Topic6\n    \n    if final_data[\"topic_nb\"][i] in clust_07:\n        final_data[\"topic_label\"][i] = Topic7\n    \n    if final_data[\"topic_nb\"][i] in clust_08:\n        final_data[\"topic_label\"][i] = Topic8\n    \n    if final_data[\"topic_nb\"][i] in clust_09:\n        final_data[\"topic_label\"][i] = Topic9\n    \n    if final_data[\"topic_nb\"][i] in clust_10:\n        final_data[\"topic_label\"][i] = Topic10\n    \n    #Progress bar\n    if i == 50290:\n        print(\"25%\")\n\n    if i == 100580:\n        print(\"50%\")\n\n    if i == 150870:\n        print(\"75%\")\n\n    ",
   "outputs": [
    {
     "name": "stdout",
     "text": "25%\n50%\n75%\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Save the results",
   "metadata": {
    "cell_id": "00097-f336f766-aabc-431c-aee0-a4bb411ae8a8",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7c259c7e",
    "execution_start": 1614415549206,
    "execution_millis": 8886,
    "cell_id": "00098-e56628e2-04b8-4586-99a4-eee3995a02f9",
    "deepnote_cell_type": "code"
   },
   "source": "final_data.to_csv('Final_data.csv', index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5b677b82-ea1e-4dc8-9b2a-3c037ad9d27c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "deepnote_notebook_id": "ad8cd940-7156-4066-bf5b-ccf9f2af07b4",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}